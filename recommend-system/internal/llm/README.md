# LLM å®¢æˆ·ç«¯æ¨¡å—

> å¤§è¯­è¨€æ¨¡å‹å®¢æˆ·ç«¯æ¥å£ä¸å®ç°

## ç›®å½•

- [æ¦‚è¿°](#æ¦‚è¿°)
- [æ¶æ„è®¾è®¡](#æ¶æ„è®¾è®¡)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [API å‚è€ƒ](#api-å‚è€ƒ)
- [å®¢æˆ·ç«¯å®ç°](#å®¢æˆ·ç«¯å®ç°)
- [é«˜çº§åŠŸèƒ½](#é«˜çº§åŠŸèƒ½)
- [æµ‹è¯•](#æµ‹è¯•)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## æ¦‚è¿°

æœ¬æ¨¡å—æä¾›ç»Ÿä¸€çš„ LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰å®¢æˆ·ç«¯æ¥å£ï¼Œæ”¯æŒå¤šç§åç«¯å®ç°ï¼š

- **OpenAI API**ï¼šæ ‡å‡† OpenAI æ¥å£
- **Azure OpenAI**ï¼šAzure æ‰˜ç®¡çš„ OpenAI æœåŠ¡
- **Ollama**ï¼šæœ¬åœ°æ¨¡å‹æ¨ç†
- **è‡ªå®šä¹‰ HTTP æœåŠ¡**ï¼šæ”¯æŒä»»æ„ HTTP æ¨ç†æœåŠ¡

### æ ¸å¿ƒç‰¹æ€§

- ğŸ”Œ **ç»Ÿä¸€æ¥å£**ï¼šæ‰€æœ‰å®¢æˆ·ç«¯å®ç°ç›¸åŒçš„ `LLMClient` æ¥å£
- ğŸ”„ **è‡ªåŠ¨é‡è¯•**ï¼šå†…ç½®é‡è¯•æœºåˆ¶ï¼Œæ”¯æŒæŒ‡æ•°é€€é¿
- ğŸ’¾ **å“åº”ç¼“å­˜**ï¼šå‡å°‘é‡å¤è°ƒç”¨ï¼Œé™ä½æˆæœ¬
- ğŸš¦ **å¹¶å‘æ§åˆ¶**ï¼šé™åˆ¶å¹¶å‘è¯·æ±‚æ•°é‡
- ğŸ§ª **Mock å®¢æˆ·ç«¯**ï¼šæ–¹ä¾¿å•å…ƒæµ‹è¯•

---

## æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      interfaces.LLMClient                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚  Complete   â”‚ â”‚    Chat     â”‚ â”‚    Embed    â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                   â”‚                   â”‚
          â–¼                   â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OpenAIClient   â”‚ â”‚  OllamaClient   â”‚ â”‚ HTTPInference   â”‚
â”‚                 â”‚ â”‚                 â”‚ â”‚     Client      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AzureOpenAI     â”‚
â”‚    Client       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### åŒ…è£…å™¨æ¨¡å¼

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      è°ƒç”¨é“¾ç¤ºä¾‹                               â”‚
â”‚                                                             â”‚
â”‚  RateLimitedClient â†’ RetryClient â†’ CachedClient â†’ åŸºç¡€å®¢æˆ·ç«¯  â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## å¿«é€Ÿå¼€å§‹

### 1. åˆ›å»º OpenAI å®¢æˆ·ç«¯

```go
import (
    "context"
    "recommend-system/internal/llm"
    "recommend-system/internal/interfaces"
)

// åˆ›å»ºå®¢æˆ·ç«¯
client, err := llm.NewOpenAIClient(llm.OpenAIConfig{
    APIKey: "your-api-key",
    Model:  "gpt-4",
})
if err != nil {
    log.Fatal(err)
}

// æ–‡æœ¬è¡¥å…¨
response, err := client.Complete(ctx, "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±")

// å¯¹è¯
messages := []interfaces.Message{
    {Role: "system", Content: "ä½ æ˜¯ä¸€ä¸ªæ¨èç³»ç»ŸåŠ©æ‰‹"},
    {Role: "user", Content: "æ¨èä¸€äº›ç§‘æŠ€ç±»äº§å“"},
}
response, err := client.Chat(ctx, messages)

// æ–‡æœ¬åµŒå…¥
embedding, err := client.Embed(ctx, "è¿™æ˜¯ä¸€æ®µéœ€è¦åµŒå…¥çš„æ–‡æœ¬")
```

### 2. åˆ›å»º Ollama å®¢æˆ·ç«¯ï¼ˆæœ¬åœ°æ¨¡å‹ï¼‰

```go
client := llm.NewOllamaClient(llm.OllamaConfig{
    BaseURL: "http://localhost:11434",
    Model:   "qwen:7b",
})

response, err := client.Complete(ctx, "Hello, world!")
```

### 3. ä½¿ç”¨é€‰é¡¹æ§åˆ¶å‚æ•°

```go
response, err := client.Chat(ctx, messages,
    interfaces.WithMaxTokens(512),
    interfaces.WithTemperature(0.3),
    interfaces.WithModel("gpt-4-turbo"),
)
```

---

## API å‚è€ƒ

### LLMClient æ¥å£

```go
type LLMClient interface {
    // Complete æ–‡æœ¬è¡¥å…¨
    Complete(ctx context.Context, prompt string, opts ...LLMOption) (string, error)
    
    // Embed æ–‡æœ¬åµŒå…¥
    Embed(ctx context.Context, text string) ([]float32, error)
    
    // Chat å¯¹è¯å¼äº¤äº’
    Chat(ctx context.Context, messages []Message, opts ...LLMOption) (string, error)
}
```

### Message ç»“æ„

```go
type Message struct {
    Role    string `json:"role"`    // system, user, assistant
    Content string `json:"content"` // æ¶ˆæ¯å†…å®¹
}
```

### LLMOption é€‰é¡¹

| é€‰é¡¹ | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `WithMaxTokens(n)` | æœ€å¤§ Token æ•° | 256 |
| `WithTemperature(t)` | æ¸©åº¦å‚æ•° (0-2) | 0.7 |
| `WithModel(m)` | æ¨¡å‹åç§° | gpt-3.5-turbo |

---

## å®¢æˆ·ç«¯å®ç°

### OpenAI å®¢æˆ·ç«¯

```go
// é…ç½®é€‰é¡¹
type OpenAIConfig struct {
    APIKey         string        // API å¯†é’¥ï¼ˆå¿…éœ€ï¼‰
    BaseURL        string        // API åœ°å€ï¼Œé»˜è®¤ https://api.openai.com/v1
    Model          string        // é»˜è®¤æ¨¡å‹ï¼Œé»˜è®¤ gpt-3.5-turbo
    EmbeddingModel string        // åµŒå…¥æ¨¡å‹ï¼Œé»˜è®¤ text-embedding-ada-002
    Timeout        time.Duration // è¶…æ—¶æ—¶é—´ï¼Œé»˜è®¤ 30s
}

// åˆ›å»ºå®¢æˆ·ç«¯
client, err := llm.NewOpenAIClient(config)
```

### Azure OpenAI å®¢æˆ·ç«¯

```go
type AzureOpenAIConfig struct {
    APIKey       string        // Azure API å¯†é’¥
    Endpoint     string        // Azure ç«¯ç‚¹ URL
    DeploymentID string        // æ¨¡å‹éƒ¨ç½² ID
    APIVersion   string        // API ç‰ˆæœ¬ï¼Œé»˜è®¤ 2023-05-15
    Timeout      time.Duration // è¶…æ—¶æ—¶é—´
}

client, err := llm.NewAzureOpenAIClient(config)
```

### Ollama å®¢æˆ·ç«¯

```go
type OllamaConfig struct {
    BaseURL        string        // æœåŠ¡åœ°å€ï¼Œé»˜è®¤ http://localhost:11434
    Model          string        // é»˜è®¤æ¨¡å‹ï¼Œé»˜è®¤ llama2
    EmbeddingModel string        // åµŒå…¥æ¨¡å‹ï¼Œé»˜è®¤ nomic-embed-text
    Timeout        time.Duration // è¶…æ—¶æ—¶é—´ï¼Œé»˜è®¤ 120s
}

client := llm.NewOllamaClient(config)
```

### è‡ªå®šä¹‰ HTTP æ¨ç†å®¢æˆ·ç«¯

```go
type HTTPInferenceConfig struct {
    BaseURL       string        // æ¨ç†æœåŠ¡åœ°å€
    APIKey        string        // å¯é€‰çš„ API å¯†é’¥
    ChatEndpoint  string        // å¯¹è¯ç«¯ç‚¹ï¼Œé»˜è®¤ /v1/chat
    EmbedEndpoint string        // åµŒå…¥ç«¯ç‚¹ï¼Œé»˜è®¤ /v1/embed
    Timeout       time.Duration // è¶…æ—¶æ—¶é—´
}

client, err := llm.NewHTTPInferenceClient(config)
```

---

## é«˜çº§åŠŸèƒ½

### 1. å“åº”ç¼“å­˜

```go
// åˆ›å»ºå¸¦ç¼“å­˜çš„å®¢æˆ·ç«¯
baseClient, _ := llm.NewOpenAIClient(config)
cachedClient := llm.NewCachedClient(baseClient, time.Hour)

// ç›¸åŒçš„è¯·æ±‚ä¼šä»ç¼“å­˜è¿”å›
response1, _ := cachedClient.Complete(ctx, "Hello")
response2, _ := cachedClient.Complete(ctx, "Hello") // ä»ç¼“å­˜è¿”å›
```

### 2. è‡ªåŠ¨é‡è¯•

```go
// åˆ›å»ºå¸¦é‡è¯•çš„å®¢æˆ·ç«¯
baseClient, _ := llm.NewOpenAIClient(config)
retryClient := llm.NewRetryClient(baseClient, 3, time.Second)

// å¤±è´¥æ—¶è‡ªåŠ¨é‡è¯•ï¼Œæœ€å¤š 3 æ¬¡
response, err := retryClient.Complete(ctx, "Hello")
```

### 3. å¹¶å‘é™åˆ¶

```go
// åˆ›å»ºå¸¦å¹¶å‘é™åˆ¶çš„å®¢æˆ·ç«¯
baseClient, _ := llm.NewOpenAIClient(config)
limitedClient := llm.NewRateLimitedClient(baseClient, 5) // æœ€å¤§ 5 å¹¶å‘

// è¶…è¿‡é™åˆ¶çš„è¯·æ±‚ä¼šç­‰å¾…
response, err := limitedClient.Complete(ctx, "Hello")
```

### 4. ç»„åˆä½¿ç”¨

```go
// ç»„åˆå¤šä¸ªåŒ…è£…å™¨
baseClient, _ := llm.NewOpenAIClient(config)

// å…ˆç¼“å­˜ -> å†é‡è¯• -> å†é™æµ
client := llm.NewRateLimitedClient(
    llm.NewRetryClient(
        llm.NewCachedClient(baseClient, time.Hour),
        3,
        time.Second,
    ),
    10,
)
```

### 5. æ¶ˆæ¯æ„å»ºè¾…åŠ©å‡½æ•°

```go
// å¿«é€Ÿæ„å»ºæ¶ˆæ¯
systemMsg := llm.BuildSystemMessage("ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹")
userMsg := llm.BuildUserMessage("ä½ å¥½")
assistantMsg := llm.BuildAssistantMessage("ä½ å¥½ï¼")

// å¿«é€Ÿæ„å»ºæ¶ˆæ¯åˆ—è¡¨
messages := llm.BuildMessages(
    "ä½ æ˜¯ä¸€ä¸ªæ¨èç³»ç»ŸåŠ©æ‰‹",
    "æ¨èä¸€äº›ç§‘æŠ€äº§å“",
)
```

---

## æµ‹è¯•

### ä½¿ç”¨ Mock å®¢æˆ·ç«¯

```go
// åˆ›å»º Mock å®¢æˆ·ç«¯
mock := llm.NewMockClient()

// è®¾ç½®é»˜è®¤å“åº”
mock.SetResponse("è¿™æ˜¯æ¨¡æ‹Ÿå“åº”")
mock.SetEmbedding([]float32{0.1, 0.2, 0.3})

// è®¾ç½®è‡ªå®šä¹‰è¡Œä¸º
mock.CompleteFunc = func(ctx context.Context, prompt string, opts ...interfaces.LLMOption) (string, error) {
    return "è‡ªå®šä¹‰å“åº”: " + prompt, nil
}

// æ¨¡æ‹Ÿé”™è¯¯
mock.SetError(errors.New("æ¨¡æ‹Ÿé”™è¯¯"))

// æ¸…é™¤é”™è¯¯
mock.ClearError()
```

### è¿è¡Œå•å…ƒæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
go test -v ./internal/llm/...

# è¿è¡Œç‰¹å®šæµ‹è¯•
go test -v ./internal/llm/ -run TestOpenAIClient

# æµ‹è¯•è¦†ç›–ç‡
go test -cover ./internal/llm/...
```

---

## æœ€ä½³å®è·µ

### 1. é”™è¯¯å¤„ç†

```go
response, err := client.Complete(ctx, prompt)
if err != nil {
    switch {
    case errors.Is(err, llm.ErrEmptyPrompt):
        // å¤„ç†ç©ºæç¤ºè¯
    case errors.Is(err, llm.ErrRateLimitExceeded):
        // å¤„ç†é€Ÿç‡é™åˆ¶
    case errors.Is(err, llm.ErrContextCanceled):
        // å¤„ç†ä¸Šä¸‹æ–‡å–æ¶ˆ
    default:
        // å…¶ä»–é”™è¯¯
    }
}
```

### 2. è¶…æ—¶æ§åˆ¶

```go
// ä¸º LLM è°ƒç”¨è®¾ç½®ä¸“ç”¨è¶…æ—¶
ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
defer cancel()

response, err := client.Complete(ctx, prompt)
```

### 3. æˆæœ¬æ§åˆ¶

- ä½¿ç”¨ `CachedClient` ç¼“å­˜ç›¸åŒè¯·æ±‚
- åˆç†è®¾ç½® `MaxTokens` é™åˆ¶è¾“å‡ºé•¿åº¦
- ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ï¼ˆå¦‚ gpt-3.5-turboï¼‰è¿›è¡Œç®€å•ä»»åŠ¡

### 4. é™çº§ç­–ç•¥

```go
// ä¸»å®¢æˆ·ç«¯ä½¿ç”¨ OpenAI
primaryClient, _ := llm.NewOpenAIClient(openaiConfig)

// å¤‡ç”¨å®¢æˆ·ç«¯ä½¿ç”¨æœ¬åœ° Ollama
fallbackClient := llm.NewOllamaClient(ollamaConfig)

// å®ç°é™çº§é€»è¾‘
response, err := primaryClient.Complete(ctx, prompt)
if err != nil {
    // é™çº§åˆ°æœ¬åœ°æ¨¡å‹
    response, err = fallbackClient.Complete(ctx, prompt)
}
```

---

## æ–‡ä»¶ç»“æ„

```
internal/llm/
â”œâ”€â”€ client.go          # æ¥å£å®šä¹‰ã€é…ç½®ã€åŒ…è£…å™¨
â”œâ”€â”€ client_test.go     # å®¢æˆ·ç«¯æµ‹è¯•
â”œâ”€â”€ openai.go          # OpenAI/Azure å®ç°
â”œâ”€â”€ openai_test.go     # OpenAI æµ‹è¯•
â”œâ”€â”€ local.go           # Ollama/HTTP æ¨ç†å®ç°
â”œâ”€â”€ local_test.go      # æœ¬åœ°å®¢æˆ·ç«¯æµ‹è¯•
â””â”€â”€ README.md          # æœ¬æ–‡æ¡£
```

---

## é”™è¯¯ç 

| é”™è¯¯ | è¯´æ˜ |
|------|------|
| `ErrEmptyPrompt` | æç¤ºè¯ä¸ºç©º |
| `ErrEmptyMessages` | æ¶ˆæ¯åˆ—è¡¨ä¸ºç©º |
| `ErrAPIKeyRequired` | ç¼ºå°‘ API å¯†é’¥ |
| `ErrRequestTimeout` | è¯·æ±‚è¶…æ—¶ |
| `ErrRateLimitExceeded` | è¶…å‡ºé€Ÿç‡é™åˆ¶ |
| `ErrModelNotAvailable` | æ¨¡å‹ä¸å¯ç”¨ |
| `ErrInvalidResponse` | æ— æ•ˆçš„å“åº” |
| `ErrContextCanceled` | ä¸Šä¸‹æ–‡å·²å–æ¶ˆ |

---

## è´¡çŒ®æŒ‡å—

1. æ–°å¢å®¢æˆ·ç«¯å®ç°éœ€è¦å®ç° `interfaces.LLMClient` æ¥å£
2. æ‰€æœ‰å…¬å¼€å‡½æ•°éœ€è¦æ·»åŠ è¯¦ç»†æ³¨é‡Š
3. ä¸ºæ–°åŠŸèƒ½ç¼–å†™å•å…ƒæµ‹è¯•
4. æ›´æ–°æœ¬æ–‡æ¡£

---

## ç‰ˆæœ¬å†å²

| ç‰ˆæœ¬ | æ—¥æœŸ | å˜æ›´ |
|------|------|------|
| 1.0.0 | 2026-01-04 | åˆå§‹ç‰ˆæœ¬ |

