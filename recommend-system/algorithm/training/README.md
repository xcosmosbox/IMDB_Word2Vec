# è®­ç»ƒ Pipeline æ¨¡å— (Person D)

> ç”Ÿæˆå¼æ¨èç³»ç»Ÿ - è®­ç»ƒæ¨¡å—å®Œæ•´æ–‡æ¡£  
> ç‰ˆæœ¬: 1.0.0  
> ä½œè€…: Person D  
> æ›´æ–°æ—¥æœŸ: 2026-01-04

---

## ğŸ“‘ ç›®å½•

1. [æ¦‚è¿°](#1-æ¦‚è¿°)
2. [å¿«é€Ÿå¼€å§‹](#2-å¿«é€Ÿå¼€å§‹)
3. [æ¨¡å—æ¶æ„](#3-æ¨¡å—æ¶æ„)
4. [é…ç½®è¯¦è§£](#4-é…ç½®è¯¦è§£)
5. [æ•°æ®é›†](#5-æ•°æ®é›†)
6. [æŸå¤±å‡½æ•°](#6-æŸå¤±å‡½æ•°)
7. [è®­ç»ƒå™¨](#7-è®­ç»ƒå™¨)
8. [ä¸‰é˜¶æ®µè®­ç»ƒ](#8-ä¸‰é˜¶æ®µè®­ç»ƒ)
9. [åˆ†å¸ƒå¼è®­ç»ƒ](#9-åˆ†å¸ƒå¼è®­ç»ƒ)
10. [è¯„ä¼°æŒ‡æ ‡](#10-è¯„ä¼°æŒ‡æ ‡)
11. [API å‚è€ƒ](#11-api-å‚è€ƒ)
12. [å¸¸è§é—®é¢˜](#12-å¸¸è§é—®é¢˜)

---

## 1. æ¦‚è¿°

### 1.1 æ¨¡å—èŒè´£

æœ¬æ¨¡å—å®ç°ç”Ÿæˆå¼æ¨èç³»ç»Ÿï¼ˆUGTï¼‰çš„å®Œæ•´è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š

- **ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥**ï¼šé¢„è®­ç»ƒ â†’ å¤šä»»åŠ¡å¾®è°ƒ â†’ åå¥½å¯¹é½
- **ç»Ÿä¸€æŸå¤±å‡½æ•°**ï¼šNTP + å¯¹æ¯”å­¦ä¹  + DPO + MoEå¹³è¡¡
- **åˆ†å¸ƒå¼è®­ç»ƒ**ï¼šæ”¯æŒ DDP å’Œ DeepSpeed
- **è®­ç»ƒå·¥ç¨‹**ï¼šæ··åˆç²¾åº¦ã€æ¢¯åº¦ç´¯ç§¯ã€æ£€æŸ¥ç‚¹ç®¡ç†

### 1.2 æŠ€æœ¯æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         è®­ç»ƒ Pipeline æ¶æ„                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                        ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥                                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚   â”‚
â”‚  â”‚  â”‚   Stage 1     â”‚â†’ â”‚   Stage 2     â”‚â†’ â”‚   Stage 3     â”‚            â”‚   â”‚
â”‚  â”‚  â”‚  åŸºç¡€é¢„è®­ç»ƒ    â”‚  â”‚  å¤šä»»åŠ¡å¾®è°ƒ    â”‚  â”‚  åå¥½å¯¹é½     â”‚            â”‚   â”‚
â”‚  â”‚  â”‚  (NTP Only)   â”‚  â”‚  (NTP + CL)   â”‚  â”‚ (NTP+CL+DPO) â”‚            â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                        ç»Ÿä¸€æŸå¤±å‡½æ•°                                   â”‚   â”‚
â”‚  â”‚  L_total = L_ntp + Î»â‚Â·L_contrastive + Î»â‚‚Â·L_preference + Î»â‚ƒÂ·L_moe   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Dataset  â”‚ â”‚ Optimizerâ”‚ â”‚ Schedulerâ”‚ â”‚Checkpointâ”‚ â”‚ Metrics  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 æ¥å£éµå¾ª

æœ¬æ¨¡å—éµå¾ª `algorithm/interfaces.py` ä¸­å®šä¹‰çš„ `TrainerInterface`ï¼š

```python
class TrainerInterface(ABC):
    @abstractmethod
    def train_epoch(self) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ª epochï¼Œè¿”å›è®­ç»ƒæŒ‡æ ‡"""
        pass
    
    @abstractmethod
    def evaluate(self) -> Dict[str, float]:
        """åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°ï¼Œè¿”å›è¯„ä¼°æŒ‡æ ‡"""
        pass
    
    @abstractmethod
    def save_checkpoint(self, path: str) -> None:
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        pass
    
    @abstractmethod
    def load_checkpoint(self, path: str) -> None:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        pass
```

---

## 2. å¿«é€Ÿå¼€å§‹

### 2.1 å®‰è£…ä¾èµ–

```bash
pip install torch>=2.0.0
pip install numpy tqdm tensorboard
pip install deepspeed  # å¯é€‰ï¼Œç”¨äºå¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒ
```

### 2.2 æœ€ç®€ç¤ºä¾‹

```python
from algorithm.training import (
    TrainingConfig,
    Trainer,
    RecommendDataset,
)

# 1. å‡†å¤‡é…ç½®
config = TrainingConfig(
    batch_size=256,
    max_epochs=5,
    learning_rate=1e-4,
    output_dir="checkpoints",
)

# 2. åŠ è½½æ•°æ®
train_dataset = RecommendDataset("data/train.jsonl")
eval_dataset = RecommendDataset("data/eval.jsonl")

# 3. åˆ›å»ºæ¨¡å‹ï¼ˆéœ€ä» encoder/decoder æ¨¡å—å¯¼å…¥ï¼‰
model = create_ugt_model()

# 4. è®­ç»ƒ
trainer = Trainer(
    model=model,
    config=config,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

result = trainer.train()
print(f"è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯æŸå¤±: {result['best_eval_loss']:.4f}")
```

### 2.3 ä½¿ç”¨è®­ç»ƒè„šæœ¬

```bash
# é˜¶æ®µ 1: é¢„è®­ç»ƒ
python -m algorithm.training.scripts.train_stage1 \
    --train_data data/pretrain/train.jsonl \
    --eval_data data/pretrain/eval.jsonl \
    --output_dir checkpoints/stage1 \
    --batch_size 256 \
    --max_epochs 5

# é˜¶æ®µ 2: å¤šä»»åŠ¡å¾®è°ƒ
python -m algorithm.training.scripts.train_stage2 \
    --pretrained checkpoints/stage1/best_model \
    --train_data data/multitask/train.jsonl \
    --output_dir checkpoints/stage2 \
    --lambda_contrastive 0.1

# é˜¶æ®µ 3: åå¥½å¯¹é½
python -m algorithm.training.scripts.train_stage3 \
    --pretrained checkpoints/stage2/best_model \
    --train_data data/preference/train.jsonl \
    --output_dir checkpoints/stage3 \
    --dpo_beta 0.1
```

---

## 3. æ¨¡å—æ¶æ„

### 3.1 æ–‡ä»¶ç»“æ„

```
training/
â”œâ”€â”€ __init__.py              # æ¨¡å—å¯¼å‡ºï¼Œå…¬å¼€ API
â”œâ”€â”€ config.py                # é…ç½®ç±»å®šä¹‰
â”‚   â”œâ”€â”€ TrainingConfig       # åŸºç¡€é…ç½®
â”‚   â”œâ”€â”€ Stage1Config         # é˜¶æ®µ1é…ç½®
â”‚   â”œâ”€â”€ Stage2Config         # é˜¶æ®µ2é…ç½®
â”‚   â””â”€â”€ Stage3Config         # é˜¶æ®µ3é…ç½®
â”œâ”€â”€ dataset.py               # æ•°æ®é›†å®ç°
â”‚   â”œâ”€â”€ RecommendDataset     # æ¨èè®­ç»ƒæ•°æ®é›†
â”‚   â”œâ”€â”€ PreferenceDataset    # åå¥½å¯¹é½æ•°æ®é›†
â”‚   â”œâ”€â”€ StreamingDataset     # æµå¼æ•°æ®é›†ï¼ˆå¤§è§„æ¨¡ï¼‰
â”‚   â””â”€â”€ DataCollator         # æ‰¹æ¬¡æ•´ç†å™¨
â”œâ”€â”€ loss.py                  # æŸå¤±å‡½æ•°
â”‚   â”œâ”€â”€ NextTokenPredictionLoss  # NTP æŸå¤±
â”‚   â”œâ”€â”€ ContrastiveLoss      # å¯¹æ¯”å­¦ä¹ æŸå¤±
â”‚   â”œâ”€â”€ DPOLoss              # DPO åå¥½æŸå¤±
â”‚   â””â”€â”€ UnifiedLoss          # ç»Ÿä¸€æŸå¤±
â”œâ”€â”€ optimizer.py             # ä¼˜åŒ–å™¨
â”‚   â”œâ”€â”€ AdamW                # å¸¦æƒé‡è¡°å‡çš„ Adam
â”‚   â”œâ”€â”€ LAMB                 # å¤§æ‰¹é‡ä¼˜åŒ–å™¨
â”‚   â””â”€â”€ create_optimizer()   # ä¼˜åŒ–å™¨å·¥å‚å‡½æ•°
â”œâ”€â”€ scheduler.py             # å­¦ä¹ ç‡è°ƒåº¦
â”‚   â”œâ”€â”€ LinearLR             # çº¿æ€§è¡°å‡
â”‚   â”œâ”€â”€ CosineLR             # ä½™å¼¦é€€ç«
â”‚   â”œâ”€â”€ PolynomialLR         # å¤šé¡¹å¼è¡°å‡
â”‚   â””â”€â”€ create_scheduler()   # è°ƒåº¦å™¨å·¥å‚å‡½æ•°
â”œâ”€â”€ trainer.py               # è®­ç»ƒå™¨ä¸»ç±»
â”‚   â””â”€â”€ Trainer              # æ ¸å¿ƒè®­ç»ƒé€»è¾‘
â”œâ”€â”€ checkpoint.py            # æ£€æŸ¥ç‚¹ç®¡ç†
â”‚   â””â”€â”€ CheckpointManager    # æ£€æŸ¥ç‚¹ä¿å­˜/åŠ è½½/æ¸…ç†
â”œâ”€â”€ metrics.py               # è¯„ä¼°æŒ‡æ ‡
â”‚   â”œâ”€â”€ recall_at_k()        # Recall@K
â”‚   â”œâ”€â”€ ndcg_at_k()          # NDCG@K
â”‚   â”œâ”€â”€ mrr()                # MRR
â”‚   â””â”€â”€ MetricsCalculator    # æŒ‡æ ‡è®¡ç®—å™¨
â”œâ”€â”€ distributed.py           # åˆ†å¸ƒå¼è®­ç»ƒ
â”‚   â”œâ”€â”€ setup_distributed()  # åˆå§‹åŒ–åˆ†å¸ƒå¼
â”‚   â”œâ”€â”€ DistributedTrainer   # DDP è®­ç»ƒå™¨
â”‚   â””â”€â”€ DeepSpeedTrainer     # DeepSpeed è®­ç»ƒå™¨
â”œâ”€â”€ scripts/                 # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ train_stage1.py      # é˜¶æ®µ1è„šæœ¬
â”‚   â”œâ”€â”€ train_stage2.py      # é˜¶æ®µ2è„šæœ¬
â”‚   â””â”€â”€ train_stage3.py      # é˜¶æ®µ3è„šæœ¬
â””â”€â”€ tests/                   # å•å…ƒæµ‹è¯•
    â””â”€â”€ test_training.py     # å®Œæ•´æµ‹è¯•ç”¨ä¾‹
```

### 3.2 æ¨¡å—ä¾èµ–å…³ç³»

```
config.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                   â”‚
    â–¼                                                   â”‚
dataset.py                                              â”‚
    â”‚                                                   â”‚
    â–¼                                                   â”‚
loss.py â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                   â”‚
    â–¼                                                   â”‚
optimizer.py â”€â”€â”€â”€â”€â”€â–º scheduler.py                      â”‚
    â”‚                    â”‚                              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
               â–¼                                        â”‚
          trainer.py â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
               â”‚                                        â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
    â–¼          â–¼          â–¼                             â”‚
checkpoint.py metrics.py distributed.py â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. é…ç½®è¯¦è§£

### 4.1 åŸºç¡€é…ç½® (TrainingConfig)

```python
from algorithm.training import TrainingConfig

config = TrainingConfig(
    # ===== åŸºç¡€é…ç½® =====
    output_dir="checkpoints",       # è¾“å‡ºç›®å½•
    experiment_name="ugt_training", # å®éªŒåç§°
    seed=42,                        # éšæœºç§å­
    
    # ===== æ‰¹æ¬¡é…ç½® =====
    batch_size=256,                 # æ¯ GPU æ‰¹æ¬¡å¤§å°
    gradient_accumulation_steps=4,  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    max_epochs=10,                  # æœ€å¤§è®­ç»ƒè½®æ•°
    max_steps=-1,                   # æœ€å¤§æ­¥æ•° (-1 è¡¨ç¤ºä¸é™åˆ¶)
    
    # ===== åºåˆ—é…ç½® =====
    max_seq_length=1024,            # æœ€å¤§åºåˆ—é•¿åº¦
    encoder_max_length=512,         # ç¼–ç å™¨æœ€å¤§é•¿åº¦
    decoder_max_length=128,         # è§£ç å™¨æœ€å¤§é•¿åº¦
    
    # ===== ä¼˜åŒ–å™¨é…ç½® =====
    learning_rate=1e-4,             # å­¦ä¹ ç‡
    weight_decay=0.01,              # æƒé‡è¡°å‡
    adam_beta1=0.9,                 # Adam Î²1
    adam_beta2=0.999,               # Adam Î²2
    adam_epsilon=1e-8,              # Adam Îµ
    max_grad_norm=1.0,              # æ¢¯åº¦è£å‰ªé˜ˆå€¼
    
    # ===== å­¦ä¹ ç‡è°ƒåº¦ =====
    lr_scheduler_type="cosine",     # è°ƒåº¦ç±»å‹
    warmup_steps=10000,             # é¢„çƒ­æ­¥æ•°
    min_lr_ratio=0.1,               # æœ€å°å­¦ä¹ ç‡æ¯”ä¾‹
    
    # ===== æ··åˆç²¾åº¦ =====
    fp16=True,                      # FP16 æ··åˆç²¾åº¦
    bf16=False,                     # BF16 æ··åˆç²¾åº¦
    
    # ===== æŸå¤±æƒé‡ =====
    lambda_contrastive=0.1,         # å¯¹æ¯”å­¦ä¹ æƒé‡ Î»â‚
    lambda_preference=0.1,          # åå¥½æŸå¤±æƒé‡ Î»â‚‚
    lambda_moe_balance=0.01,        # MoE å¹³è¡¡æƒé‡ Î»â‚ƒ
    
    # ===== å±‚æ¬¡åŒ–æŸå¤±æƒé‡ =====
    l1_loss_weight=0.5,             # L1 å±‚æƒé‡
    l2_loss_weight=0.3,             # L2 å±‚æƒé‡
    l3_loss_weight=0.2,             # L3 å±‚æƒé‡
    
    # ===== æ—¥å¿—å’Œä¿å­˜ =====
    logging_steps=100,              # æ—¥å¿—é—´éš”
    save_steps=1000,                # ä¿å­˜é—´éš”
    eval_steps=500,                 # è¯„ä¼°é—´éš”
    save_total_limit=3,             # æœ€å¤šä¿ç•™æ£€æŸ¥ç‚¹æ•°
    
    # ===== åˆ†å¸ƒå¼è®­ç»ƒ =====
    ddp=False,                      # æ˜¯å¦ä½¿ç”¨ DDP
    deepspeed=False,                # æ˜¯å¦ä½¿ç”¨ DeepSpeed
    zero_stage=2,                   # ZeRO é˜¶æ®µ
)
```

### 4.2 é˜¶æ®µç‰¹å®šé…ç½®

#### Stage 1: é¢„è®­ç»ƒé…ç½®

```python
from algorithm.training import Stage1Config

config = Stage1Config(
    max_epochs=5,
    learning_rate=1e-4,
    batch_size=512,
    
    # é˜¶æ®µ1ç‰¹æœ‰ï¼šä¸ä½¿ç”¨å¯¹æ¯”å­¦ä¹ å’Œåå¥½å­¦ä¹ 
    lambda_contrastive=0.0,
    lambda_preference=0.0,
)
```

#### Stage 2: å¤šä»»åŠ¡å¾®è°ƒé…ç½®

```python
from algorithm.training import Stage2Config

config = Stage2Config(
    max_epochs=3,
    learning_rate=5e-5,  # è¾ƒå°å­¦ä¹ ç‡
    
    # é˜¶æ®µ2ç‰¹æœ‰ï¼šåŠ å…¥å¯¹æ¯”å­¦ä¹ 
    lambda_contrastive=0.1,
    contrastive_temperature=0.07,
    num_negatives=127,
    
    # é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
    pretrained_model_path="checkpoints/stage1/best_model",
)
```

#### Stage 3: åå¥½å¯¹é½é…ç½®

```python
from algorithm.training import Stage3Config

config = Stage3Config(
    max_epochs=2,
    learning_rate=1e-5,  # æ›´å°å­¦ä¹ ç‡
    
    # é˜¶æ®µ3ç‰¹æœ‰ï¼šDPO å‚æ•°
    lambda_preference=0.1,
    dpo_beta=0.1,
    dpo_reference_free=False,
    
    # æ¨¡å‹è·¯å¾„
    pretrained_model_path="checkpoints/stage2/best_model",
    reference_model_path="checkpoints/stage2/best_model",
)
```

### 4.3 é…ç½®æ–‡ä»¶ (YAML)

æ”¯æŒä» YAML æ–‡ä»¶åŠ è½½é…ç½®ï¼š

```yaml
# configs/stage1.yaml
experiment_name: ugt_stage1_pretrain
batch_size: 256
max_epochs: 5
learning_rate: 1.0e-4
warmup_steps: 10000

fp16: true
lambda_contrastive: 0.0
lambda_preference: 0.0

output_dir: checkpoints/stage1
logging_steps: 100
save_steps: 1000
```

---

## 5. æ•°æ®é›†

### 5.1 æ•°æ®æ ¼å¼

#### æ¨èè®­ç»ƒæ•°æ® (JSON Lines)

```json
{
    "user_id": "user_123",
    "encoder_l1_ids": [1, 2, 3, 4, 5],
    "encoder_l2_ids": [10, 20, 30, 40, 50],
    "encoder_l3_ids": [100, 200, 300, 400, 500],
    "encoder_positions": [0, 1, 2, 3, 4],
    "encoder_token_types": [0, 1, 1, 1, 1],
    "encoder_mask": [1, 1, 1, 1, 1],
    "decoder_l1_ids": [6, 7],
    "decoder_l2_ids": [60, 70],
    "decoder_l3_ids": [600, 700],
    "decoder_positions": [0, 1],
    "decoder_token_types": [1, 1],
    "decoder_mask": [1, 1],
    "labels_l1": [7, 8],
    "labels_l2": [70, 80],
    "labels_l3": [700, 800]
}
```

#### åå¥½å¯¹é½æ•°æ® (DPO)

```json
{
    "user_id": "user_123",
    "user_sequence": {
        "encoder_l1_ids": [1, 2, 3],
        "encoder_l2_ids": [10, 20, 30],
        "encoder_l3_ids": [100, 200, 300],
        "encoder_positions": [0, 1, 2],
        "encoder_token_types": [0, 1, 1],
        "encoder_mask": [1, 1, 1]
    },
    "chosen_item": {
        "l1_id": 5,
        "l2_id": 50,
        "l3_id": 500
    },
    "rejected_item": {
        "l1_id": 6,
        "l2_id": 60,
        "l3_id": 600
    },
    "preference_score": 0.8
}
```

### 5.2 æ•°æ®é›†ä½¿ç”¨

```python
from algorithm.training import RecommendDataset, PreferenceDataset, DataCollator

# æ¨èæ•°æ®é›†
train_dataset = RecommendDataset(
    data_path="data/train.jsonl",
    max_encoder_length=512,
    max_decoder_length=128,
    pad_token_id=0,
    lazy_loading=False,  # å°æ•°æ®é›†ç«‹å³åŠ è½½
)

# åå¥½æ•°æ®é›†ï¼ˆç”¨äº Stage 3ï¼‰
preference_dataset = PreferenceDataset(
    data_path="data/preference.jsonl",
    max_encoder_length=512,
)

# æ•°æ®æ•´ç†å™¨
collator = DataCollator(
    pad_token_id=0,
    dynamic_padding=True,
)

# åˆ›å»º DataLoader
from torch.utils.data import DataLoader
dataloader = DataLoader(
    train_dataset,
    batch_size=32,
    shuffle=True,
    collate_fn=collator,
)
```

### 5.3 æµå¼æ•°æ®é›†ï¼ˆå¤§è§„æ¨¡è®­ç»ƒï¼‰

```python
from algorithm.training.dataset import StreamingDataset

# ç”¨äºè¶…å¤§è§„æ¨¡æ•°æ®
dataset = StreamingDataset(
    data_paths=["data/shard_0.jsonl", "data/shard_1.jsonl", ...],
    shuffle=True,
    world_size=8,  # åˆ†å¸ƒå¼è®­ç»ƒ
    rank=0,
)
```

---

## 6. æŸå¤±å‡½æ•°

### 6.1 ç»Ÿä¸€æŸå¤±å…¬å¼

```
L_total = L_ntp + Î»â‚ * L_contrastive + Î»â‚‚ * L_preference + Î»â‚ƒ * L_moe_balance

å…¶ä¸­ï¼š
- L_ntp = 0.5 * CE(L1) + 0.3 * CE(L2) + 0.2 * CE(L3)
- L_contrastive = InfoNCE(user_repr, item_repr)
- L_preference = DPO(chosen, rejected)
- L_moe_balance = ä¸“å®¶è´Ÿè½½å‡è¡¡æŸå¤±
```

### 6.2 å„æŸå¤±å‡½æ•°è¯¦è§£

#### Next Token Prediction (NTP)

```python
from algorithm.training import NextTokenPredictionLoss

ntp_loss = NextTokenPredictionLoss(
    l1_weight=0.5,       # L1 å±‚æƒé‡ï¼ˆç²—ç²’åº¦ï¼Œæœ€é‡è¦ï¼‰
    l2_weight=0.3,       # L2 å±‚æƒé‡
    l3_weight=0.2,       # L3 å±‚æƒé‡ï¼ˆç»†ç²’åº¦ï¼‰
    ignore_index=-100,   # å¿½ç•¥ padding
    label_smoothing=0.0, # æ ‡ç­¾å¹³æ»‘
)

loss, metrics = ntp_loss(
    l1_logits, l2_logits, l3_logits,
    labels_l1, labels_l2, labels_l3,
)
# metrics: {"ntp_loss", "ntp_l1_loss", "ntp_l2_loss", "ntp_l3_loss", 
#           "ntp_l1_acc", "ntp_l2_acc", "ntp_l3_acc"}
```

#### å¯¹æ¯”å­¦ä¹ æŸå¤± (InfoNCE)

```python
from algorithm.training import ContrastiveLoss

contrastive_loss = ContrastiveLoss(
    temperature=0.07,  # æ¸©åº¦å‚æ•°
    normalize=True,    # L2 å½’ä¸€åŒ–
)

loss, metrics = contrastive_loss(user_repr, item_repr)
# metrics: {"contrastive_loss", "contrastive_u2i_acc", "contrastive_i2u_acc"}
```

#### DPO åå¥½æŸå¤±

```python
from algorithm.training import DPOLoss

dpo_loss = DPOLoss(
    beta=0.1,              # æ¸©åº¦å‚æ•°
    reference_free=False,  # æ˜¯å¦ä½¿ç”¨å‚è€ƒæ¨¡å‹
)

loss, metrics = dpo_loss(
    chosen_logps,
    rejected_logps,
    reference_chosen_logps,
    reference_rejected_logps,
)
# metrics: {"dpo_loss", "dpo_accuracy", "dpo_reward_margin"}
```

#### ç»Ÿä¸€æŸå¤±

```python
from algorithm.training import UnifiedLoss

loss_fn = UnifiedLoss(
    l1_weight=0.5,
    l2_weight=0.3,
    l3_weight=0.2,
    lambda_contrastive=0.1,
    lambda_preference=0.1,
    lambda_moe_balance=0.01,
)

losses = loss_fn(
    model_outputs={"l1_logits": ..., "l2_logits": ..., "l3_logits": ...},
    labels={"l1": ..., "l2": ..., "l3": ...},
    aux_loss=moe_balance_loss,
)
# losses: {"total_loss", "ntp_loss", "contrastive_loss", "dpo_loss", ...}
```

---

## 7. è®­ç»ƒå™¨

### 7.1 åŸºç¡€è®­ç»ƒå™¨

```python
from algorithm.training import Trainer, TrainingConfig

trainer = Trainer(
    model=model,                    # PyTorch æ¨¡å‹
    config=config,                  # è®­ç»ƒé…ç½®
    train_dataset=train_dataset,    # è®­ç»ƒæ•°æ®é›†
    eval_dataset=eval_dataset,      # éªŒè¯æ•°æ®é›†ï¼ˆå¯é€‰ï¼‰
    optimizer=None,                 # è‡ªå®šä¹‰ä¼˜åŒ–å™¨ï¼ˆå¯é€‰ï¼‰
    scheduler=None,                 # è‡ªå®šä¹‰è°ƒåº¦å™¨ï¼ˆå¯é€‰ï¼‰
    reference_model=None,           # å‚è€ƒæ¨¡å‹ï¼Œç”¨äº DPOï¼ˆå¯é€‰ï¼‰
)

# å®Œæ•´è®­ç»ƒ
result = trainer.train()

# å•ä¸ª epoch è®­ç»ƒ
metrics = trainer.train_epoch()

# è¯„ä¼°
eval_metrics = trainer.evaluate()

# ä¿å­˜/åŠ è½½æ£€æŸ¥ç‚¹
trainer.save_checkpoint("checkpoints/step_1000")
trainer.load_checkpoint("checkpoints/step_1000")
```

### 7.2 è®­ç»ƒæµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           è®­ç»ƒæµç¨‹                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  for epoch in range(max_epochs):                                            â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â”œâ”€â”€â–º for batch in dataloader:                                          â”‚
â”‚      â”‚        â”‚                                                             â”‚
â”‚      â”‚        â”œâ”€â”€â–º 1. å‰å‘ä¼ æ’­ (with autocast if fp16)                       â”‚
â”‚      â”‚        â”‚        outputs = model(**batch)                             â”‚
â”‚      â”‚        â”‚                                                             â”‚
â”‚      â”‚        â”œâ”€â”€â–º 2. è®¡ç®—æŸå¤±                                               â”‚
â”‚      â”‚        â”‚        losses = loss_fn(outputs, labels)                    â”‚
â”‚      â”‚        â”‚                                                             â”‚
â”‚      â”‚        â”œâ”€â”€â–º 3. åå‘ä¼ æ’­                                               â”‚
â”‚      â”‚        â”‚        loss.backward()                                      â”‚
â”‚      â”‚        â”‚                                                             â”‚
â”‚      â”‚        â”œâ”€â”€â–º 4. æ¢¯åº¦ç´¯ç§¯ (if step % accumulation == 0)                 â”‚
â”‚      â”‚        â”‚        â”œâ”€â”€ æ¢¯åº¦è£å‰ª                                          â”‚
â”‚      â”‚        â”‚        â”œâ”€â”€ optimizer.step()                                 â”‚
â”‚      â”‚        â”‚        â”œâ”€â”€ scheduler.step()                                 â”‚
â”‚      â”‚        â”‚        â””â”€â”€ optimizer.zero_grad()                            â”‚
â”‚      â”‚        â”‚                                                             â”‚
â”‚      â”‚        â”œâ”€â”€â–º 5. æ—¥å¿—è®°å½• (if step % logging_steps == 0)                â”‚
â”‚      â”‚        â”‚                                                             â”‚
â”‚      â”‚        â”œâ”€â”€â–º 6. ä¿å­˜æ£€æŸ¥ç‚¹ (if step % save_steps == 0)                 â”‚
â”‚      â”‚        â”‚                                                             â”‚
â”‚      â”‚        â””â”€â”€â–º 7. è¯„ä¼° (if step % eval_steps == 0)                       â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â””â”€â”€â–º ä¿å­˜ epoch æ£€æŸ¥ç‚¹                                                  â”‚
â”‚                                                                             â”‚
â”‚  ä¿å­˜æœ€ç»ˆæ¨¡å‹                                                                â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.3 è®­ç»ƒè¾“å‡ºç¤ºä¾‹

```
================================================================================
å¼€å§‹è®­ç»ƒ
  å®éªŒåç§°: ugt_stage1_pretrain
  æ€»è½®æ•°: 5
  æ‰¹æ¬¡å¤§å°: 256
  æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: 4
  æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: 1024
  æ€»æ­¥æ•°: 50000
  é¢„çƒ­æ­¥æ•°: 10000
  å­¦ä¹ ç‡: 0.0001
================================================================================

Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [2:30:00<00:00, loss=3.2145, lr=1.00e-04]
Epoch 0 è®­ç»ƒæŒ‡æ ‡: {'loss': 3.2145, 'ntp_loss': 3.1823, 'learning_rate': 0.0001}
Epoch 0 éªŒè¯æŒ‡æ ‡: {'loss': 2.8934, 'recall@10': 0.1523, 'ndcg@10': 0.0892}

æ£€æŸ¥ç‚¹å·²ä¿å­˜è‡³ checkpoints/stage1/checkpoint-epoch-0

...

è®­ç»ƒå®Œæˆï¼
```

---

## 8. ä¸‰é˜¶æ®µè®­ç»ƒ

### 8.1 è®­ç»ƒç­–ç•¥æ¦‚è¿°

| é˜¶æ®µ | ç›®æ ‡ | æŸå¤±å‡½æ•° | å­¦ä¹ ç‡ | å…¸å‹è½®æ•° |
|------|------|----------|--------|----------|
| **Stage 1** | åŸºç¡€åºåˆ—å»ºæ¨¡ | L_ntp | 1e-4 | 5 |
| **Stage 2** | ç”¨æˆ·-ç‰©å“è¡¨ç¤ºå¯¹é½ | L_ntp + Î»â‚Â·L_cl | 5e-5 | 3 |
| **Stage 3** | åå¥½å¯¹é½ | L_ntp + Î»â‚Â·L_cl + Î»â‚‚Â·L_dpo | 1e-5 | 2 |

### 8.2 é˜¶æ®µ 1: åŸºç¡€é¢„è®­ç»ƒ

**ç›®æ ‡**ï¼šè®©æ¨¡å‹å­¦ä¼šåŸºç¡€çš„åºåˆ—å»ºæ¨¡èƒ½åŠ›

```python
from algorithm.training import Stage1Config, Trainer

config = Stage1Config(
    train_data_path="data/pretrain/train.jsonl",
    eval_data_path="data/pretrain/eval.jsonl",
    output_dir="checkpoints/stage1",
    
    batch_size=512,
    max_epochs=5,
    learning_rate=1e-4,
    
    # åªä½¿ç”¨ NTP æŸå¤±
    lambda_contrastive=0.0,
    lambda_preference=0.0,
)

trainer = Trainer(model=model, config=config, ...)
trainer.train()
```

### 8.3 é˜¶æ®µ 2: å¤šä»»åŠ¡å¾®è°ƒ

**ç›®æ ‡**ï¼šå­¦ä¹ ç”¨æˆ·å’Œç‰©å“çš„è¡¨ç¤ºå¯¹é½

```python
from algorithm.training import Stage2Config, Trainer

config = Stage2Config(
    pretrained_model_path="checkpoints/stage1/best_model",
    train_data_path="data/multitask/train.jsonl",
    output_dir="checkpoints/stage2",
    
    batch_size=256,
    max_epochs=3,
    learning_rate=5e-5,  # è¾ƒå°å­¦ä¹ ç‡
    
    # åŠ å…¥å¯¹æ¯”å­¦ä¹ 
    lambda_contrastive=0.1,
    contrastive_temperature=0.07,
)

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model.load_state_dict(torch.load("checkpoints/stage1/best_model/model.pt"))

trainer = Trainer(model=model, config=config, ...)
trainer.train()
```

### 8.4 é˜¶æ®µ 3: åå¥½å¯¹é½

**ç›®æ ‡**ï¼šè®©æ¨¡å‹åå¥½ç”¨æˆ·é€‰æ‹©çš„ç‰©å“

```python
from algorithm.training import Stage3Config
from algorithm.training.scripts.train_stage3 import DPOTrainer

config = Stage3Config(
    pretrained_model_path="checkpoints/stage2/best_model",
    reference_model_path="checkpoints/stage2/best_model",
    train_data_path="data/preference/train.jsonl",
    output_dir="checkpoints/stage3",
    
    batch_size=128,
    max_epochs=2,
    learning_rate=1e-5,
    
    # DPO å‚æ•°
    lambda_preference=0.1,
    dpo_beta=0.1,
)

# åˆ›å»ºå‚è€ƒæ¨¡å‹ï¼ˆå†»ç»“ï¼‰
reference_model = create_model()
reference_model.load_state_dict(torch.load("checkpoints/stage2/best_model/model.pt"))
reference_model.eval()
for param in reference_model.parameters():
    param.requires_grad = False

trainer = DPOTrainer(
    model=model,
    config=config,
    reference_model=reference_model,
    ...
)
trainer.train()
```

---

## 9. åˆ†å¸ƒå¼è®­ç»ƒ

### 9.1 DDP è®­ç»ƒ

```bash
# å•æœºå¤šå¡
torchrun --nproc_per_node=8 \
    -m algorithm.training.scripts.train_stage1 \
    --config configs/stage1.yaml \
    --ddp

# å¤šæœºå¤šå¡
torchrun --nnodes=2 --nproc_per_node=8 \
    --rdzv_id=job1 --rdzv_backend=c10d --rdzv_endpoint=master:29400 \
    -m algorithm.training.scripts.train_stage1 \
    --config configs/stage1.yaml \
    --ddp
```

### 9.2 ä½¿ç”¨ DistributedTrainer

```python
from algorithm.training import DistributedTrainer, Stage1Config

config = Stage1Config(ddp=True, local_rank=local_rank)

trainer = DistributedTrainer(
    model=model,
    config=config,
    train_dataset=train_dataset,
)

trainer.train()
```

### 9.3 DeepSpeed è®­ç»ƒ

```bash
deepspeed \
    algorithm/training/scripts/train_stage1.py \
    --config configs/stage1.yaml \
    --deepspeed \
    --zero_stage 2
```

```python
from algorithm.training.distributed import DeepSpeedTrainer

trainer = DeepSpeedTrainer(
    model=model,
    config=config,
    train_dataset=train_dataset,
)
trainer.train()
```

### 9.4 DeepSpeed é…ç½®ç¤ºä¾‹

```json
{
    "train_batch_size": 1024,
    "train_micro_batch_size_per_gpu": 256,
    "gradient_accumulation_steps": 4,
    
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 1e-4,
            "weight_decay": 0.01
        }
    },
    
    "fp16": {
        "enabled": true,
        "loss_scale": 0
    },
    
    "zero_optimization": {
        "stage": 2,
        "contiguous_gradients": true,
        "overlap_comm": true
    }
}
```

---

## 10. è¯„ä¼°æŒ‡æ ‡

### 10.1 ç¦»çº¿è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ | è®¡ç®—æ–¹å¼ |
|------|------|----------|
| **Recall@K** | å¬å›ç‡ | å‰ K ä¸ªé¢„æµ‹ä¸­åŒ…å«æ­£ç¡®ç­”æ¡ˆçš„æ¯”ä¾‹ |
| **NDCG@K** | å½’ä¸€åŒ–æŠ˜æŸç´¯è®¡å¢ç›Š | è€ƒè™‘æ­£ç¡®ç­”æ¡ˆä½ç½®çš„åŠ æƒå¾—åˆ† |
| **MRR** | å¹³å‡å€’æ•°æ’å | æ­£ç¡®ç­”æ¡ˆæ’åçš„å€’æ•°å¹³å‡å€¼ |
| **Hit Rate@K** | å‘½ä¸­ç‡ | ç­‰åŒäºå•æ ‡ç­¾çš„ Recall@K |
| **Coverage** | è¦†ç›–ç‡ | è¢«æ¨èç‰©å“å æ€»ç‰©å“çš„æ¯”ä¾‹ |
| **Diversity** | å¤šæ ·æ€§ | æ¨èåˆ—è¡¨å†…ç±»åˆ«çš„å¤šæ ·æ€§ |

### 10.2 ä½¿ç”¨æŒ‡æ ‡è®¡ç®—å™¨

```python
from algorithm.training import MetricsCalculator, recall_at_k, ndcg_at_k, mrr

# ç›´æ¥ä½¿ç”¨å‡½æ•°
predictions = [
    [(1, 1, 1), (2, 2, 2), (3, 3, 3)],  # æ ·æœ¬1çš„é¢„æµ‹
    [(4, 4, 4), (5, 5, 5), (6, 6, 6)],  # æ ·æœ¬2çš„é¢„æµ‹
]
ground_truth = [(1, 1, 1), (5, 5, 5)]

recall_10 = recall_at_k(predictions, ground_truth, k=10)
ndcg_10 = ndcg_at_k(predictions, ground_truth, k=10)
mrr_value = mrr(predictions, ground_truth)

# ä½¿ç”¨æŒ‡æ ‡è®¡ç®—å™¨ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
calculator = MetricsCalculator()
calculator.add_batch(predictions, ground_truth)
metrics = calculator.compute(k_values=[5, 10, 20, 50])
# è¿”å›: {"recall@5", "recall@10", ..., "ndcg@5", ..., "mrr", "gini"}
```

---

## 11. API å‚è€ƒ

### 11.1 é…ç½®ç±»

```python
# åŸºç¡€é…ç½®
TrainingConfig(
    batch_size: int = 256,
    learning_rate: float = 1e-4,
    max_epochs: int = 10,
    ...
)

# é˜¶æ®µé…ç½®
Stage1Config(...)  # é¢„è®­ç»ƒ
Stage2Config(...)  # å¤šä»»åŠ¡å¾®è°ƒ
Stage3Config(...)  # åå¥½å¯¹é½
```

### 11.2 æ•°æ®é›†ç±»

```python
# æ¨èæ•°æ®é›†
RecommendDataset(
    data_path: str,
    max_encoder_length: int = 512,
    max_decoder_length: int = 128,
    pad_token_id: int = 0,
    lazy_loading: bool = False,
)

# åå¥½æ•°æ®é›†
PreferenceDataset(
    data_path: str,
    ...
)

# æ•°æ®æ•´ç†å™¨
DataCollator(
    pad_token_id: int = 0,
    dynamic_padding: bool = True,
)
```

### 11.3 æŸå¤±å‡½æ•°

```python
# NTP æŸå¤±
NextTokenPredictionLoss(l1_weight, l2_weight, l3_weight, ...)
    .forward(l1_logits, l2_logits, l3_logits, labels_l1, labels_l2, labels_l3)
    -> (loss, metrics_dict)

# å¯¹æ¯”å­¦ä¹ æŸå¤±
ContrastiveLoss(temperature=0.07)
    .forward(user_repr, item_repr)
    -> (loss, metrics_dict)

# DPO æŸå¤±
DPOLoss(beta=0.1, reference_free=False)
    .forward(chosen_logps, rejected_logps, ref_chosen_logps, ref_rejected_logps)
    -> (loss, metrics_dict)

# ç»Ÿä¸€æŸå¤±
UnifiedLoss(lambda_contrastive, lambda_preference, lambda_moe_balance, ...)
    .forward(model_outputs, labels, aux_loss)
    -> losses_dict
```

### 11.4 ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨

```python
# åˆ›å»ºä¼˜åŒ–å™¨
create_optimizer(
    model: nn.Module,
    optimizer_type: str = "adamw",
    learning_rate: float = 1e-4,
    weight_decay: float = 0.01,
    ...
) -> Optimizer

# åˆ›å»ºè°ƒåº¦å™¨
create_scheduler(
    optimizer: Optimizer,
    scheduler_type: str = "cosine",
    total_steps: int = 100000,
    warmup_steps: int = 10000,
    ...
) -> LRScheduler
```

### 11.5 è®­ç»ƒå™¨

```python
Trainer(
    model: nn.Module,
    config: TrainingConfig,
    train_dataset: Dataset,
    eval_dataset: Optional[Dataset] = None,
    ...
)

# æ–¹æ³•
.train() -> Dict[str, float]           # å®Œæ•´è®­ç»ƒ
.train_epoch() -> Dict[str, float]     # å• epoch è®­ç»ƒ
.evaluate() -> Dict[str, float]        # éªŒè¯
.save_checkpoint(path: str) -> None    # ä¿å­˜æ£€æŸ¥ç‚¹
.load_checkpoint(path: str) -> None    # åŠ è½½æ£€æŸ¥ç‚¹
```

### 11.6 æ£€æŸ¥ç‚¹ç®¡ç†

```python
CheckpointManager(
    save_dir: str,
    max_checkpoints: int = 3,
    keep_best: bool = True,
)

.save(checkpoint, path, step, is_best) -> str
.load(path) -> Dict
.load_latest() -> Optional[Dict]
.load_best() -> Optional[Dict]
```

### 11.7 è¯„ä¼°æŒ‡æ ‡

```python
# å‡½æ•°
recall_at_k(predictions, ground_truth, k=10) -> float
ndcg_at_k(predictions, ground_truth, k=10) -> float
mrr(predictions, ground_truth) -> float
hit_rate(predictions, ground_truth, k=10) -> float
coverage(predictions, all_items, k=10) -> float

# è®¡ç®—å™¨ç±»
MetricsCalculator()
    .add_batch(predictions, ground_truth)
    .compute(k_values=[5, 10, 20, 50]) -> Dict[str, float]
    .reset()
```

---

## 12. å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•è°ƒæ•´è¶…å‚æ•°ï¼Ÿ

**æ¨èçš„è¶…å‚æ•°æœç´¢é¡ºåº**ï¼š
1. **å­¦ä¹ ç‡**ï¼šå…ˆå›ºå®šå…¶ä»–å‚æ•°ï¼Œæœç´¢ 1e-5 ~ 1e-3
2. **æ‰¹æ¬¡å¤§å°**ï¼šåœ¨ GPU å†…å­˜å…è®¸çš„èŒƒå›´å†…å°½é‡å¤§
3. **æŸå¤±æƒé‡**ï¼šÎ»â‚, Î»â‚‚ é€šå¸¸åœ¨ 0.05 ~ 0.2 ä¹‹é—´
4. **é¢„çƒ­æ­¥æ•°**ï¼šé€šå¸¸ä¸ºæ€»æ­¥æ•°çš„ 5-10%

### Q2: è®­ç»ƒè¿‡ç¨‹ä¸­ loss ä¸ä¸‹é™ï¼Ÿ

**æ’æŸ¥æ­¥éª¤**ï¼š
1. æ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦è¿‡å¤§æˆ–è¿‡å°
2. æ£€æŸ¥æ•°æ®æ˜¯å¦æ­£ç¡®åŠ è½½
3. æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æ­£å¸¸ï¼ˆä½¿ç”¨ `torch.nn.utils.clip_grad_norm_` çš„è¿”å›å€¼ï¼‰
4. æ£€æŸ¥æ˜¯å¦æœ‰ NaN å€¼ï¼ˆç‰¹åˆ«æ˜¯ä½¿ç”¨ FP16 æ—¶ï¼‰

### Q3: GPU å†…å­˜ä¸è¶³ï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. å‡å° `batch_size`
2. å¢å¤§ `gradient_accumulation_steps` ä¿æŒæœ‰æ•ˆæ‰¹æ¬¡å¤§å°
3. ä½¿ç”¨ FP16 æ··åˆç²¾åº¦è®­ç»ƒ
4. ä½¿ç”¨ DeepSpeed ZeRO-2/3

### Q4: å¦‚ä½•ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒï¼Ÿ

```python
config = TrainingConfig(
    resume_from_checkpoint="checkpoints/checkpoint-step-5000",
    ...
)

trainer = Trainer(model=model, config=config, ...)
trainer.train()  # è‡ªåŠ¨ä»æ£€æŸ¥ç‚¹ç»§ç»­
```

### Q5: å¦‚ä½•è¿›è¡Œå¤šæœºå¤šå¡è®­ç»ƒï¼Ÿ

```bash
# æœºå™¨ 1
torchrun --nnodes=2 --node_rank=0 --nproc_per_node=8 \
    --rdzv_backend=c10d --rdzv_endpoint=master_ip:29400 \
    train_stage1.py --ddp

# æœºå™¨ 2
torchrun --nnodes=2 --node_rank=1 --nproc_per_node=8 \
    --rdzv_backend=c10d --rdzv_endpoint=master_ip:29400 \
    train_stage1.py --ddp
```

---

## é™„å½•

### A. ä¾èµ–ç‰ˆæœ¬

```
torch>=2.0.0
numpy>=1.21.0
tqdm>=4.64.0
tensorboard>=2.10.0
pyyaml>=6.0
deepspeed>=0.9.0  # å¯é€‰
```

### B. è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest algorithm/training/tests/ -v

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest algorithm/training/tests/test_training.py::TestLoss -v

# è¿è¡Œå¹¶æ˜¾ç¤ºè¦†ç›–ç‡
pytest algorithm/training/tests/ --cov=algorithm/training --cov-report=html
```

### C. å‚è€ƒèµ„æ–™

- [æ¶æ„æ–‡æ¡£: ç”Ÿæˆå¼æ¨èç³»ç»Ÿæ¶æ„è®¾è®¡](../../docs/ç”Ÿæˆå¼æ¨èç³»ç»Ÿæ¶æ„è®¾è®¡.md)
- [æ¥å£å®šä¹‰: algorithm/interfaces.py](../interfaces.py)
- [ä»»åŠ¡æè¿°: prompts/person_d_training.md](../prompts/person_d_training.md)

---

> ğŸ“ **ç»´æŠ¤è¯´æ˜**  
> æœ¬æ–‡æ¡£éšä»£ç æ›´æ–°åŒæ­¥ç»´æŠ¤ã€‚å¦‚æœ‰é—®é¢˜ï¼Œè¯·è”ç³» Person D æˆ–æäº¤ Issueã€‚

