# æ¨ç†æœåŠ¡æ¨¡å— (Serving)

## ğŸ“‹ æ¦‚è¿°

æœ¬æ¨¡å—è´Ÿè´£å°†è®­ç»ƒå¥½çš„ UGT ç”Ÿæˆå¼æ¨èæ¨¡å‹å¯¼å‡ºä¸ºå¯éƒ¨ç½²çš„é«˜æ€§èƒ½æ¨ç†æœåŠ¡ã€‚

### æ€§èƒ½ç›®æ ‡

| æŒ‡æ ‡ | ç›®æ ‡å€¼ |
|------|--------|
| P99 å»¶è¿Ÿ | < 30ms |
| GPU åˆ©ç”¨ç‡ | > 80% |
| æ”¯æŒåŠ¨æ€ Batching | âœ… |

### éƒ¨ç½²æ¶æ„

```
PyTorch Model â†’ ONNX â†’ TensorRT â†’ Triton Inference Server
```

## ğŸ“ ç›®å½•ç»“æ„

```
algorithm/serving/
â”œâ”€â”€ __init__.py              # æ¨¡å—å¯¼å‡º
â”œâ”€â”€ config.py                # é…ç½®ç±»å®šä¹‰
â”œâ”€â”€ export_onnx.py           # ONNX æ¨¡å‹å¯¼å‡º
â”œâ”€â”€ optimize_trt.py          # TensorRT ä¼˜åŒ–
â”œâ”€â”€ triton_config.py         # Triton é…ç½®ç”Ÿæˆ
â”œâ”€â”€ benchmark.py             # æ€§èƒ½åŸºå‡†æµ‹è¯•
â”œâ”€â”€ exporter.py              # ç»Ÿä¸€å¯¼å‡ºå™¨ï¼ˆå®ç°æ¥å£ï¼‰
â”œâ”€â”€ model_repository/        # Triton æ¨¡å‹ä»“åº“
â”œâ”€â”€ scripts/                 # éƒ¨ç½²è„šæœ¬
â”‚   â”œâ”€â”€ export.sh           # å¯¼å‡ºè„šæœ¬
â”‚   â””â”€â”€ benchmark.sh        # åŸºå‡†æµ‹è¯•è„šæœ¬
â”œâ”€â”€ tests/                   # å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ test_config.py
â”‚   â”œâ”€â”€ test_export_onnx.py
â”‚   â”œâ”€â”€ test_optimize_trt.py
â”‚   â”œâ”€â”€ test_triton_config.py
â”‚   â”œâ”€â”€ test_benchmark.py
â”‚   â””â”€â”€ test_exporter.py
â””â”€â”€ README.md                # æœ¬æ–‡æ¡£
```

## ğŸ”§ æ ¸å¿ƒç»„ä»¶

### 1. ServingExporter (ç»Ÿä¸€æ¥å£)

å®ç° `interfaces.py` ä¸­å®šä¹‰çš„ `ServingExporterInterface`ï¼š

```python
from algorithm.serving import ServingExporter, ExportConfig

# åˆ›å»ºå¯¼å‡ºå™¨
exporter = ServingExporter()

# å¯¼å‡º ONNX æ¨¡å‹
onnx_path = exporter.export_onnx(model, "models/ugt.onnx", config)

# TensorRT ä¼˜åŒ–
engine_path = exporter.optimize_tensorrt(onnx_path, "models/ugt.plan", config)

# ç”Ÿæˆ Triton é…ç½®
config_path = exporter.generate_triton_config("model_repository", config)

# æ€§èƒ½åŸºå‡†æµ‹è¯•
metrics = exporter.benchmark("localhost:8001", "ugt_recommend", num_requests=10000)
```

### 2. ä¾¿æ·å‡½æ•°

```python
from algorithm.serving import (
    export_to_onnx,
    build_trt_engine,
    generate_triton_config,
    run_benchmark,
    create_exporter,
)

# å¿«é€Ÿåˆ›å»ºé…ç½®å¥½çš„å¯¼å‡ºå™¨
exporter = create_exporter(
    model_name="my_model",
    precision="fp16",
    max_batch_size=64
)

# ä¸€é”®å®Œæˆå®Œæ•´éƒ¨ç½²
paths = exporter.deploy_full_pipeline(model, "./model_repository")
```

## âš™ï¸ é…ç½®è¯´æ˜

### ExportConfig

```python
from algorithm.serving import ExportConfig

config = ExportConfig(
    model_name="ugt_recommend",     # æ¨¡å‹åç§°
    precision="fp16",               # ç²¾åº¦: fp32, fp16, int8
    max_batch_size=64,              # æœ€å¤§æ‰¹æ¬¡å¤§å°
    max_seq_length=1024,            # æœ€å¤§åºåˆ—é•¿åº¦
    target_latency_ms=30.0,         # ç›®æ ‡å»¶è¿Ÿ (ms)
    opset_version=17,               # ONNX opset ç‰ˆæœ¬
    workspace_size_gb=4,            # TensorRT å·¥ä½œç©ºé—´ (GB)
)
```

### TritonConfig

```python
from algorithm.serving import TritonConfig

config = TritonConfig(
    platform="tensorrt_plan",       # æ¨ç†å¹³å°
    instance_count=2,               # GPU å®ä¾‹æ•°
    preferred_batch_sizes=(8, 16, 32, 64),  # é¦–é€‰æ‰¹æ¬¡å¤§å°
    max_queue_delay_us=100,         # æœ€å¤§é˜Ÿåˆ—å»¶è¿Ÿ (å¾®ç§’)
    gpus=(0, 1),                    # GPU è®¾å¤‡ ID
)
```

### BenchmarkConfig

```python
from algorithm.serving import BenchmarkConfig

config = BenchmarkConfig(
    triton_url="localhost:8001",    # Triton gRPC URL
    num_warmup_requests=100,        # é¢„çƒ­è¯·æ±‚æ•°
    num_requests=10000,             # æµ‹è¯•è¯·æ±‚æ•°
    concurrency=1,                  # å¹¶å‘æ•°
)
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install torch onnx onnxruntime tensorrt tritonclient[http]
```

### 2. å¯¼å‡ºæ¨¡å‹

```python
from algorithm.serving import ServingExporter, ExportConfig

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
model = load_trained_model("checkpoints/ugt_best.pt")

# é…ç½®
config = ExportConfig(
    model_name="ugt_recommend",
    precision="fp16"
)

# å¯¼å‡º
exporter = ServingExporter(config)
paths = exporter.deploy_full_pipeline(model, "./model_repository")

print(f"ONNX: {paths['onnx_path']}")
print(f"TensorRT: {paths['engine_path']}")
print(f"Config: {paths['config_path']}")
```

### 3. å¯åŠ¨ Triton Server

```bash
docker run --gpus all -p 8000:8000 -p 8001:8001 -p 8002:8002 \
    -v $(pwd)/model_repository:/models \
    nvcr.io/nvidia/tritonserver:24.01-py3 \
    tritonserver --model-repository=/models
```

### 4. è¿è¡ŒåŸºå‡†æµ‹è¯•

```python
from algorithm.serving import run_benchmark

metrics = run_benchmark(
    triton_url="localhost:8001",
    model_name="ugt_recommend",
    num_requests=10000
)

print(f"ååé‡: {metrics['throughput']:.2f} req/s")
print(f"P99 å»¶è¿Ÿ: {metrics['latency_p99']:.2f} ms")
```

## ğŸ§ª è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest algorithm/serving/tests/ -v

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest algorithm/serving/tests/test_exporter.py -v

# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest algorithm/serving/tests/ --cov=algorithm/serving --cov-report=html
```

## ğŸ“ è„šæœ¬ä½¿ç”¨

### export.sh

```bash
# åŸºæœ¬ç”¨æ³•
./scripts/export.sh

# è‡ªå®šä¹‰é…ç½®
MODEL_NAME=my_model PRECISION=fp16 ./scripts/export.sh

# è·³è¿‡æ­¥éª¤
./scripts/export.sh --skip-export --skip-trt

# å¯åŠ¨æœåŠ¡
./scripts/export.sh --start-server
```

### benchmark.sh

```bash
# åŸºæœ¬ç”¨æ³•
./scripts/benchmark.sh

# è‡ªå®šä¹‰é…ç½®
./scripts/benchmark.sh --url localhost:8001 --model ugt_recommend --requests 5000

# ä½¿ç”¨ perf_analyzer
./scripts/benchmark.sh --perf-analyzer
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **åŠ¨æ€å½¢çŠ¶**: ONNX å¯¼å‡ºæ—¶æ­£ç¡®è®¾ç½® `dynamic_axes`ï¼Œæ”¯æŒå¯å˜æ‰¹æ¬¡å’Œåºåˆ—é•¿åº¦
2. **ç²¾åº¦**: FP16 å¯æå‡ 2-3x æ€§èƒ½ï¼Œä½†éœ€éªŒè¯ç²¾åº¦æŸå¤±
3. **æ‰¹å¤„ç†**: Triton åŠ¨æ€æ‰¹å¤„ç†æ˜¯å»¶è¿Ÿä¸ååçš„æƒè¡¡
4. **å†…å­˜**: TensorRT æ„å»ºéœ€è¦è¶³å¤Ÿçš„ GPU å†…å­˜ï¼ˆå»ºè®® >= 8GBï¼‰
5. **ç‰ˆæœ¬**: ç¡®ä¿ TensorRT ç‰ˆæœ¬ä¸ Triton Server å…¼å®¹

## ğŸ“Š æ€§èƒ½å‚è€ƒ

| é…ç½® | ååé‡ (req/s) | P99 å»¶è¿Ÿ (ms) |
|------|---------------|--------------|
| BS=1, FP16 | ~500 | ~15 |
| BS=16, FP16 | ~2000 | ~20 |
| BS=32, FP16 | ~3000 | ~25 |
| BS=64, FP16 | ~4000 | ~30 |

*æµ‹è¯•ç¯å¢ƒ: NVIDIA A100 40GB, åºåˆ—é•¿åº¦ 512*

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [Triton Inference Server æ–‡æ¡£](https://github.com/triton-inference-server/server)
- [TensorRT æ–‡æ¡£](https://developer.nvidia.com/tensorrt)
- [ONNX æ–‡æ¡£](https://onnx.ai/)

