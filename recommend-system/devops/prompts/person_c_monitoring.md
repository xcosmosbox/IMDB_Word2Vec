# Person C: ç›‘æ§å‘Šè­¦ç³»ç»Ÿ

## ä½ çš„è§’è‰²
ä½ æ˜¯ä¸€å DevOps å·¥ç¨‹å¸ˆï¼Œè´Ÿè´£å®ç°ç”Ÿæˆå¼æ¨èç³»ç»Ÿçš„ **ç›‘æ§å‘Šè­¦ç³»ç»Ÿ**ï¼ŒåŒ…æ‹¬ Prometheus è§„åˆ™ã€Grafana ä»ªè¡¨æ¿ã€AlertManager é…ç½®ç­‰ã€‚

---

## âš ï¸ é‡è¦ï¼šæ¥å£é©±åŠ¨å¼€å‘

**å¼€å§‹ç¼–ç å‰ï¼Œå¿…é¡»å…ˆé˜…è¯»æ¥å£å®šä¹‰æ–‡ä»¶ï¼š**

```
devops/interfaces.yaml
```

ä½ éœ€è¦å®ç°çš„å¥‘çº¦ï¼š

```yaml
monitoring:
  metrics:
    go_services:
      - http_requests_total{service, method, path, status}
      - http_request_duration_seconds{service, method, path}
    inference:
      - inference_requests_total{model}
      - inference_latency_seconds{model, batch_size}
  
  alert_rules:
    critical:
      - ServiceDown
      - HighErrorRate
      - HighLatency
```

---

## ä½ çš„ä»»åŠ¡

```
devops/monitoring/
â”œâ”€â”€ prometheus/
â”‚   â”œâ”€â”€ prometheus.yaml
â”‚   â”œâ”€â”€ rules/
â”‚   â”‚   â”œâ”€â”€ recording-rules.yaml
â”‚   â”‚   â””â”€â”€ alerting-rules.yaml
â”‚   â””â”€â”€ scrape-configs/
â”‚       â”œâ”€â”€ kubernetes.yaml
â”‚       â””â”€â”€ custom.yaml
â”œâ”€â”€ grafana/
â”‚   â”œâ”€â”€ provisioning/
â”‚   â”‚   â”œâ”€â”€ datasources/
â”‚   â”‚   â”‚   â””â”€â”€ prometheus.yaml
â”‚   â”‚   â””â”€â”€ dashboards/
â”‚   â”‚       â””â”€â”€ default.yaml
â”‚   â””â”€â”€ dashboards/
â”‚       â”œâ”€â”€ overview.json
â”‚       â”œâ”€â”€ services.json
â”‚       â”œâ”€â”€ inference.json
â”‚       â””â”€â”€ database.json
â””â”€â”€ alertmanager/
    â”œâ”€â”€ alertmanager.yaml
    â””â”€â”€ templates/
        â””â”€â”€ slack.tmpl
```

---

## 1. Prometheus é…ç½® (prometheus/prometheus.yaml)

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'recommend-prod'
    env: 'production'

rule_files:
  - /etc/prometheus/rules/*.yaml

alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager:9093

scrape_configs:
  # Kubernetes æœåŠ¡å‘ç°
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: pod

  # Go æœåŠ¡
  - job_name: 'recommend-service'
    static_configs:
      - targets: ['recommend-service:9091']
    metrics_path: /metrics

  # Python æ¨ç†æœåŠ¡
  - job_name: 'ugt-inference'
    static_configs:
      - targets: ['ugt-inference:9094']
    metrics_path: /metrics

  # Redis
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']

  # PostgreSQL
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']

  # Milvus
  - job_name: 'milvus'
    static_configs:
      - targets: ['milvus:9091']
```

---

## 2. å‘Šè­¦è§„åˆ™ (prometheus/rules/alerting-rules.yaml)

```yaml
groups:
  # ==========================================================================
  # æœåŠ¡å¯ç”¨æ€§å‘Šè­¦
  # ==========================================================================
  - name: service-availability
    rules:
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "æœåŠ¡ {{ $labels.job }} ä¸å¯ç”¨"
          description: "æœåŠ¡ {{ $labels.instance }} å·²åœæ­¢å“åº”è¶…è¿‡ 1 åˆ†é’Ÿ"
          runbook_url: "https://wiki.example.com/runbooks/service-down"

      - alert: HighErrorRate
        expr: |
          sum(rate(http_requests_total{status=~"5.."}[5m])) by (service)
          /
          sum(rate(http_requests_total[5m])) by (service)
          > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "æœåŠ¡ {{ $labels.service }} é”™è¯¯ç‡è¿‡é«˜"
          description: "é”™è¯¯ç‡ {{ $value | humanizePercentage }} è¶…è¿‡ 5%"

      - alert: HighLatency
        expr: |
          histogram_quantile(0.99, 
            sum(rate(http_request_duration_seconds_bucket[5m])) by (le, service)
          ) > 0.5
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "æœåŠ¡ {{ $labels.service }} P99 å»¶è¿Ÿè¿‡é«˜"
          description: "P99 å»¶è¿Ÿ {{ $value | humanizeDuration }} è¶…è¿‡ 500ms"

  # ==========================================================================
  # æ¨ç†æœåŠ¡å‘Šè­¦
  # ==========================================================================
  - name: inference-alerts
    rules:
      - alert: InferenceLatencyHigh
        expr: |
          histogram_quantile(0.95, 
            sum(rate(inference_latency_seconds_bucket[5m])) by (le, model)
          ) > 0.2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "æ¨¡å‹ {{ $labels.model }} æ¨ç†å»¶è¿Ÿè¿‡é«˜"
          description: "P95 æ¨ç†å»¶è¿Ÿ {{ $value | humanizeDuration }}"

      - alert: GPUMemoryHigh
        expr: gpu_memory_usage_bytes / gpu_memory_total_bytes > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "GPU å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"
          description: "GPU {{ $labels.device }} å†…å­˜ä½¿ç”¨ {{ $value | humanizePercentage }}"

      - alert: ModelLoadFailed
        expr: increase(model_load_errors_total[5m]) > 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "æ¨¡å‹åŠ è½½å¤±è´¥"
          description: "æ¨¡å‹ {{ $labels.model }} åŠ è½½å¤±è´¥"

  # ==========================================================================
  # èµ„æºä½¿ç”¨å‘Šè­¦
  # ==========================================================================
  - name: resource-alerts
    rules:
      - alert: HighMemoryUsage
        expr: |
          container_memory_usage_bytes / container_spec_memory_limit_bytes > 0.8
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "å®¹å™¨å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"
          description: "Pod {{ $labels.pod }} å†…å­˜ä½¿ç”¨ {{ $value | humanizePercentage }}"

      - alert: HighCPUUsage
        expr: |
          sum(rate(container_cpu_usage_seconds_total[5m])) by (pod)
          /
          sum(container_spec_cpu_quota/container_spec_cpu_period) by (pod)
          > 0.8
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "å®¹å™¨ CPU ä½¿ç”¨ç‡è¿‡é«˜"
          description: "Pod {{ $labels.pod }} CPU ä½¿ç”¨ {{ $value | humanizePercentage }}"

      - alert: PodCrashLooping
        expr: increase(kube_pod_container_status_restarts_total[1h]) > 3
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Pod é¢‘ç¹é‡å¯"
          description: "Pod {{ $labels.pod }} åœ¨è¿‡å» 1 å°æ—¶é‡å¯ {{ $value }} æ¬¡"

  # ==========================================================================
  # æ•°æ®åº“å‘Šè­¦
  # ==========================================================================
  - name: database-alerts
    rules:
      - alert: PostgresDown
        expr: pg_up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "PostgreSQL ä¸å¯ç”¨"
          description: "PostgreSQL å®ä¾‹ {{ $labels.instance }} å·²åœæ­¢å“åº”"

      - alert: PostgresHighConnections
        expr: |
          pg_stat_database_numbackends / pg_settings_max_connections > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "PostgreSQL è¿æ¥æ•°è¿‡é«˜"
          description: "è¿æ¥ä½¿ç”¨ç‡ {{ $value | humanizePercentage }}"

      - alert: RedisDown
        expr: redis_up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Redis ä¸å¯ç”¨"

      - alert: RedisHighMemory
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Redis å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"

  # ==========================================================================
  # ä¸šåŠ¡æŒ‡æ ‡å‘Šè­¦
  # ==========================================================================
  - name: business-alerts
    rules:
      - alert: LowRecommendationCTR
        expr: |
          sum(rate(recommendation_clicks_total[1h]))
          /
          sum(rate(recommendation_impressions_total[1h]))
          < 0.01
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "æ¨èç‚¹å‡»ç‡è¿‡ä½"
          description: "CTR {{ $value | humanizePercentage }} ä½äº 1%"

      - alert: HighCacheHitMiss
        expr: |
          sum(rate(cache_misses_total[5m]))
          /
          sum(rate(cache_requests_total[5m]))
          > 0.3
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "ç¼“å­˜å‘½ä¸­ç‡è¿‡ä½"
          description: "ç¼“å­˜æœªå‘½ä¸­ç‡ {{ $value | humanizePercentage }}"
```

---

## 3. AlertManager é…ç½® (alertmanager/alertmanager.yaml)

```yaml
global:
  resolve_timeout: 5m
  slack_api_url: 'https://hooks.slack.com/services/xxx/xxx/xxx'

route:
  receiver: 'default'
  group_by: ['alertname', 'severity', 'service']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h
  
  routes:
    # Critical å‘Šè­¦ - ç«‹å³é€šçŸ¥
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 10s
      repeat_interval: 1h
    
    # Warning å‘Šè­¦
    - match:
        severity: warning
      receiver: 'warning-alerts'
      repeat_interval: 4h
    
    # ä¸šåŠ¡å‘Šè­¦ - å‘é€åˆ°ä¸šåŠ¡é¢‘é“
    - match_re:
        alertname: 'Low.*CTR|High.*CacheHitMiss'
      receiver: 'business-alerts'

receivers:
  - name: 'default'
    slack_configs:
      - channel: '#alerts'
        send_resolved: true
        title: '{{ template "slack.title" . }}'
        text: '{{ template "slack.text" . }}'

  - name: 'critical-alerts'
    slack_configs:
      - channel: '#alerts-critical'
        send_resolved: true
        title: 'ğŸš¨ {{ template "slack.title" . }}'
        text: '{{ template "slack.text" . }}'
    pagerduty_configs:
      - service_key: '<pagerduty-service-key>'
        severity: critical

  - name: 'warning-alerts'
    slack_configs:
      - channel: '#alerts-warning'
        send_resolved: true

  - name: 'business-alerts'
    slack_configs:
      - channel: '#business-metrics'
        send_resolved: true

inhibit_rules:
  # å¦‚æœæœåŠ¡ Downï¼ŒæŠ‘åˆ¶å…¶ä»–å‘Šè­¦
  - source_match:
      alertname: 'ServiceDown'
    target_match_re:
      alertname: '.*'
    equal: ['service']
```

---

## 4. Grafana Dashboard - ç³»ç»Ÿæ€»è§ˆ (grafana/dashboards/overview.json)

```json
{
  "dashboard": {
    "title": "æ¨èç³»ç»Ÿ - æ€»è§ˆ",
    "uid": "recommend-overview",
    "timezone": "browser",
    "refresh": "30s",
    "panels": [
      {
        "title": "æœåŠ¡çŠ¶æ€",
        "type": "stat",
        "gridPos": { "x": 0, "y": 0, "w": 6, "h": 4 },
        "targets": [
          {
            "expr": "sum(up{job=~\"recommend.*\"})",
            "legendFormat": "åœ¨çº¿æœåŠ¡æ•°"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "thresholds": {
              "steps": [
                { "value": 0, "color": "red" },
                { "value": 3, "color": "yellow" },
                { "value": 5, "color": "green" }
              ]
            }
          }
        }
      },
      {
        "title": "è¯·æ±‚é€Ÿç‡ (QPS)",
        "type": "stat",
        "gridPos": { "x": 6, "y": 0, "w": 6, "h": 4 },
        "targets": [
          {
            "expr": "sum(rate(http_requests_total[5m]))",
            "legendFormat": "QPS"
          }
        ]
      },
      {
        "title": "P99 å»¶è¿Ÿ",
        "type": "stat",
        "gridPos": { "x": 12, "y": 0, "w": 6, "h": 4 },
        "targets": [
          {
            "expr": "histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))",
            "legendFormat": "P99"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "s",
            "thresholds": {
              "steps": [
                { "value": 0, "color": "green" },
                { "value": 0.2, "color": "yellow" },
                { "value": 0.5, "color": "red" }
              ]
            }
          }
        }
      },
      {
        "title": "é”™è¯¯ç‡",
        "type": "stat",
        "gridPos": { "x": 18, "y": 0, "w": 6, "h": 4 },
        "targets": [
          {
            "expr": "sum(rate(http_requests_total{status=~\"5..\"}[5m])) / sum(rate(http_requests_total[5m]))",
            "legendFormat": "Error Rate"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percentunit",
            "thresholds": {
              "steps": [
                { "value": 0, "color": "green" },
                { "value": 0.01, "color": "yellow" },
                { "value": 0.05, "color": "red" }
              ]
            }
          }
        }
      },
      {
        "title": "è¯·æ±‚è¶‹åŠ¿",
        "type": "timeseries",
        "gridPos": { "x": 0, "y": 4, "w": 12, "h": 8 },
        "targets": [
          {
            "expr": "sum(rate(http_requests_total[5m])) by (service)",
            "legendFormat": "{{ service }}"
          }
        ]
      },
      {
        "title": "å»¶è¿Ÿåˆ†å¸ƒ",
        "type": "timeseries",
        "gridPos": { "x": 12, "y": 4, "w": 12, "h": 8 },
        "targets": [
          {
            "expr": "histogram_quantile(0.50, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))",
            "legendFormat": "P50"
          },
          {
            "expr": "histogram_quantile(0.90, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))",
            "legendFormat": "P90"
          },
          {
            "expr": "histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))",
            "legendFormat": "P99"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "s"
          }
        }
      }
    ]
  }
}
```

---

## æ³¨æ„äº‹é¡¹

1. å‘Šè­¦è§„åˆ™è¦æœ‰åˆç†çš„é˜ˆå€¼
2. é…ç½®å‘Šè­¦æŠ‘åˆ¶é¿å…å‘Šè­¦é£æš´
3. Dashboard è¦æœ‰æ¸…æ™°çš„å±‚æ¬¡
4. è®°å½•è§„åˆ™æé«˜æŸ¥è¯¢æ€§èƒ½
5. é…ç½®åˆç†çš„æ•°æ®ä¿ç•™ç­–ç•¥

## è¾“å‡ºè¦æ±‚

è¯·è¾“å‡ºå®Œæ•´çš„ç›‘æ§é…ç½®ï¼ŒåŒ…å«ï¼š
1. Prometheus å®Œæ•´é…ç½®
2. å‘Šè­¦è§„åˆ™
3. AlertManager é…ç½®
4. Grafana Dashboard JSON

