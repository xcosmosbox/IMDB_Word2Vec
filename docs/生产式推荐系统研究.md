# 引言

## 引言

推荐系统作为现代互联网产品的核心技术组件，其架构设计直接决定了用户体验和商业价值。传统推荐系统长期采用 "双塔 + 召回 + 精排 + 重排" 的级联架构，通过分层处理实现效率与效果的平衡。然而，随着大语言模型时代的到来，基于 Transformer 的生成式推荐系统正在引发一场深刻的范式变革。

**生成式推荐系统**将推荐任务重新定义为序列生成问题，利用 Transformer 的自回归机制直接生成用户可能感兴趣的物品序列[(15)](https://d197for5662m48.cloudfront.net/documents/publicationstatus/294826/preprint_pdf/52b722a0aa98a01329d60832ed81aa16.pdf)。这种新范式与传统的判别式方法存在根本性差异：传统方法从固定候选集中选择最佳项，而生成式方法能够直接从全量物品库中生成推荐结果[(125)](https://github.com/AkihikoWatanabe/paper_notes/issues/1344)。在这一背景下，一个关键问题浮现：生成式推荐系统是否还需要保留传统架构的核心组件？

本研究旨在深入分析基于 Transformer 的生成式推荐系统与传统推荐架构的关系，通过技术原理对比、性能表现分析和工程实践考量，为推荐系统的架构设计提供决策依据。研究将重点关注以下核心问题：生成式推荐系统的技术本质是什么？传统架构组件在新范式下的作用是否发生变化？如何设计适合不同场景的架构融合策略？

## 一、传统推荐系统架构的核心逻辑与技术特征

### 1.1 多阶段级联架构的设计哲学

传统推荐系统采用多阶段级联架构，主要包含召回、粗排、精排和重排四个核心阶段，形成了一个逐层过滤和优化的漏斗结构[(35)](https://juejin.cn/post/7560517862835470345)。这种架构设计的根本动机在于**计算资源的分级分配**，通过不同复杂度的模型处理不同规模的候选集，实现效率与效果的平衡。

**召回层**负责从百万甚至亿级的全量物品库中快速筛选出几百到几千个候选集，重点关注速度和覆盖度。典型的召回方法包括协同过滤、向量检索、多路召回策略等，通过双塔模型和近似最近邻搜索（如 Faiss）实现百毫秒内的候选集筛选[(31)](https://blog.csdn.net/weixin_73958875/article/details/150381655)。双塔架构将用户特征和物品特征分别输入两个独立的神经网络（用户塔和物品塔），将它们映射到相同的低维特征空间，通过向量点积或余弦相似度计算匹配度[(58)](https://blog.csdn.net/weixin_42001184/article/details/145974412)。

**粗排层**作为召回与精排之间的过渡，使用相对轻量的模型（如简化的浅层神经网络或逻辑回归）将几千个候选集进一步筛选到几百个。这一层的设计哲学是 "保留大部分有潜力的候选，同时砍掉明显不合适的"，通过简单特征和模型预估点击率，平衡精度和性能开销。

**精排层**是整个链路的核心竞争力所在，使用复杂的深度神经网络、大量特征交叉、多目标学习等技术，将几百个候选按预估效果进行精准排序。精排模型通常采用 DIN（深度兴趣网络）、Wide\&Deep、DCN（深度交叉网络）等架构，能够处理数百维甚至上千维的特征，包括用户长期兴趣、实时行为、物品多模态特征等。

**重排层**在精排基础上考虑全局性约束，不再只看单个物品分数，而是处理多样性、去重、打散同类内容、插入运营位等策略需求。这一层确保最终展示列表既符合用户体验又满足业务目标，通过规则或机器学习方法优化整个列表的质量。

### 1.2 双塔架构的技术原理与工程价值

双塔架构是传统推荐系统召回阶段的主流技术，其核心思想是**特征分治与向量化匹配**[(58)](https://blog.csdn.net/weixin_42001184/article/details/145974412)。用户塔和物品塔分别处理用户特征和物品特征，将它们映射到相同的低维特征空间（如 128 维），通过向量点积或余弦相似度计算匹配度，得分高的物品被推荐给用户。

这种架构的技术优势体现在三个方面：首先是**计算效率**，用户塔和物品塔可以离线训练，在线服务时只需计算用户向量，然后与所有物品向量进行批量相似度计算；其次是**可扩展性**，物品向量可以预先计算并存储，支持快速更新和增量维护；最后是**工程友好性**，双塔架构支持分布式部署和水平扩展，能够处理大规模物品库的实时检索需求。

然而，双塔架构也存在固有限制。由于用户塔和物品塔在训练时相互独立，缺乏深度交互，模型难以捕捉复杂的用户 - 物品交互模式[(60)](http://m.toutiao.com/group/7583727814915015174/?upstream_biz=doubao)。这种设计虽然训练速度快，适合大规模检索场景，但在个性化推荐精度上存在天花板。

### 1.3 传统架构面临的技术挑战

传统级联架构在面对现代推荐场景时面临多重挑战。首先是**阶段间的目标不一致问题**，各阶段独立运行，上一阶段的推荐效果是下一阶段推荐效果的上限，制约了整体性能的提升。召回阶段追求覆盖率，精排阶段追求准确率，两者的优化目标存在天然冲突，导致误差在级联过程中不断累积。

其次是**特征工程的复杂性**。传统推荐系统依赖大量手工设计的特征和复杂的特征交叉，不仅增加了系统的维护成本，也限制了模型的泛化能力[(2)](https://arxiv.org/pdf/2507.06507)。特征工程需要领域知识和经验积累，难以快速适应新的业务场景和数据变化。

第三是**扩展性瓶颈**。随着物品库规模的增长和用户行为的复杂化，传统架构的计算成本呈线性增长，难以满足实时推荐的延迟要求。特别是在面对海量候选集时，精排模型的推理时间可能从可接受的几十毫秒飙升到几百毫秒，严重影响用户体验。

## 二、生成式推荐系统的技术范式与架构革新

### 2.1 生成式推荐的技术本质

生成式推荐系统代表了推荐技术的**范式革新**，将推荐任务从传统的 "判别式" 方法转变为 "生成式" 方法[(125)](https://github.com/AkihikoWatanabe/paper_notes/issues/1344)。这种转变的核心在于将推荐问题重新定义为序列生成任务，利用 Transformer 的自回归机制直接生成用户可能感兴趣的物品序列，而不是从固定候选集中选择最佳项[(15)](https://d197for5662m48.cloudfront.net/documents/publicationstatus/294826/preprint_pdf/52b722a0aa98a01329d60832ed81aa16.pdf)。

生成式推荐的技术基础是**语义 ID（Semantic ID）编码**。通过向量量化技术（如 RQ-VAE）将物品编码为语义有意义的 token 序列，将用户历史行为和物品都表示为统一的 token 格式，然后使用自回归 Transformer 模型学习用户行为序列模式，直接生成目标物品的语义 ID 序列[(118)](http://dev.guyuehome.com/wap/detail?id=1986048319400472577)。这种方法突破了传统推荐系统对离散物品 ID 的依赖，能够处理大规模、高维的物品特征。

**Transformer 架构**在生成式推荐中发挥着核心作用。相比传统的 RNN 或 LSTM 架构，Transformer 通过多头自注意力机制能够更好地捕获用户序列中的长程依赖关系，支持并行计算，显著提升训练效率。特别是在处理长序列时，Transformer 的注意力机制能够直接建模任意两个位置之间的关系，而无需像 RNN 那样顺序处理，这对于理解用户的复杂行为模式具有重要意义。

### 2.2 代表性生成式推荐架构

近年来，工业界和学术界提出了多个具有代表性的生成式推荐架构，它们在技术路线和应用场景上各有特色。

\*\*HSTU（Hierarchical Sequential Transduction Unit）\*\* 是 Meta 提出的万亿参数级生成式推荐模型，将推荐问题重构为序列转换任务，采用专门设计的分层架构处理高基数、非平稳的推荐数据[(18)](https://icml.cc/media/icml-2024/Slides/32684.pdf?ref=https%3A%2F%2Fgithubhelp.com)。HSTU 通过点积聚合注意力机制替代传统的 softmax 注意力，能够更好地处理非平稳词汇表，并通过 M-FALCON 算法实现推理加速，在 8192 长度序列上比 FlashAttention2 快 5.3-15.2 倍。

**OneRec**是快手推出的端到端生成式推荐系统，采用 Encoder-Decoder 架构，将推荐问题转化为序列生成任务[(17)](https://arxiv.org/pdf/2502.18965)。OneRec 的创新之处在于引入了基于奖励机制的偏好对齐方法，借助强化学习增强模型效果，在训练过程中使用 NTP（Next Token Prediction）损失函数优化。该系统在快手主场景部署后，实现了 1.6% 的观看时长增长[(17)](https://arxiv.org/pdf/2502.18965)。

**TIGER**提出了生成式检索方法，通过量化技术为每个物品创建语义有意义的码字元组作为语义 ID，使用基于 Transformer 的序列到序列模型预测用户将交互的下一个物品的语义 ID[(19)](https://proceedings.neurips.cc/paper_files/paper/2023/file/20dcab0f14046a5c6b02b61da9f13229-Paper-Conference.pdf)。TIGER 的核心贡献在于语义 ID 的设计，通过层次化的语义编码减少词汇表大小，提升生成效率。

**MTGR**是美团提出的工业级生成式推荐框架，基于 HSTU 架构但保留了传统 DLRM 的特征体系，包括交叉特征等，实现了生成式架构与传统特征工程的有机结合[(27)](https://arxiv.org/pdf/2505.18654)。MTGR 通过用户级压缩实现训练和推理加速，并提出了 Group-Layer Normalization（GLN）来增强不同语义空间的编码性能。

### 2.3 生成式推荐的技术优势

生成式推荐系统相比传统方法具有多方面的技术优势。首先是**可解释性提升**，生成式模型能够通过生成过程解释推荐理由，增强用户信任并促进反馈循环[(2)](https://arxiv.org/pdf/2507.06507)。系统可以生成自然语言解释，说明为什么推荐某个物品，这对于提升用户体验和建立信任关系具有重要价值。

其次是**创新性和多样性**，生成式模型能够推荐超出基于过去行为的可预测范围的物品，帮助用户发现新兴趣[(2)](https://arxiv.org/pdf/2507.06507)。传统推荐系统往往局限于相似物品的推荐，容易形成 "信息茧房"，而生成式模型通过学习更丰富的语义表示，能够推荐具有创新性和多样性的内容。

第三是**架构简化**，统一的语言模型方法减少了对复杂手工特征工程和不同任务独立模块的需求[(2)](https://arxiv.org/pdf/2507.06507)。生成式架构通过统一的 token 表示和自回归生成机制，避免了传统系统中复杂的特征处理流程，降低了系统的维护成本。

最后是**规模扩展能力**，研究表明生成式推荐模型的质量随训练计算量呈幂律增长，覆盖三个数量级，达到 GPT-3/LLaMA-2 的规模[(18)](https://icml.cc/media/icml-2024/Slides/32684.pdf?ref=https%3A%2F%2Fgithubhelp.com)。这种扩展定律为推荐系统的性能提升提供了理论指导，表明通过增加模型规模和训练数据，可以持续提升推荐效果。

## 三、传统架构组件在生成式推荐中的作用分析

### 3.1 双塔架构的必要性评估

在生成式推荐系统中，**双塔架构的必要性发生了根本性变化**。传统双塔架构的核心价值在于解决大规模物品库的高效检索问题，通过预计算物品向量实现快速相似度匹配。然而，生成式推荐系统采用了完全不同的技术路径，通过语义 ID 和自回归生成机制，从根本上改变了物品表示和匹配方式。

生成式推荐系统通常采用**统一的特征空间**，将用户行为、物品 ID、属性等所有异构特征都转化为时间序列上的事件（Event）[(104)](https://blog.csdn.net/lifallen/article/details/151119541)。在这种架构下，用户和物品的表示在生成过程中是动态计算的，不需要像传统双塔那样预先计算和存储物品向量。因此，从技术原理上看，生成式推荐系统**不再需要传统意义上的双塔架构**。

然而，在某些特定场景下，双塔架构的思想仍有借鉴价值。例如，在**混合架构设计**中，可以将 LLMs 生成的表征作为补充特征融入传统推荐系统[(115)](https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/149410220)。快手的 LEARN、字节跳动的 HLLM 和美团的 SRP4CTR 等系统都采用了这种混合策略，既保留了生成式模型的语义理解能力，又利用了传统双塔架构的工程优势。

### 3.2 召回层的功能替代与演进

生成式推荐系统对召回层的影响是**颠覆性的**。传统召回层的核心功能是从海量物品库中快速筛选候选集，而生成式系统通过自回归生成机制可以直接从全量物品库中生成推荐结果，理论上可以完全替代召回层的功能[(125)](https://github.com/AkihikoWatanabe/paper_notes/issues/1344)。

快手的 OneRec 系统就是一个典型案例，它**完全摒弃了传统的 "召回 - 粗排 - 精排 - 重排" 多阶段流水线模式**，通过统一的生成式框架将整个推荐过程建模为序列生成任务[(98)](https://wenku.csdn.net/doc/58qpiu2980)。OneRec 采用 Encoder-Decoder 架构，编码器整合用户终身 / 短期行为序列实现多尺度建模，解码器基于 MoE（Mixture-of-Experts）结构通过 Next Token Prediction 生成推荐结果[(90)](https://aiguide.cc/17719/)。

然而，在**工业级大规模场景**中，完全抛弃召回层可能面临效率挑战。生成式模型虽然能够直接生成推荐结果，但在处理亿级物品库时，每次生成都需要遍历整个物品空间，计算成本仍然很高。因此，一些系统采用了**分层生成策略**，如 COBRA 框架通过级联过程创新地整合稀疏语义 ID 和稠密向量，先生成稀疏 ID 作为条件辅助生成稠密向量[(113)](https://arxiv.org/pdf/2503.02453v1)。

### 3.3 精排层的融合与重构

精排层在生成式推荐系统中的角色发生了**根本性转变**。传统精排层的目标是对候选物品进行精准排序，而生成式系统的排序能力直接集成在生成过程中。通过自回归生成机制，模型在生成推荐序列时已经隐含了排序逻辑，概率最高的物品会被优先生成。

研究表明，生成式推荐的效果提升主要来源于**生成式架构本身**，而非训练范式。这意味着生成式架构在建模用户偏好和物品关系方面具有天然优势，能够更好地捕获复杂的交互模式。例如，HSTU 通过点积聚合注意力机制，能够直接建模用户行为序列与物品之间的复杂关系，无需像传统方法那样依赖大量特征工程。

在**工程实现**上，生成式精排面临的主要挑战是推理效率。由于需要自回归生成推荐序列，每次生成都需要多步计算，延迟可能比传统精排高。为此，研究者提出了多种优化方法：



1. **序列压缩技术**：如小红书的 GenRank、美团的 DFGR、快手的 KuaiFormer 通过缩短序列长度降低计算成本

2. **模型结构优化**：Meta 的 HSTU 和美团 RecFormer 改进注意力机制，将复杂度从二次降低到线性

3. **专用加速技巧**：谷歌的 TIGER 生成语义 ID 减少词表大小，HSTU 的 M-FALCON 通过掩码策略提升排序效率

### 3.4 重排层的保留与增强

重排层在生成式推荐系统中通常**需要保留**，但其功能和实现方式发生了变化。传统重排层的主要任务是处理多样性、去重、打散同类内容等全局约束，这些需求在生成式系统中仍然存在。

快手 OneRec 的实践表明，生成式推荐系统通过引入统一的生成式框架，将推荐过程建模为序列生成任务，根据用户历史行为序列自回归地生成感兴趣的物品序列[(120)](https://blog.csdn.net/weixin_46351593/article/details/147277623)。在这种架构下，重排层的功能可以通过**生成过程中的约束机制**来实现，例如在生成过程中加入多样性约束、去重规则等。

一些研究探索了基于**强化学习的重排方法**，将重排建模为序列决策问题，用强化学习优化整个列表的长期收益。虽然这种方法工程复杂度高，但能够更好地平衡相关性和多样性，特别适合需要复杂业务规则的场景。

## 四、生成式推荐系统架构设计的策略选择

### 4.1 完全替代策略：端到端生成架构

**完全替代策略**代表了生成式推荐系统的理想形态，通过端到端的生成架构完全取代传统的多阶段级联架构。这种策略的核心思想是将推荐任务彻底重构为序列生成问题，利用生成式模型的强大能力实现全链路的统一优化。

快手的 OneRec 系统是这一策略的典型代表。OneRec 采用 Encoder-Decoder 架构，将推荐问题转化为序列生成任务，在训练过程中使用 NTP 损失函数优化[(86)](https://juejin.cn/post/7517852192637485108)。该系统的技术创新包括：



1. **协同感知多模态分词器**：融合视频标题、图像等多维信息与用户行为，利用 RQ-Kmeans 分层生成语义 ID[(88)](https://www.csdn.net/article/2025-06-23/148843176)

2. **多尺度用户行为建模**：编码器整合用户终身 / 短期行为序列，实现多尺度建模[(90)](https://aiguide.cc/17719/)

3. **MoE 增强解码器**：解码器基于 Mixture-of-Experts 结构，通过 Next Token Prediction 生成推荐结果[(90)](https://aiguide.cc/17719/)

OneRec 的实践效果证明了完全替代策略的可行性：在快手主场景部署后，实现了 1.6% 的观看时长增长，服务成本降至原系统的 1/10[(17)](https://arxiv.org/pdf/2502.18965)。这一成果表明，生成式架构不仅能够在效果上超越传统系统，还能在成本上实现显著优化。

**技术优势**体现在多个方面：首先是消除了传统多阶段架构的目标不一致问题，召回追求覆盖率，精排追求准确率的冲突不复存在；其次是简化了系统架构，减少了组件间的协调成本；最后是提升了模型的表达能力，能够捕获更复杂的用户行为模式。

然而，完全替代策略也面临**技术挑战**：



1. **推理延迟**：自回归生成过程的计算复杂度较高，可能影响实时推荐的响应时间

2. **冷启动问题**：对于新用户或新物品，生成式模型可能缺乏足够的训练数据

3. **可解释性**：虽然生成式模型理论上具有更好的可解释性，但在实际应用中仍需要设计合适的解释机制

4. **工程复杂度**：端到端架构需要重新设计整个推荐链路，包括数据处理、模型训练、在线服务等各个环节

### 4.2 部分融合策略：混合架构设计

**部分融合策略**是当前工业界采用较多的过渡方案，通过在传统架构中引入生成式能力，实现渐进式的技术升级。这种策略的核心思想是保留传统架构的工程优势，同时利用生成式模型的技术创新。

美团的 MTGR 系统采用了典型的混合架构设计，结合传统 DLRM 模型特征体系，保留了全部 DLRM 原始特征（包括交叉特征），采用 Group LayerNorm 和动态混合掩码策略，用统一的 HSTU 架构针对不同语义空间的 Token 信息进行编码[(102)](https://juejin.cn/post/7508029170229035071)。这种设计的优势在于：



1. **兼容性**：能够与现有系统无缝集成，保护已有技术投资

2. **渐进性**：可以逐步增加生成式组件的比重，降低技术迁移风险

3. **灵活性**：可以根据不同场景选择不同的技术组合

混合架构的实现方式主要有两种：

**加权混合**：通过线性组合的方式结合多种推荐技术的输出，允许根据性能指标或上下文因素自适应调整各组件的影响[(110)](https://journalwjaets.com/sites/default/files/fulltext_pdf/WJAETS-2025-0998.pdf)。例如，可以将生成式模型的输出和传统模型的输出进行加权融合，根据实时效果动态调整权重。

**级联混合**：实现分阶段过滤过程，二级推荐器优化主系统产生的候选集[(110)](https://journalwjaets.com/sites/default/files/fulltext_pdf/WJAETS-2025-0998.pdf)。例如，可以先用传统方法进行召回，然后用生成式模型对候选集进行精排，充分发挥各自的优势。

### 4.3 渐进迁移策略：分阶段实施路径

**渐进迁移策略**适合已有成熟推荐系统的企业，通过分阶段的方式逐步引入生成式能力，最终实现架构升级。这种策略需要在保持系统稳定性的前提下，逐步提升推荐效果。

迁移路径可以设计为以下几个阶段：

**第一阶段：生成式组件试点**。在现有系统中选择部分场景或流量进行生成式模型的试点，例如在召回阶段引入生成式模型作为其中一路召回，或者在精排阶段使用生成式模型作为备选方案。这一阶段的目标是验证技术可行性，积累实践经验。

**第二阶段：混合架构部署**。在试点成功的基础上，逐步扩大生成式组件的应用范围，形成混合架构。可以采用 A/B 测试的方式对比不同架构的效果，根据业务指标逐步调整生成式组件的权重。

**第三阶段：全链路升级**。在充分验证生成式架构效果的基础上，逐步替换传统组件，最终实现全链路的生成式架构。这一阶段需要特别注意系统的稳定性和可维护性，确保业务连续性。

**技术考量**在渐进迁移过程中至关重要：



1. **数据兼容性**：生成式模型需要大量的序列数据进行训练，需要评估现有数据是否满足要求

2. **模型可扩展性**：生成式模型通常需要更大的计算资源，需要评估基础设施的承载能力

3. **人员技能**：生成式推荐涉及新的技术栈，需要对技术团队进行培训

4. **业务指标监控**：需要建立完善的监控体系，及时发现和解决迁移过程中的问题

### 4.4 架构选择的决策框架

基于以上分析，我们可以构建一个**架构选择决策框架**，帮助企业根据自身情况选择合适的技术路线：



| 决策因素      | 完全替代策略       | 部分融合策略         | 渐进迁移策略       |
| --------- | ------------ | -------------- | ------------ |
| **技术成熟度** | 高（已有成功案例）    | 中高（混合架构）       | 高（风险可控）      |
| **业务规模**  | 适合大规模场景      | 适合各种规模         | 适合所有规模       |
| **技术储备**  | 需要较强的生成式技术能力 | 需要传统 + 生成式技术能力 | 可以逐步建立技术能力   |
| **风险承受度** | 高（颠覆性创新）     | 中（渐进式改进）       | 低（分阶段实施）     |
| **成本投入**  | 初期投入大，长期成本低  | 中等投入           | 分阶段投入        |
| **实施周期**  | 较长（6-12 个月）  | 中等（3-6 个月）     | 较长（12-24 个月） |

**选择建议**：



1. **大型互联网平台**（如快手、美团、字节跳动）：建议采用完全替代策略或激进的混合架构，充分发挥生成式技术的优势，实现技术领先

2. **中型企业**：建议采用部分融合策略，在关键环节引入生成式技术，平衡效果提升和风险控制

3. **传统企业**：建议采用渐进迁移策略，从试点开始，逐步积累经验，最终实现架构升级

4. **技术资源有限的企业**：可以考虑使用开源的生成式推荐框架（如 MiniOneRec），降低技术门槛

## 五、工程实践中的关键技术考虑

### 5.1 推理效率优化

生成式推荐系统在工业部署中面临的最大挑战之一是**推理效率**。传统推荐系统可以通过预计算和缓存等技术实现毫秒级响应，而生成式系统由于需要自回归生成，计算复杂度显著提高。

**OneRec 的实践经验**表明，通过架构优化可以显著提升推理效率。OneRec 的生成式架构通过采用类 LLM 的 Encoder-Decoder 架构精简组件，将关键算子数量压缩 92% 至 1,200 个，配合更大模型规模提升计算密度；通过重构推荐链路释放延迟压力，使训练 / 推理 MFU 分别飙升至 23.7% 和 28.6%，较传统方案实现 3-5 倍提升[(99)](http://m.toutiao.com/group/7517593297582441001/?upstream_biz=doubao)。

**美团 MTGR 的优化策略**包括多个层面：



1. **H2D 优化**：通过优化 CPU 到 GPU 的数据传输，将耗时从 7.5ms 减少至 12us，整体推理延迟从 19ms 减少至 12ms（降低 37%），吞吐从 68 req/s 提升至 94 req/s（提升 38%）[(107)](https://tech.meituan.com/2025/05/19/meituan-generative-recommendation.html)

2. **CUDA Graph 优化**：使用 CUDA Graph 优化后吞吐提升 13%，在不同 QPS 压力下，Avg 延迟降低 17%-52%，P99 延迟降低 16%-57%[(107)](https://tech.meituan.com/2025/05/19/meituan-generative-recommendation.html)

3. **模型架构优化**：通过用户级压缩实现训练和推理加速，提出 Group-Layer Normalization（GLN）来增强不同语义空间的编码性能

**技术优化方向**包括：



1. **序列压缩**：通过缩短输入序列长度降低计算成本，如快手的 KuaiFormer 采用自适应物品压缩机制，将早期用户交互分组为粗粒度聚合表示

2. **注意力机制优化**：如 HSTU 的点积聚合注意力替代 softmax 注意力，在处理非平稳词汇表时更有效率

3. **并行生成**：通过束搜索（beam search）等技术实现并行生成，提升生成速度

4. **模型蒸馏**：通过知识蒸馏技术将大模型的能力迁移到小模型，在保持效果的同时降低计算成本

### 5.2 冷启动问题处理

冷启动问题是推荐系统面临的经典挑战，在生成式推荐系统中同样存在。生成式模型需要大量的历史交互数据来学习用户行为模式和物品关系，对于新用户或新物品，模型可能缺乏足够的训练数据。

**大语言模型为冷启动问题提供了新的解决方案**，主要包括两种策略：



1. **信息增强**：利用 LLMs 生成的表征或知识补充推荐数据。例如，蚂蚁集团的 SAID 基于文本信息生成物品 embedding 并集成到下游推荐任务；清华的 CSRec 融合基于元数据和常识的 LLMs 知识作为辅助信息增强推荐

2. **模型推理**：直接基于 LLMs 学习到的模式生成推荐结果。例如，Meta 的 LLM-Rec 采用精心设计的提示策略推导出有效的推荐解决方案

**世界知识的利用**是冷启动处理的关键。预训练大语言模型（如 Llama 3、Qwen3）包含的世界知识来源于在大规模、多领域数据集上的训练，这些知识能够有效帮助推荐系统在冷启动阶段学习用户 - 物品交互模式。例如，LCRec 将 Llama 的语言语义与协同信号整合，在推荐系统中获得世界知识和任务特定特征。

**多模态融合**是另一个重要方向。通过结合图像、视频、语音等多模态数据，并通过对比学习对齐不同模态表示，可以有效缓解冷启动问题。例如：



1. 小红书的 NoteLLM-2 利用视觉信息提升笔记推荐效果

2. TALKPLAY 通过音频和语义信息进行音乐推荐

3. InteraRec 从网页截图中提取商品信息

### 5.3 多模态数据处理

现代推荐系统越来越依赖多模态数据，包括文本、图像、视频、音频等。生成式推荐系统在处理多模态数据方面具有天然优势，能够通过统一的 token 表示将不同模态的数据整合到生成过程中。

**快手 OneRec 的多模态分词器**是一个典型案例，它融合视频标题、图像等多维信息与用户行为，利用 RQ-Kmeans 分层生成语义 ID[(88)](https://www.csdn.net/article/2025-06-23/148843176)。这种设计的优势在于：



1. **语义理解**：能够理解物品的多模态内容，提升推荐的准确性

2. **跨模态检索**：支持基于文本搜索视频、基于图像搜索相关物品等跨模态推荐

3. **内容生成**：结合 AIGC 技术，可以生成新的推荐内容

**多模态生成式推荐的技术挑战**包括：



1. **模态对齐**：不同模态的数据需要在语义空间中对齐，确保生成的推荐结果在各个模态下都有意义

2. **计算复杂度**：多模态处理会增加计算成本，需要设计高效的融合策略

3. **数据质量**：多模态数据的质量参差不齐，需要建立质量评估和清洗机制

### 5.4 实时性与可扩展性要求

推荐系统通常需要**毫秒级响应**，以满足用户的实时交互需求。生成式推荐系统在实现实时性方面面临特殊挑战，因为自回归生成过程的计算复杂度较高。

**快手的实践经验**表明，通过优化可以实现可接受的延迟。OneRec 通过 TensorRT 加速生成模型，响应延迟小于 200ms，支撑抖音日活亿级用户的实时推荐；通过缓存机制将热门歌手的语义嵌入预存 GPU 显存，生成延迟可降至 100ms 以下[(73)](https://blog.csdn.net/u014535908/article/details/149781053)。

**可扩展性**是另一个关键考虑因素。随着用户规模和物品库的增长，推荐系统需要具备良好的水平扩展能力。生成式推荐系统的扩展策略包括：



1. **模型并行**：将大模型分布在多个 GPU 上进行训练和推理

2. **数据并行**：将不同用户的数据分配到不同的计算节点上处理

3. **服务分层**：通过负载均衡和缓存机制，将热点请求分散到多个服务节点

## 六、结论与展望

### 6.1 核心结论

基于对生成式推荐系统技术原理和架构设计的深入分析，我们可以得出以下**核心结论**：

**传统架构组件的必要性发生根本性变化**。生成式推荐系统基于 Transformer 的自回归生成机制，将推荐任务重构为序列生成问题，从技术原理上不再需要传统的 "双塔 + 召回 + 精排" 架构。生成式模型通过语义 ID 编码和统一的特征空间，可以直接从全量物品库中生成推荐结果，实现端到端的推荐流程。

然而，在**工业级大规模场景**中，完全抛弃传统组件并非最优选择。虽然生成式架构在技术上可以实现全功能替代，但考虑到推理延迟、系统稳定性、工程复杂度等因素，**混合架构或渐进迁移策略**往往更加务实。特别是在处理亿级物品库和毫秒级响应要求时，传统组件的工程优势仍有其价值。

**架构选择应基于具体场景和业务需求**。对于技术实力强、业务规模大的企业，可以考虑采用完全替代策略，充分发挥生成式技术的优势；对于大多数企业，建议采用部分融合或渐进迁移策略，在保持系统稳定性的同时逐步引入生成式能力。

### 6.2 技术发展趋势

展望未来，生成式推荐系统的发展将呈现以下趋势：

**模型规模持续增长**。随着计算能力的提升和训练成本的降低，生成式推荐模型的规模将持续增长。研究表明，生成式推荐模型的质量随训练计算量呈幂律增长[(18)](https://icml.cc/media/icml-2024/Slides/32684.pdf?ref=https%3A%2F%2Fgithubhelp.com)，这为通过增加模型规模提升推荐效果提供了理论基础。

**多模态融合深化**。未来的生成式推荐系统将更加注重多模态数据的融合，不仅包括文本、图像、视频等静态内容，还将包括用户的语音、表情、动作等动态信息。通过统一的多模态表示，实现更加精准和丰富的推荐体验。

**实时生成技术突破**。随着硬件技术的进步和算法优化的深入，生成式推荐系统的实时性将得到显著提升。预计未来将出现专门针对推荐场景优化的生成架构，在保持推荐质量的同时实现亚毫秒级响应。

**个性化程度提升**。生成式推荐系统将具备更强的个性化能力，不仅能够理解用户的显性偏好，还能推断用户的潜在需求和情感状态。通过结合用户画像、历史行为、实时情境等多维度信息，提供真正个性化的推荐体验。

### 6.3 实践建议

基于研究发现，我们对企业实施生成式推荐系统提出以下**实践建议**：

**技术评估阶段**：企业应首先评估自身的技术基础和业务需求，明确生成式推荐的应用场景和预期目标。建议从试点项目开始，选择部分业务场景验证技术可行性。

**架构设计阶段**：根据企业规模和技术实力选择合适的架构策略。大型平台可以考虑激进的技术路线，中小企业建议采用渐进式迁移。在架构设计中应充分考虑可扩展性和可维护性。

**工程实施阶段**：重点关注推理效率优化、冷启动处理、多模态融合等关键技术问题。建议建立完善的监控体系，确保系统稳定运行。同时要重视团队技能提升，培养既懂推荐算法又熟悉生成式技术的复合型人才。

**持续优化阶段**：生成式推荐系统需要持续的优化和改进。建议建立 A/B 测试机制，通过数据驱动的方式不断提升推荐效果。同时要关注技术发展趋势，及时引入新的技术突破。

生成式推荐系统代表了推荐技术的重要发展方向，它不仅是技术的革新，更是推荐理念的转变。从 "选择" 到 "生成"，从 "匹配" 到 "创造"，生成式推荐系统正在重新定义推荐的边界。企业应积极拥抱这一技术变革，在保持理性的同时勇于创新，共同推动推荐技术的进步。

**参考资料&#x20;**

\[1] Augmenting Sequential Recommendation with Pseudo-Prior Items via Reversely Pre-training Transformer[ https://arxiv.org/pdf/2105.00522](https://arxiv.org/pdf/2105.00522)

\[2] GR-LLMs: Recent Advances in Generative Recommendation Based on Large Language Models[ https://arxiv.org/pdf/2507.06507](https://arxiv.org/pdf/2507.06507)

\[3] GenRec: Generative Sequential Recommendation with Large Language Models[ https://arxiv.org/pdf/2407.21191](https://arxiv.org/pdf/2407.21191)

\[4] A Survey on Large Language Models for Recommendation[ https://arxiv.org/pdf/2305.19860](https://arxiv.org/pdf/2305.19860)

\[5] Transformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation[ https://research.facebook.com/file/1068762590546101/Transformers4Rec-Bridging-the-Gap-between-NLP-and-Sequential-Session-Based-Recommendation-1.pdf](https://research.facebook.com/file/1068762590546101/Transformers4Rec-Bridging-the-Gap-between-NLP-and-Sequential-Session-Based-Recommendation-1.pdf)

\[6] All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era[ https://arxiv.org/pdf/2407.10081](https://arxiv.org/pdf/2407.10081)

\[7] Personalized Re-ranking for Recommendation[ https://www.researchgate.net/profile/Fei-Sun-41/publication/332439435\_Personalized\_Context-aware\_Re-ranking\_for\_E-commerce\_Recommender\_Systems/links/6047567b4585154e8c87e02e/Personalized-Context-aware-Re-ranking-for-E-commerce-Recommender-Systems.pdf](https://www.researchgate.net/profile/Fei-Sun-41/publication/332439435_Personalized_Context-aware_Re-ranking_for_E-commerce_Recommender_Systems/links/6047567b4585154e8c87e02e/Personalized-Context-aware-Re-ranking-for-E-commerce-Recommender-Systems.pdf)

\[8] Content-Based Collaborative Generation for Recommender Systems[ https://arxiv.org/pdf/2403.18480](https://arxiv.org/pdf/2403.18480)

\[9] Empowering Sequential Recommendation from Collaborative Signals and Semantic Relatedness[ https://arxiv.org/pdf/2403.07623](https://arxiv.org/pdf/2403.07623)

\[10] MiniOneRec: An Open-Source Framework for Scaling Generative Recommendation[ https://arxiv.org/pdf/2510.24431](https://arxiv.org/pdf/2510.24431)

\[11] Long Short-Term Preference Modeling for Continuous-Time Sequential Recommendation[ https://arxiv.org/pdf/2208.00593](https://arxiv.org/pdf/2208.00593)

\[12] Multi-Aspect Cross-modal Quantization for Generative Recommendation[ https://arxiv.org/pdf/2511.15122](https://arxiv.org/pdf/2511.15122)

\[13] GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization[ https://arxiv.org/pdf/2507.14758](https://arxiv.org/pdf/2507.14758)

\[14] Scaling New Frontiers: Insights into Large Recommendation Models[ https://arxiv.org/pdf/2412.00714](https://arxiv.org/pdf/2412.00714)

\[15] Generative Recommendation: A Survey of Models, Systems, and Industrial Advances[ https://d197for5662m48.cloudfront.net/documents/publicationstatus/294826/preprint\_pdf/52b722a0aa98a01329d60832ed81aa16.pdf](https://d197for5662m48.cloudfront.net/documents/publicationstatus/294826/preprint_pdf/52b722a0aa98a01329d60832ed81aa16.pdf)

\[16] Scenario-aware and Mutual-based approach for Multi-scenario Recommendation in E-Commerce[ https://arxiv.org/pdf/2012.08952](https://arxiv.org/pdf/2012.08952)

\[17] OneRec: Unifying Retrieve and Rank with Generative Recommender and Iterative Preference Alignment[ https://arxiv.org/pdf/2502.18965](https://arxiv.org/pdf/2502.18965)

\[18] Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations[ https://icml.cc/media/icml-2024/Slides/32684.pdf?ref=https%3A%2F%2Fgithubhelp.com](https://icml.cc/media/icml-2024/Slides/32684.pdf?ref=https%3A%2F%2Fgithubhelp.com)

\[19] Recommender Systems with Generative Retrieval[ https://proceedings.neurips.cc/paper\_files/paper/2023/file/20dcab0f14046a5c6b02b61da9f13229-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/20dcab0f14046a5c6b02b61da9f13229-Paper-Conference.pdf)

\[20] OneRec Technical Report[ https://arxiv.org/pdf/2506.13695](https://arxiv.org/pdf/2506.13695)

\[21] Scaling New Frontiers: Insights into Large Recommendation Models[ https://arxiv.org/pdf/2412.00714](https://arxiv.org/pdf/2412.00714)

\[22] Tiger: Transferable Interest Graph Embedding for Domain-Level Zero-Shot Recommendation[ https://dl.acm.org/doi/pdf/10.1145/3511808.3557472](https://dl.acm.org/doi/pdf/10.1145/3511808.3557472)

\[23] OneRec-V2 Technical Report[ https://arxiv.org/pdf/2508.20900](https://arxiv.org/pdf/2508.20900)

\[24] Action is All You Need: Dual-Flow Generative Ranking Network for Recommendation[ https://arxiv.org/pdf/2505.16752](https://arxiv.org/pdf/2505.16752)

\[25] Beyond Unimodal Boundaries: Generative Recommendation with Multimodal Semantics[ https://arxiv.org/pdf/2503.23333](https://arxiv.org/pdf/2503.23333)

\[26] MiniOneRec: An Open-Source Framework for Scaling Generative Recommendation[ https://arxiv.org/pdf/2510.24431](https://arxiv.org/pdf/2510.24431)

\[27] MTGR: Industrial-Scale Generative Recommendation Framework in Meituan[ https://arxiv.org/pdf/2505.18654](https://arxiv.org/pdf/2505.18654)

\[28] Unifying Generative and Dense Retrieval for Sequential Recommendation[ https://arxiv.org/pdf/2411.18814](https://arxiv.org/pdf/2411.18814)

\[29] CCL4Rec: Contrast over Contrastive Learning for Micro-video Recommendation[ https://arxiv.org/pdf/2208.08024](https://arxiv.org/pdf/2208.08024)

\[30] LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders[ https://arxiv.org/pdf/2505.04421](https://arxiv.org/pdf/2505.04421)

\[31] 奈飞工厂:算法优化实战-CSDN博客[ https://blog.csdn.net/weixin\_73958875/article/details/150381655](https://blog.csdn.net/weixin_73958875/article/details/150381655)

\[32] 网易云音乐基于YouTube 推荐系统双塔架构解析(含代码)\_51CTO博客\_网易云音乐云梯[ https://blog.51cto.com/u\_13270164/13854413](https://blog.51cto.com/u_13270164/13854413)

\[33] AI推荐系统:如何悄无声息地重塑你的购物车?AI推荐系统:如何悄无声息地重塑你的购物车? ——从“人找货”到“货找人”的 - 掘金[ https://juejin.cn/post/7546781433844138003](https://juejin.cn/post/7546781433844138003)

\[34] 推荐算法 面试\_推荐算法面试-CSDN博客[ https://blog.csdn.net/qq\_43431934/article/details/133215178](https://blog.csdn.net/qq_43431934/article/details/133215178)

\[35] 热题解析:推荐系统四层架构深度拆解，2025面试官最想听的答案召回、粗排、精排、重排分别是干什么的?2025 了，有什么 - 掘金[ https://juejin.cn/post/7560517862835470345](https://juejin.cn/post/7560517862835470345)

\[36] 推荐系统 AI 面试:召回/排序/特征/冷启动/评测全链路追问清单 | Gank Interview[ https://www.gankinterview.cn/blog/recommendation-system-ai-interview-full-pipeline-follow-up-checklist-recallranki](https://www.gankinterview.cn/blog/recommendation-system-ai-interview-full-pipeline-follow-up-checklist-recallranki)

\[37] 久别重逢话双塔 - 文章 - 开发者社区 - 火山引擎[ https://developer.volcengine.com/articles/7391691895778312211](https://developer.volcengine.com/articles/7391691895778312211)

\[38] On Ranking Consistency of Pre-ranking Stage[ https://arxiv.org/pdf/2205.01289v3](https://arxiv.org/pdf/2205.01289v3)

\[39] User Long-Term Multi-Interest Retrieval Model for Recommendation[ https://arxiv.org/html/2507.10097v1/](https://arxiv.org/html/2507.10097v1/)

\[40] Learning Cascade Ranking as One Network[ https://icml.cc/media/icml-2025/Slides/44498\_ZtA5b6H.pdf](https://icml.cc/media/icml-2025/Slides/44498_ZtA5b6H.pdf)

\[41] Contrastive Prototype Framework for Calibrating Video Recommendation[ https://dl.acm.org/doi/pdf/10.1145/3746027.3755869](https://dl.acm.org/doi/pdf/10.1145/3746027.3755869)

\[42] Learning Cascade Ranking as One Network[ https://openreview.net/forum?id=fvmnx3OxTI](https://openreview.net/forum?id=fvmnx3OxTI)

\[43] AIF: Asynchronous Inference Framework for Cost-Effective Pre-Ranking[ https://arxiv.org/pdf/2511.12934v2](https://arxiv.org/pdf/2511.12934v2)

\[44] Theoretical Guarantees for LT-TTD: A Unified Transformer-based Architecture for Two-Level Ranking Systems[ https://www.arxiv.org/pdf/2505.04434](https://www.arxiv.org/pdf/2505.04434)

\[45] Emotion and sentiment enriched decision transformer for personalized recommendations - PMC[ https://pmc.ncbi.nlm.nih.gov/articles/PMC12218365/](https://pmc.ncbi.nlm.nih.gov/articles/PMC12218365/)

\[46] Трансформерные архитектуры для рекомендаций: от SASRec до сегодняшнего дня. Сравниваем с помощью RecTools[ https://habr.com/en/companies/ru\_mts/articles/947710/](https://habr.com/en/companies/ru_mts/articles/947710/)

\[47] Tuesday Posters[ https://recsys.acm.org/recsys24/posters-1/](https://recsys.acm.org/recsys24/posters-1/)

\[48] 【推荐系统】BERT4Rec:使用Bert进行序列推荐-CSDN博客[ https://blog.csdn.net/qq\_27590277/article/details/109396779](https://blog.csdn.net/qq_27590277/article/details/109396779)

\[49] DenseRec: Revisiting Dense Content Embeddings for Sequential Transformer-based Recommendation[ https://earl-workshop.github.io/pdf/recsys2025-workshops\_paper\_167.pdf](https://earl-workshop.github.io/pdf/recsys2025-workshops_paper_167.pdf)

\[50] Turning Dross Into Gold Loss: Is BERT4Rec really better than SASRec?[ https://github.com/antklen/sasrec-bert4rec-recsys23](https://github.com/antklen/sasrec-bert4rec-recsys23)

\[51] STEERING DIFFUSION MODELS TOWARDS CREDIBLE CONTENT RECOMMENDATION[ https://openreview.net/pdf/2a0bf8bbf16da4866f03326abdc16e9394d3d0aa.pdf](https://openreview.net/pdf/2a0bf8bbf16da4866f03326abdc16e9394d3d0aa.pdf)

\[52] transfromer终极理解 - 指尖下的世界 - 博客园[ https://www.cnblogs.com/luzhanshi/articles/19068486](https://www.cnblogs.com/luzhanshi/articles/19068486)

\[53] course/chapters/en/chapter1/4.mdx at main · huggingface/course · GitHub[ https://github.com/huggingface/course/blob/main/chapters/en/chapter1/4.mdx](https://github.com/huggingface/course/blob/main/chapters/en/chapter1/4.mdx)

\[54] VISION TRANSFORMERS IN 2022:(pdf)[ https://arxiv.org/pdf/2205.10660v1](https://arxiv.org/pdf/2205.10660v1)

\[55] Attention Over Self-Attention: Intention-Aware Re-Ranking With Dynamic Transformer Encoders for Recommendation[ https://dl.acm.org/doi/10.1109/TKDE.2022.3208633](https://dl.acm.org/doi/10.1109/TKDE.2022.3208633)

\[56] Recommender System Using Transformer Model: A Systematic Literature Review[ https://www.researchgate.net/publication/363358748\_Recommender\_System\_Using\_Transformer\_Model\_A\_Systematic\_Literature\_Review](https://www.researchgate.net/publication/363358748_Recommender_System_Using_Transformer_Model_A_Systematic_Literature_Review)

\[57] Researches Advanced in the Development and Application of Transformers(pdf)[ https://pdfs.semanticscholar.org/24fe/3f9c1f2943849556a89fc72577c0fa6ada89.pdf](https://pdfs.semanticscholar.org/24fe/3f9c1f2943849556a89fc72577c0fa6ada89.pdf)

\[58] 双塔模型学习-CSDN博客[ https://blog.csdn.net/weixin\_42001184/article/details/145974412](https://blog.csdn.net/weixin_42001184/article/details/145974412)

\[59] 【RAG解惑】Reranker 真的必要吗?双塔 vs 交叉编码器的性价比拐点在哪?\_双塔结构和交叉编码器-CSDN博客[ https://blog.csdn.net/l35633/article/details/152809688](https://blog.csdn.net/l35633/article/details/152809688)

\[60] 深度对决:多模态AI的两大始祖:CLIP vs BLIP，谁才是你的最优解?\_心眸AI笔记[ http://m.toutiao.com/group/7583727814915015174/?upstream\_biz=doubao](http://m.toutiao.com/group/7583727814915015174/?upstream_biz=doubao)

\[61] MTGR:美团外卖生成式推荐Scaling Law落地实践 - 美团技术团队[ https://tech.meituan.com/2025/05/19/meituan-generative-recommendation.html](https://tech.meituan.com/2025/05/19/meituan-generative-recommendation.html)

\[62] 训练推荐模型 - Azure Databricks | Microsoft Learn[ https://learn.microsoft.com/zh-cn/azure/databricks/machine-learning/train-recommender-models](https://learn.microsoft.com/zh-cn/azure/databricks/machine-learning/train-recommender-models)

\[63] 【智屏未来】解码Netflix/Spotify背后个性化算法战争\_流媒体网[ http://m.toutiao.com/group/7581790101710094898/?upstream\_biz=doubao](http://m.toutiao.com/group/7581790101710094898/?upstream_biz=doubao)

\[64] CLIP你所说的‘双塔架构’:‘双塔架构，即图像编码器和文本编码器分别处理各自模态的数据’和推荐系统中的双塔架构的异同点在哪?请深刻理解各自的‘双塔’\_clip图像塔-CSDN博客[ https://blog.csdn.net/sinat\_37574187/article/details/149066475](https://blog.csdn.net/sinat_37574187/article/details/149066475)

\[65] Learning Cascade Ranking as One Network[ https://icml.cc/virtual/2025/poster/44498](https://icml.cc/virtual/2025/poster/44498)

\[66] Towards Large-scale Generative Ranking[ https://arxiv.org/html/2505.04180v2](https://arxiv.org/html/2505.04180v2)

\[67] Sparse Meets Dense: Unified Generative Recommendations with Cascaded Sparse-Dense Representations[ https://openreview.net/pdf/07a05b3c414fd9c8cc5fb879c1b378d7e51d52d1.pdf](https://openreview.net/pdf/07a05b3c414fd9c8cc5fb879c1b378d7e51d52d1.pdf)

\[68] OneRec Technical Report[ https://arxiv.org/html/2506.13695v1](https://arxiv.org/html/2506.13695v1)

\[69] Personalized Ranking on Cascading Behavior Graphs for Accurate Multi-Behavior Recommendation(pdf)[ https://arxiv.org/pdf/2502.11335v1.pdf](https://arxiv.org/pdf/2502.11335v1.pdf)

\[70] Theoretical Guarantees for LT-TTD: A Unified Transformer-based Architecture for Two-Level Ranking Systems[ https://arxiv.org/html/2505.04434v1](https://arxiv.org/html/2505.04434v1)

\[71] RankTower: A Synergistic Framework for Enhancing Two-Tower Pre-Ranking Model(pdf)[ https://ceur-ws.org/Vol-3837/paper\_07\_ceur\_paper.pdf](https://ceur-ws.org/Vol-3837/paper_07_ceur_paper.pdf)

\[72] MTGR:美团外卖生成式推荐Scaling Law落地实践 - 美团技术团队[ https://tech.meituan.com/2025/05/19/meituan-generative-recommendation.html](https://tech.meituan.com/2025/05/19/meituan-generative-recommendation.html)

\[73] 生成式推荐网络架构汇总\_onerec和hstu的异同-CSDN博客[ https://blog.csdn.net/u014535908/article/details/149781053](https://blog.csdn.net/u014535908/article/details/149781053)

\[74] 2024\_HSTU[ https://www.huaxiaozhuan.com/applications/recommendation/sequential\_recommendation/chapters/2024\_HSTU.html](https://www.huaxiaozhuan.com/applications/recommendation/sequential_recommendation/chapters/2024_HSTU.html)

\[75] 抛弃“级联”架构!快手OneRec用大模型重构推荐系统，服务成本降至1/10\_InfoQ[ http://m.toutiao.com/group/7575464804840636955/?upstream\_biz=doubao](http://m.toutiao.com/group/7575464804840636955/?upstream_biz=doubao)

\[76] 推荐大模型来了?OneRec论文:端到端训练如何同时吃掉效果与成本\_机器之心Pro[ http://m.toutiao.com/group/7517593297582441001/?upstream\_biz=doubao](http://m.toutiao.com/group/7517593297582441001/?upstream_biz=doubao)

\[77] 告别选择困难!AI Ping一站式评测助你精准选型一、引言 在生成式AI技术爆发式增长的今天，开发者面临前所未有的选择困 - 掘金[ https://juejin.cn/post/7550684680058372146](https://juejin.cn/post/7550684680058372146)

\[78] 开源ChatGPT模型性能测评与开放性解析[ https://www.iesdouyin.com/share/video/7535408892105346356/?region=\&mid=7535408985491507987\&u\_code=0\&did=MS4wLjABAAAANwkJuWIRFOzg5uCpDRpMj4OX-QryoDgn-yYlXQnRwQQ\&iid=MS4wLjABAAAANwkJuWIRFOzg5uCpDRpMj4OX-QryoDgn-yYlXQnRwQQ\&with\_sec\_did=1\&video\_share\_track\_ver=\&titleType=title\&share\_sign=i64JmIeEwbpeZw0Xi.Th.iyg7\_JKhvw5oXrJ8YMdpBg-\&share\_version=280700\&ts=1767442005\&from\_aid=1128\&from\_ssr=1\&share\_track\_info=%7B%22link\_description\_type%22%3A%22%22%7D](https://www.iesdouyin.com/share/video/7535408892105346356/?region=\&mid=7535408985491507987\&u_code=0\&did=MS4wLjABAAAANwkJuWIRFOzg5uCpDRpMj4OX-QryoDgn-yYlXQnRwQQ\&iid=MS4wLjABAAAANwkJuWIRFOzg5uCpDRpMj4OX-QryoDgn-yYlXQnRwQQ\&with_sec_did=1\&video_share_track_ver=\&titleType=title\&share_sign=i64JmIeEwbpeZw0Xi.Th.iyg7_JKhvw5oXrJ8YMdpBg-\&share_version=280700\&ts=1767442005\&from_aid=1128\&from_ssr=1\&share_track_info=%7B%22link_description_type%22%3A%22%22%7D)

\[79] MTGR:美团外卖生成式推荐Scaling Law落地实践\_mtgr: industrial-scale generative recommendation f-CSDN博客[ https://blog.csdn.net/MeituanTech/article/details/147997167](https://blog.csdn.net/MeituanTech/article/details/147997167)

\[80] Breaking the Likelihood Trap: Consistent Generative Recommendation with Graph-structured Model(pdf)[ https://web3.arxiv.org/pdf/2510.10127](https://web3.arxiv.org/pdf/2510.10127)

\[81] Can LLMs Outshine Conventional Recommenders? A Comparative Evaluation(pdf)[ https://arxiv.org/pdf/2503.05493v2](https://arxiv.org/pdf/2503.05493v2)

\[82] 1 Introduction[ https://arxiv.org/html/2509.22681v1](https://arxiv.org/html/2509.22681v1)

\[83] MiniOneRec: An Open-Source Framework for Scaling Generative Recommendation[ https://arxiv.org/pdf/2510.24431](https://arxiv.org/pdf/2510.24431)

\[84] (pdf)[ https://journalijsra.com/sites/default/files/fulltext\_pdf/IJSRA-2025-0061.pdf](https://journalijsra.com/sites/default/files/fulltext_pdf/IJSRA-2025-0061.pdf)

\[85] SYNERGen: CONTEXTUALIZED GENERATIVE RECOMMENDER FOR UNIFIED SEARCH AND RECOMMENDATION[ https://openreview.net/pdf?id=P6y3gZDsFa](https://openreview.net/pdf?id=P6y3gZDsFa)

\[86] 效果 & 成本双突破!快手提出端到端生成式推荐系统 OneRec!近日，快手推出全新端到端生成式推荐系统OneRec，实 - 掘金[ https://juejin.cn/post/7517852192637485108](https://juejin.cn/post/7517852192637485108)

\[87] OneRec 技术报告\_onerec技术报告-CSDN博客[ https://blog.csdn.net/u013524655/article/details/149003862](https://blog.csdn.net/u013524655/article/details/149003862)

\[88] 大模型杀入推荐系统!快手OneRec端到端方案:降本增效的终极答案?-CSDN.NET[ https://www.csdn.net/article/2025-06-23/148843176](https://www.csdn.net/article/2025-06-23/148843176)

\[89] 抛弃传统架构!大模型驱动新推荐系统，实现成本仅十分之一\_圆桌世界观[ http://m.toutiao.com/group/7576540403755663872/?upstream\_biz=doubao](http://m.toutiao.com/group/7576540403755663872/?upstream_biz=doubao)

\[90] OneRec : 快手推出的端到端生成式推荐系统 | AI智库导航-aiguide.cc[ https://aiguide.cc/17719/](https://aiguide.cc/17719/)

\[91] 快手推出杀手级AI应用[ https://www2.jiuyangongshe.com/h5/article/3f6celfl35u](https://www2.jiuyangongshe.com/h5/article/3f6celfl35u)

\[92] 快手推出OneRec:用大模型简化视频推荐流程 | 最新资讯 | HyperAI超神经[ https://hyper.ai/cn/headlines/663dc44b7c20435729bce2596bcf0e96](https://hyper.ai/cn/headlines/663dc44b7c20435729bce2596bcf0e96)

\[93] 抛弃“级联”架构!快手OneRec用大模型重构推荐系统，服务成本降至1/10\_InfoQ[ http://m.toutiao.com/group/7575464804840636955/?upstream\_biz=doubao](http://m.toutiao.com/group/7575464804840636955/?upstream_biz=doubao)

\[94] OneRec-V2 Technical Report[ https://arxiv.org/html/2508.20900v2](https://arxiv.org/html/2508.20900v2)

\[95] OneRec-Think : In-Text Reasoning for Generative Recommendation[ https://arxiv.org/html/2510.11639v2](https://arxiv.org/html/2510.11639v2)

\[96] OneRec Technical Report[ https://arxiv.org/pdf/2506.13695v3](https://arxiv.org/pdf/2506.13695v3)

\[97] MiniOneRec: An Open-Source Framework for Scaling Generative Recommendation[ https://arxiv.org/pdf/2510.24431v1](https://arxiv.org/pdf/2510.24431v1)

\[98] 快手OneRec大模型推荐系统源码解析 - CSDN文库[ https://wenku.csdn.net/doc/58qpiu2980](https://wenku.csdn.net/doc/58qpiu2980)

\[99] 推荐大模型来了?OneRec论文:端到端训练如何同时吃掉效果与成本\_机器之心Pro[ http://m.toutiao.com/group/7517593297582441001/?upstream\_biz=doubao](http://m.toutiao.com/group/7517593297582441001/?upstream_biz=doubao)

\[100] 推荐也讲Scaling Law?召回、排序、端到端全覆盖:一文读懂生成式推荐系统架构进化-CSDN博客[ https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/149410220](https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/149410220)

\[101] 打破推荐系统「信息孤岛」!中科大与华为提出首个生成式多阶段统一框架，性能全面超越 SOTA-腾讯云开发者社区-腾讯云[ https://cloud.tencent.cn/developer/article/2533212](https://cloud.tencent.cn/developer/article/2533212)

\[102] 推荐算法:生成式排序调研(一)## 背景 生成式排序作为生成式模型在推荐系统中的重要应用方向，旨在通过生成式模型对用户行 - 掘金[ https://juejin.cn/post/7508029170229035071](https://juejin.cn/post/7508029170229035071)

\[103] 傅聪团队新作:OnePiece!通用生成式推荐模型新范式\_搜狐网[ https://m.sohu.com/a/949805483\_122014422/](https://m.sohu.com/a/949805483_122014422/)

\[104] Meta生成式推荐:重塑万亿级推荐系统\_meta 生成式推荐-CSDN博客[ https://blog.csdn.net/lifallen/article/details/151119541](https://blog.csdn.net/lifallen/article/details/151119541)

\[105] AIGC 领域下 AIGC 视频的个性化推荐技术\_禅与计算机程序设计艺术的技术博客\_51CTO博客[ https://blog.51cto.com/universsky/13939800](https://blog.51cto.com/universsky/13939800)

\[106] 抛弃“级联”架构!快手OneRec用大模型重构推荐系统，服务成本降至1/10\_InfoQ[ http://m.toutiao.com/group/7575464804840636955/?upstream\_biz=doubao](http://m.toutiao.com/group/7575464804840636955/?upstream_biz=doubao)

\[107] MTGR:美团外卖生成式推荐Scaling Law落地实践 - 美团技术团队[ https://tech.meituan.com/2025/05/19/meituan-generative-recommendation.html](https://tech.meituan.com/2025/05/19/meituan-generative-recommendation.html)

\[108] Architecting for AI agility: How hybrid by design can help tech architectures accelerate business outcomes[ https://www.ibm.com/thought-leadership/institute-business-value/en-us/report/hybrid-by-design/hybrid-by-design-architecting-for-agility](https://www.ibm.com/thought-leadership/institute-business-value/en-us/report/hybrid-by-design/hybrid-by-design-architecting-for-agility)

\[109] The Future of Commerce Recommendation: Why Hybrid Systems Will Win[ https://www.criteo.com/blog/the-future-of-commerce-recommendation-why-hybrid-systems-will-win/](https://www.criteo.com/blog/the-future-of-commerce-recommendation-why-hybrid-systems-will-win/)

\[110] (pdf)[ https://journalwjaets.com/sites/default/files/fulltext\_pdf/WJAETS-2025-0998.pdf](https://journalwjaets.com/sites/default/files/fulltext_pdf/WJAETS-2025-0998.pdf)

\[111] (pdf)[ https://journalwjaets.com/sites/default/files/fulltext\_pdf/WJAETS-2025-0434.pdf](https://journalwjaets.com/sites/default/files/fulltext_pdf/WJAETS-2025-0434.pdf)

\[112] Building a Unified Lakehouse for Large-Scale Recommendation Systems with Apache Paimon at TikTok - Alibaba Cloud Community[ https://www.alibabacloud.com/blog/building-a-unified-lakehouse-for-large-scale-recommendation-systems-with-apache-paimon-at-tiktok\_602568](https://www.alibabacloud.com/blog/building-a-unified-lakehouse-for-large-scale-recommendation-systems-with-apache-paimon-at-tiktok_602568)

\[113] Sparse Meets Dense: Unified Generative Recommendations with Cascaded Sparse-Dense Representations(pdf)[ https://arxiv.org/pdf/2503.02453v1](https://arxiv.org/pdf/2503.02453v1)

\[114] 探索无限可能:生成式推荐的演进、前沿与挑战TL;DR 过去一年间，生成式推荐取得了长足的实质性进展，特别是在凭借大型语言 - 掘金[ https://juejin.cn/post/7561781514922835978](https://juejin.cn/post/7561781514922835978)

\[115] 推荐也讲Scaling Law?召回、排序、端到端全覆盖:一文读懂生成式推荐系统架构进化-CSDN博客[ https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/149410220](https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/149410220)

\[116] 手把手教你激活推荐系统!来自闪购一线的复盘，多模态如何让点击率“原地起飞”?-CSDN博客[ https://blog.csdn.net/xx\_nm98/article/details/156013727](https://blog.csdn.net/xx_nm98/article/details/156013727)

\[117] 抛弃“级联”架构!快手OneRec用大模型重构推荐系统，服务成本降至1/10\_InfoQ[ http://m.toutiao.com/group/7575464804840636955/?upstream\_biz=doubao](http://m.toutiao.com/group/7575464804840636955/?upstream_biz=doubao)

\[118] 深度解析MiniOneRec:可扩展生成式推荐的开源新范式[ http://dev.guyuehome.com/wap/detail?id=1986048319400472577](http://dev.guyuehome.com/wap/detail?id=1986048319400472577)

\[119] 09\_昇腾适配GR模型实践\_深度学习1的技术博客\_51CTO博客[ https://blog.51cto.com/u\_13728936/14418170](https://blog.51cto.com/u_13728936/14418170)

\[120] 快手OneRec 重构推荐系统:从检索排序到生成统一的跃迁-CSDN博客[ https://blog.csdn.net/weixin\_46351593/article/details/147277623](https://blog.csdn.net/weixin_46351593/article/details/147277623)

\[121] MiniOneRec: An Open-Source Framework for Scaling Generative Recommendation(pdf)[ https://arxiv.org/pdf/2510.24431v1](https://arxiv.org/pdf/2510.24431v1)

\[122] Recommendation Systems • LLM[ https://vinija.ai/recsys1/LLM/](https://vinija.ai/recsys1/LLM/)

\[123] 从协同过滤到认知推理个性化推荐的范式革新-CSDN博客[ https://blog.csdn.net/wwlsm\_zql/article/details/154979063](https://blog.csdn.net/wwlsm_zql/article/details/154979063)

\[124] 推荐系统三十年:从协同过滤到大模型时代的技术编年史-腾讯云开发者社区-腾讯云[ https://cloud.tencent.com/developer/article/2608140?policyId=1003](https://cloud.tencent.com/developer/article/2608140?policyId=1003)

\[125] Large Language Models for Generative Recommendation: A Survey and Visionary Discussions, Lei Li+, N/A, LREC-COLING'24 #1344[ https://github.com/AkihikoWatanabe/paper\_notes/issues/1344](https://github.com/AkihikoWatanabe/paper_notes/issues/1344)

\[126] Analysis of Recommender System Using Generative Artificial Intelligence: A Systematic Literature Review(pdf)[ https://xplorestaging.ieee.org/ielx8/6287639/10380310/10565860.pdf?arnumber=10565860](https://xplorestaging.ieee.org/ielx8/6287639/10380310/10565860.pdf?arnumber=10565860)

\[127] (pdf)[ https://web3.arxiv.org/pdf/2502.18965](https://web3.arxiv.org/pdf/2502.18965)

> （）