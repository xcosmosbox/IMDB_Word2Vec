# 算法与大模型技术文档

> 版本: v1.0  
> 更新日期: 2026-01-03  
> 对应架构文档: 《生成式推荐系统架构设计》第三章、第四章、第五章、第八章

---

## 目录

1. [概述](#1-概述)
2. [生成式推荐模型架构](#2-生成式推荐模型架构)
3. [训练流程](#3-训练流程)
4. [推理流程](#4-推理流程)
5. [特征工程方案](#5-特征工程方案)
6. [大模型接入方案](#6-大模型接入方案)
7. [附录](#7-附录)

---

## 1. 概述

### 1.1 文档目的

本文档详细描述生成式推荐系统的算法架构与大模型接入方案，为算法工程师提供可直接落地的技术实现指导。

### 1.2 架构文档依据

| 本文档章节 | 对应架构文档章节 | 核心内容 |
|-----------|-----------------|----------|
| 第2章 模型架构 | 第三章 UGT模型 | Unified Generative Transformer 设计 |
| 第3章 训练流程 | 第八章 训练与部署 | 三阶段训练策略 |
| 第4章 推理流程 | 第六章 推理加速 | M-FALCON 加速策略 |
| 第5章 特征工程 | 第四章 数据流 | 事件序列化方案 |
| 第6章 大模型接入 | 第五章 冷启动 | LLM 辅助冷启动 |

### 1.3 性能目标

| 指标 | 目标值 | 依据 |
|------|--------|------|
| 模型推理延迟 | P99 < 30ms | 架构文档 6.3 节延迟分解 |
| 训练吞吐 | MFU > 23% | 架构文档 OneRec 实践参考 |
| 冷启动时效 | < 1小时 | 架构文档 1.2 节性能目标 |
| 向量维度 | 256 | 架构文档 3.2.1 节 |

---

## 2. 生成式推荐模型架构

### 2.1 UGT (Unified Generative Transformer) 总体结构

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    Unified Generative Transformer (UGT)                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│                          ┌─────────────────┐                                │
│                          │   Output Head   │  → Semantic ID 序列            │
│                          │  Linear + Softmax│                               │
│                          └────────┬────────┘                                │
│                                   │                                         │
│  ┌────────────────────────────────┴────────────────────────────────────┐   │
│  │                    Decoder (MoE-Enhanced) × N_dec                    │   │
│  │  ┌─────────────────────────────────────────────────────────────┐    │   │
│  │  │  [Causal Attention] → [MoE FFN (16 experts, top-4)] → [GLN] │    │   │
│  │  └─────────────────────────────────────────────────────────────┘    │   │
│  └────────────────────────────────┬────────────────────────────────────┘   │
│                                   │                                         │
│  ┌────────────────────────────────┴────────────────────────────────────┐   │
│  │                    Encoder (Multi-Scale) × N_enc                     │   │
│  │  ┌─────────────────────────────────────────────────────────────┐    │   │
│  │  │  [Dot-Product Aggregated Attention] → [FFN] → [GLN]         │    │   │
│  │  └─────────────────────────────────────────────────────────────┘    │   │
│  └────────────────────────────────┬────────────────────────────────────┘   │
│                                   │                                         │
│  ┌────────────────────────────────┴────────────────────────────────────┐   │
│  │                     Unified Input Embedding                          │   │
│  │  E_input = E_semantic + E_position + E_type + E_time + E_feature    │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 2.2 模型超参数配置

| 参数 | 小规模 | 中规模 | 大规模 (生产) | 说明 |
|------|--------|--------|--------------|------|
| d_model | 256 | 512 | 1024 | 隐藏层维度 |
| n_heads | 8 | 16 | 32 | 注意力头数 |
| n_enc_layers | 6 | 12 | 24 | 编码器层数 |
| n_dec_layers | 6 | 12 | 24 | 解码器层数 |
| d_ff | 1024 | 2048 | 4096 | FFN 中间维度 |
| num_experts | 8 | 16 | 32 | MoE 专家数 |
| top_k_experts | 2 | 4 | 4 | 激活专家数 |
| max_seq_len | 512 | 1024 | 2048 | 最大序列长度 |
| vocab_size | 100K | 500K | 2M | 语义 ID 词表大小 |
| dropout | 0.1 | 0.1 | 0.1 | Dropout 率 |

### 2.3 Semantic ID 编码器

#### 2.3.1 RQ-VAE 架构

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    Residual Vector Quantization (RQ-VAE)                     │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  输入: 物品多模态特征 x ∈ R^d                                                │
│                                                                             │
│  ┌─────────────┐     ┌─────────────┐     ┌─────────────┐                   │
│  │  Codebook 1 │     │  Codebook 2 │     │  Codebook 3 │                   │
│  │  1024 codes │     │  4096 codes │     │ 16384 codes │                   │
│  │  (粗粒度类目) │     │ (细粒度属性) │     │  (实例区分)  │                   │
│  └──────┬──────┘     └──────┬──────┘     └──────┬──────┘                   │
│         │                   │                   │                           │
│         ▼                   ▼                   ▼                           │
│  z_1 = quantize(x)   z_2 = quantize(x-z_1)  z_3 = quantize(x-z_1-z_2)      │
│         │                   │                   │                           │
│         └───────────────────┼───────────────────┘                           │
│                             ▼                                               │
│              Semantic ID = [c_1, c_2, c_3]                                  │
│              例: 电影 → [C_drama, A_classic, I_tt0111161]                    │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### 2.3.2 Python 实现

```python
"""
Semantic ID 编码器实现
对应架构文档: 3.2.1 节 语义 ID 系统
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from dataclasses import dataclass
from typing import List, Tuple, Optional
import numpy as np


@dataclass
class SemanticIDConfig:
    """语义 ID 编码器配置"""
    num_codebooks: int = 3                    # 码本数量 (层数)
    codebook_sizes: Tuple[int, ...] = (1024, 4096, 16384)  # 各层码本大小
    embedding_dim: int = 256                  # 嵌入维度
    commitment_cost: float = 0.25             # 承诺损失权重
    ema_decay: float = 0.99                   # EMA 衰减率
    

class VectorQuantizer(nn.Module):
    """
    单层向量量化器
    
    使用 EMA 更新码本，避免码本坍塌问题
    """
    
    def __init__(
        self, 
        num_embeddings: int, 
        embedding_dim: int,
        commitment_cost: float = 0.25,
        ema_decay: float = 0.99,
    ):
        super().__init__()
        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim
        self.commitment_cost = commitment_cost
        self.ema_decay = ema_decay
        
        # 码本嵌入
        self.embedding = nn.Embedding(num_embeddings, embedding_dim)
        self.embedding.weight.data.uniform_(-1/num_embeddings, 1/num_embeddings)
        
        # EMA 更新参数
        self.register_buffer('ema_cluster_size', torch.zeros(num_embeddings))
        self.register_buffer('ema_embedding_sum', self.embedding.weight.clone())
        
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Args:
            x: 输入张量 (batch_size, embedding_dim)
            
        Returns:
            quantized: 量化后的向量
            indices: 码本索引
            loss: 量化损失
        """
        # 计算距离 (batch_size, num_embeddings)
        distances = torch.cdist(x, self.embedding.weight)
        
        # 找最近的码本向量
        indices = distances.argmin(dim=-1)  # (batch_size,)
        
        # 获取量化向量
        quantized = self.embedding(indices)  # (batch_size, embedding_dim)
        
        # 计算损失
        e_latent_loss = F.mse_loss(quantized.detach(), x)  # 编码器损失
        q_latent_loss = F.mse_loss(quantized, x.detach())  # 码本损失
        loss = q_latent_loss + self.commitment_cost * e_latent_loss
        
        # 直通估计器
        quantized = x + (quantized - x).detach()
        
        # EMA 更新码本 (训练时)
        if self.training:
            self._ema_update(x, indices)
        
        return quantized, indices, loss
    
    def _ema_update(self, x: torch.Tensor, indices: torch.Tensor):
        """EMA 更新码本"""
        # 统计每个码本的使用次数
        encodings = F.one_hot(indices, self.num_embeddings).float()
        
        # 更新聚类大小
        self.ema_cluster_size.mul_(self.ema_decay).add_(
            encodings.sum(0), alpha=1 - self.ema_decay
        )
        
        # 更新嵌入和
        dw = encodings.T @ x
        self.ema_embedding_sum.mul_(self.ema_decay).add_(dw, alpha=1 - self.ema_decay)
        
        # 更新码本
        n = self.ema_cluster_size.sum()
        cluster_size = (
            (self.ema_cluster_size + 1e-5) / (n + self.num_embeddings * 1e-5) * n
        )
        self.embedding.weight.data.copy_(self.ema_embedding_sum / cluster_size.unsqueeze(1))


class ResidualVectorQuantizer(nn.Module):
    """
    残差向量量化器 (RQ-VAE)
    
    通过多层残差量化生成层次化的 Semantic ID
    
    对应架构文档: 3.2.1 节
    """
    
    def __init__(self, config: SemanticIDConfig):
        super().__init__()
        self.config = config
        self.num_codebooks = config.num_codebooks
        
        # 创建多层量化器
        self.quantizers = nn.ModuleList([
            VectorQuantizer(
                num_embeddings=size,
                embedding_dim=config.embedding_dim,
                commitment_cost=config.commitment_cost,
                ema_decay=config.ema_decay,
            )
            for size in config.codebook_sizes
        ])
        
    def forward(
        self, 
        x: torch.Tensor
    ) -> Tuple[torch.Tensor, List[torch.Tensor], torch.Tensor]:
        """
        残差量化
        
        Args:
            x: 输入特征 (batch_size, embedding_dim)
            
        Returns:
            quantized: 最终量化向量
            indices_list: 各层码本索引列表
            total_loss: 总量化损失
        """
        residual = x
        quantized_sum = torch.zeros_like(x)
        indices_list = []
        total_loss = 0.0
        
        for quantizer in self.quantizers:
            # 量化残差
            quantized, indices, loss = quantizer(residual)
            
            # 累加量化结果
            quantized_sum = quantized_sum + quantized
            
            # 更新残差
            residual = residual - quantized
            
            indices_list.append(indices)
            total_loss = total_loss + loss
        
        return quantized_sum, indices_list, total_loss
    
    def encode(self, x: torch.Tensor) -> List[int]:
        """
        编码为 Semantic ID 序列 (推理用)
        
        Args:
            x: 输入特征 (embedding_dim,) 或 (1, embedding_dim)
            
        Returns:
            Semantic ID 列表 [c1, c2, c3]
        """
        if x.dim() == 1:
            x = x.unsqueeze(0)
        
        with torch.no_grad():
            _, indices_list, _ = self.forward(x)
        
        return [idx.item() for idx in indices_list]
    
    def decode(self, semantic_ids: List[int]) -> torch.Tensor:
        """
        从 Semantic ID 解码回向量
        
        Args:
            semantic_ids: Semantic ID 列表 [c1, c2, c3]
            
        Returns:
            重建向量 (embedding_dim,)
        """
        vector = torch.zeros(self.config.embedding_dim)
        
        for quantizer, idx in zip(self.quantizers, semantic_ids):
            vector = vector + quantizer.embedding.weight[idx]
        
        return vector


class MultiModalEncoder(nn.Module):
    """
    多模态特征编码器
    
    支持文本、图像、视频、结构化特征的统一编码
    
    对应架构文档: 4.2 节多模态物品表示
    """
    
    def __init__(self, config: SemanticIDConfig):
        super().__init__()
        self.config = config
        
        # 文本编码器 (使用预训练模型)
        self.text_encoder = nn.Sequential(
            nn.Linear(768, 512),  # BERT 输出维度
            nn.ReLU(),
            nn.Linear(512, config.embedding_dim),
        )
        
        # 图像编码器 (使用预训练模型)
        self.image_encoder = nn.Sequential(
            nn.Linear(2048, 512),  # ResNet 输出维度
            nn.ReLU(),
            nn.Linear(512, config.embedding_dim),
        )
        
        # 视频编码器
        self.video_encoder = nn.Sequential(
            nn.Linear(1024, 512),  # 视频特征维度
            nn.ReLU(),
            nn.Linear(512, config.embedding_dim),
        )
        
        # 结构化特征编码器
        self.struct_encoder = nn.Sequential(
            nn.Linear(128, 256),  # 结构化特征维度
            nn.ReLU(),
            nn.Linear(256, config.embedding_dim),
        )
        
        # 协同特征编码器 (用户-物品交互)
        self.collab_encoder = nn.Sequential(
            nn.Linear(config.embedding_dim, config.embedding_dim),
            nn.ReLU(),
            nn.Linear(config.embedding_dim, config.embedding_dim),
        )
        
        # 多模态融合层
        self.fusion = nn.Sequential(
            nn.Linear(config.embedding_dim * 5, config.embedding_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(config.embedding_dim * 2, config.embedding_dim),
        )
        
    def forward(
        self,
        text_features: Optional[torch.Tensor] = None,
        image_features: Optional[torch.Tensor] = None,
        video_features: Optional[torch.Tensor] = None,
        struct_features: Optional[torch.Tensor] = None,
        collab_features: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        """
        多模态特征融合
        
        Args:
            text_features: 文本特征 (batch_size, 768)
            image_features: 图像特征 (batch_size, 2048)
            video_features: 视频特征 (batch_size, 1024)
            struct_features: 结构化特征 (batch_size, 128)
            collab_features: 协同特征 (batch_size, embedding_dim)
            
        Returns:
            融合后的特征 (batch_size, embedding_dim)
        """
        embeddings = []
        batch_size = None
        
        # 处理各模态
        if text_features is not None:
            batch_size = text_features.size(0)
            embeddings.append(self.text_encoder(text_features))
        
        if image_features is not None:
            batch_size = image_features.size(0)
            embeddings.append(self.image_encoder(image_features))
        
        if video_features is not None:
            batch_size = video_features.size(0)
            embeddings.append(self.video_encoder(video_features))
        
        if struct_features is not None:
            batch_size = struct_features.size(0)
            embeddings.append(self.struct_encoder(struct_features))
        
        if collab_features is not None:
            batch_size = collab_features.size(0)
            embeddings.append(self.collab_encoder(collab_features))
        
        # 填充缺失模态为零向量
        while len(embeddings) < 5:
            embeddings.append(torch.zeros(batch_size, self.config.embedding_dim))
        
        # 拼接并融合
        concat = torch.cat(embeddings, dim=-1)
        fused = self.fusion(concat)
        
        return fused


class SemanticIDEncoder:
    """
    语义 ID 编码器完整实现
    
    将任意物品编码为层次化语义 ID 序列
    
    对应架构文档: 3.2.1 节
    """
    
    def __init__(self, config: SemanticIDConfig):
        self.config = config
        self.multi_modal_encoder = MultiModalEncoder(config)
        self.quantizer = ResidualVectorQuantizer(config)
        
    def encode(self, item_features: dict) -> List[int]:
        """
        将物品编码为语义 ID
        
        Args:
            item_features: 物品特征字典
                - text_features: 文本特征
                - image_features: 图像特征
                - video_features: 视频特征 (可选)
                - struct_features: 结构化特征
                - collab_features: 协同特征 (可选)
                
        Returns:
            语义 ID 列表 [c1, c2, c3]
        """
        # 多模态编码
        fused = self.multi_modal_encoder(**item_features)
        
        # 量化为语义 ID
        semantic_ids = self.quantizer.encode(fused)
        
        return semantic_ids
    
    def batch_encode(self, batch_features: dict) -> List[List[int]]:
        """
        批量编码
        """
        fused = self.multi_modal_encoder(**batch_features)
        _, indices_list, _ = self.quantizer(fused)
        
        batch_size = fused.size(0)
        results = []
        for i in range(batch_size):
            semantic_ids = [indices[i].item() for indices in indices_list]
            results.append(semantic_ids)
        
        return results
```

### 2.4 点积聚合注意力 (Dot-Product Aggregated Attention)

#### 2.4.1 原理说明

传统 Softmax 注意力存在的问题：
- 归一化导致注意力权重必须和为 1，在非平稳分布下损失信息
- 推荐场景中物品分布高度不均匀（头部效应）

点积聚合注意力的优势（来源：HSTU）：
- 使用 ReLU/Sigmoid 替代 Softmax，避免强制归一化
- 更适合处理推荐场景的非平稳词汇表
- 支持高效的 M-FALCON 加速

#### 2.4.2 Python 实现

```python
"""
点积聚合注意力实现
对应架构文档: 3.2.2 节
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional


class DotProductAggregatedAttention(nn.Module):
    """
    点积聚合注意力 (HSTU)
    
    核心特点:
    1. 使用 ReLU 替代 Softmax，避免归一化信息损失
    2. 支持因果掩码 (Causal Mask) 用于自回归生成
    3. 支持 M-FALCON 加速策略
    
    对应架构文档: 3.2.2 节
    """
    
    def __init__(
        self, 
        d_model: int, 
        n_heads: int,
        dropout: float = 0.1,
        use_bias: bool = True,
    ):
        super().__init__()
        
        assert d_model % n_heads == 0, "d_model must be divisible by n_heads"
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        self.scale = 1.0 / math.sqrt(self.head_dim)
        
        # QKV 投影
        self.q_proj = nn.Linear(d_model, d_model, bias=use_bias)
        self.k_proj = nn.Linear(d_model, d_model, bias=use_bias)
        self.v_proj = nn.Linear(d_model, d_model, bias=use_bias)
        
        # 输出投影
        self.out_proj = nn.Linear(d_model, d_model, bias=use_bias)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
        # 激活函数选择 (ReLU 更稳定，Sigmoid 更平滑)
        self.activation = F.relu  # 或 torch.sigmoid
        
    def forward(
        self, 
        x: torch.Tensor,
        mask: Optional[torch.Tensor] = None,
        is_causal: bool = False,
    ) -> torch.Tensor:
        """
        前向传播
        
        Args:
            x: 输入张量 (batch_size, seq_len, d_model)
            mask: 注意力掩码 (batch_size, seq_len, seq_len) 或 None
            is_causal: 是否使用因果掩码
            
        Returns:
            输出张量 (batch_size, seq_len, d_model)
        """
        B, L, D = x.shape
        
        # 计算 Q, K, V
        Q = self.q_proj(x)  # (B, L, D)
        K = self.k_proj(x)  # (B, L, D)
        V = self.v_proj(x)  # (B, L, D)
        
        # 重塑为多头格式
        Q = Q.view(B, L, self.n_heads, self.head_dim).transpose(1, 2)  # (B, H, L, d)
        K = K.view(B, L, self.n_heads, self.head_dim).transpose(1, 2)  # (B, H, L, d)
        V = V.view(B, L, self.n_heads, self.head_dim).transpose(1, 2)  # (B, H, L, d)
        
        # 计算注意力分数
        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale  # (B, H, L, L)
        
        # 应用掩码
        if is_causal:
            causal_mask = torch.triu(
                torch.ones(L, L, dtype=torch.bool, device=x.device), 
                diagonal=1
            )
            scores = scores.masked_fill(causal_mask, float('-inf'))
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))
        
        # 使用 ReLU 替代 Softmax (核心区别)
        # 对于被掩码的位置，-inf 经过 ReLU 后变为 0
        attn_weights = self.activation(scores)
        
        # 归一化 (可选，用于稳定训练)
        # attn_weights = attn_weights / (attn_weights.sum(dim=-1, keepdim=True) + 1e-8)
        
        # Dropout
        attn_weights = self.dropout(attn_weights)
        
        # 加权聚合
        out = torch.matmul(attn_weights, V)  # (B, H, L, d)
        
        # 合并多头
        out = out.transpose(1, 2).contiguous().view(B, L, D)  # (B, L, D)
        
        # 输出投影
        out = self.out_proj(out)
        
        return out


class CausalSelfAttention(DotProductAggregatedAttention):
    """
    因果自注意力 (用于解码器)
    
    继承点积聚合注意力，默认启用因果掩码
    """
    
    def forward(
        self, 
        x: torch.Tensor,
        mask: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        return super().forward(x, mask=mask, is_causal=True)


class CrossAttention(nn.Module):
    """
    交叉注意力 (编码器-解码器连接)
    
    用于解码器关注编码器输出
    """
    
    def __init__(
        self, 
        d_model: int, 
        n_heads: int,
        dropout: float = 0.1,
    ):
        super().__init__()
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        self.scale = 1.0 / math.sqrt(self.head_dim)
        
        self.q_proj = nn.Linear(d_model, d_model)
        self.k_proj = nn.Linear(d_model, d_model)
        self.v_proj = nn.Linear(d_model, d_model)
        self.out_proj = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(
        self,
        query: torch.Tensor,       # 解码器状态
        key_value: torch.Tensor,   # 编码器输出
        mask: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        """
        Args:
            query: 解码器状态 (B, L_dec, D)
            key_value: 编码器输出 (B, L_enc, D)
            mask: 交叉注意力掩码
            
        Returns:
            输出 (B, L_dec, D)
        """
        B, L_dec, D = query.shape
        _, L_enc, _ = key_value.shape
        
        Q = self.q_proj(query).view(B, L_dec, self.n_heads, self.head_dim).transpose(1, 2)
        K = self.k_proj(key_value).view(B, L_enc, self.n_heads, self.head_dim).transpose(1, 2)
        V = self.v_proj(key_value).view(B, L_enc, self.n_heads, self.head_dim).transpose(1, 2)
        
        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))
        
        attn_weights = F.relu(scores)  # 点积聚合
        attn_weights = self.dropout(attn_weights)
        
        out = torch.matmul(attn_weights, V)
        out = out.transpose(1, 2).contiguous().view(B, L_dec, D)
        out = self.out_proj(out)
        
        return out
```

### 2.5 Group Layer Normalization (GLN)

#### 2.5.1 设计原理

推荐系统中的 Token 具有不同的语义空间：
- 用户行为 Token: 点击、收藏、购买等动作
- 物品属性 Token: 类目、品牌、价格等
- 上下文 Token: 时间、地点、设备等
- 交叉特征 Token: 用户-物品交互特征

传统 LayerNorm 对所有 Token 使用相同的归一化参数，无法捕捉不同语义空间的分布差异。

GLN 的核心思想（来源：MTGR）：
- 为不同类型的 Token 使用独立的归一化参数
- 保持各语义空间的分布特性
- 提升模型对异构特征的表达能力

#### 2.5.2 Python 实现

```python
"""
Group Layer Normalization 实现
对应架构文档: 3.2.3 节
"""

import torch
import torch.nn as nn
from typing import Dict, Optional


class GroupLayerNorm(nn.Module):
    """
    分组层归一化 (MTGR)
    
    针对不同语义空间的 Token 使用不同的归一化参数
    
    Token 类型分组:
    - Group 0: 用户行为 Token (ACTION_*)
    - Group 1: 物品属性 Token (ITEM_*, SEMANTIC_*)
    - Group 2: 上下文 Token (CONTEXT_*, TIME_*)
    - Group 3: 交叉特征 Token (CROSS_*)
    
    对应架构文档: 3.2.3 节
    """
    
    # Token 类型到分组的映射
    TOKEN_TYPE_TO_GROUP = {
        "USER": 0,
        "ACTION": 0,
        "ITEM": 1,
        "SEMANTIC": 1,
        "CONTEXT": 2,
        "TIME": 2,
        "CROSS": 3,
        "FEATURE": 3,
    }
    
    def __init__(
        self, 
        d_model: int, 
        num_groups: int = 4,
        eps: float = 1e-6,
    ):
        super().__init__()
        
        self.d_model = d_model
        self.num_groups = num_groups
        self.eps = eps
        
        # 为每个分组创建独立的 LayerNorm
        self.norms = nn.ModuleList([
            nn.LayerNorm(d_model, eps=eps)
            for _ in range(num_groups)
        ])
        
    def forward(
        self, 
        x: torch.Tensor, 
        token_types: torch.Tensor,
    ) -> torch.Tensor:
        """
        分组归一化
        
        Args:
            x: 输入张量 (batch_size, seq_len, d_model)
            token_types: Token 类型标识 (batch_size, seq_len)
                        取值范围 [0, num_groups-1]
            
        Returns:
            归一化后的张量 (batch_size, seq_len, d_model)
        """
        B, L, D = x.shape
        output = torch.zeros_like(x)
        
        # 对每个分组分别进行归一化
        for group_id in range(self.num_groups):
            # 找到属于该分组的位置
            mask = (token_types == group_id)  # (B, L)
            
            if mask.any():
                # 提取该分组的 Token
                group_tokens = x[mask]  # (num_tokens, D)
                
                # 应用对应的 LayerNorm
                normalized = self.norms[group_id](group_tokens)
                
                # 放回原位置
                output[mask] = normalized
        
        return output
    
    @classmethod
    def get_token_group(cls, token_prefix: str) -> int:
        """
        根据 Token 前缀获取分组 ID
        
        Args:
            token_prefix: Token 前缀，如 "ACTION", "ITEM"
            
        Returns:
            分组 ID
        """
        return cls.TOKEN_TYPE_TO_GROUP.get(token_prefix, 3)


class AdaptiveGroupLayerNorm(nn.Module):
    """
    自适应分组层归一化
    
    通过可学习的门控机制动态选择归一化参数
    """
    
    def __init__(
        self, 
        d_model: int, 
        num_groups: int = 4,
        eps: float = 1e-6,
    ):
        super().__init__()
        
        self.d_model = d_model
        self.num_groups = num_groups
        
        # 分组 LayerNorm
        self.norms = nn.ModuleList([
            nn.LayerNorm(d_model, eps=eps)
            for _ in range(num_groups)
        ])
        
        # 门控网络：学习每个位置应该使用哪个分组
        self.gate = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Linear(d_model // 2, num_groups),
            nn.Softmax(dim=-1),
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        自适应归一化
        
        Args:
            x: 输入张量 (batch_size, seq_len, d_model)
            
        Returns:
            归一化后的张量 (batch_size, seq_len, d_model)
        """
        # 计算门控权重
        gate_weights = self.gate(x)  # (B, L, num_groups)
        
        # 对每个分组进行归一化
        normalized_outputs = []
        for norm in self.norms:
            normalized_outputs.append(norm(x))
        
        # 堆叠: (B, L, num_groups, D)
        stacked = torch.stack(normalized_outputs, dim=2)
        
        # 加权求和: (B, L, D)
        output = torch.einsum('blg,blgd->bld', gate_weights, stacked)
        
        return output
```

### 2.6 MoE 增强 FFN

#### 2.6.1 设计原理

Mixture-of-Experts (MoE) 的核心思想：
- 使用多个专家网络，每次只激活部分专家
- 不同类型的推荐任务激活不同的专家组合
- 在增加模型容量的同时控制计算成本

专家分配策略（来源：OneRec）：
- Expert 1-4: 视频推荐专家
- Expert 5-8: 商品推荐专家
- Expert 9-12: 文章推荐专家
- Expert 13-16: 通用专家

#### 2.6.2 Python 实现

```python
"""
MoE 增强 FFN 实现
对应架构文档: 3.2.4 节
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, Optional


class Expert(nn.Module):
    """单个专家网络"""
    
    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout),
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.net(x)


class TopKRouter(nn.Module):
    """
    Top-K 路由器
    
    为每个 Token 选择 Top-K 个专家
    """
    
    def __init__(
        self, 
        d_model: int, 
        num_experts: int, 
        top_k: int = 4,
        noise_std: float = 0.1,
    ):
        super().__init__()
        
        self.num_experts = num_experts
        self.top_k = top_k
        self.noise_std = noise_std
        
        # 门控网络
        self.gate = nn.Linear(d_model, num_experts, bias=False)
        
    def forward(
        self, 
        x: torch.Tensor,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        路由决策
        
        Args:
            x: 输入张量 (batch_size, seq_len, d_model)
            
        Returns:
            gate_weights: 门控权重 (batch_size, seq_len, top_k)
            selected_experts: 选中的专家索引 (batch_size, seq_len, top_k)
            load_balance_loss: 负载均衡损失
        """
        # 计算门控 logits
        logits = self.gate(x)  # (B, L, num_experts)
        
        # 训练时添加噪声以促进探索
        if self.training and self.noise_std > 0:
            noise = torch.randn_like(logits) * self.noise_std
            logits = logits + noise
        
        # 选择 Top-K 专家
        gate_weights, selected_experts = torch.topk(
            F.softmax(logits, dim=-1),
            self.top_k,
            dim=-1,
        )
        
        # 重新归一化权重
        gate_weights = gate_weights / gate_weights.sum(dim=-1, keepdim=True)
        
        # 计算负载均衡损失
        load_balance_loss = self._compute_load_balance_loss(logits)
        
        return gate_weights, selected_experts, load_balance_loss
    
    def _compute_load_balance_loss(self, logits: torch.Tensor) -> torch.Tensor:
        """
        计算负载均衡损失
        
        鼓励所有专家被均匀使用，避免专家坍塌
        """
        # 计算每个专家被选中的概率
        probs = F.softmax(logits, dim=-1)  # (B, L, num_experts)
        
        # 平均专家使用率
        mean_probs = probs.mean(dim=[0, 1])  # (num_experts,)
        
        # 方差损失：鼓励均匀分布
        target = 1.0 / self.num_experts
        load_balance_loss = ((mean_probs - target) ** 2).sum()
        
        return load_balance_loss


class MoEFeedForward(nn.Module):
    """
    Mixture-of-Experts 前馈网络
    
    核心特点:
    1. 稀疏激活：每次只激活 Top-K 个专家
    2. 负载均衡：通过辅助损失避免专家坍塌
    3. 专家专业化：不同专家处理不同类型的推荐
    
    对应架构文档: 3.2.4 节
    """
    
    def __init__(
        self,
        d_model: int,
        d_ff: int,
        num_experts: int = 16,
        top_k: int = 4,
        dropout: float = 0.1,
    ):
        super().__init__()
        
        self.d_model = d_model
        self.num_experts = num_experts
        self.top_k = top_k
        
        # 路由器
        self.router = TopKRouter(d_model, num_experts, top_k)
        
        # 专家网络
        self.experts = nn.ModuleList([
            Expert(d_model, d_ff, dropout)
            for _ in range(num_experts)
        ])
        
        # 记录负载均衡损失 (用于训练)
        self.last_load_balance_loss = None
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        MoE 前向传播
        
        Args:
            x: 输入张量 (batch_size, seq_len, d_model)
            
        Returns:
            输出张量 (batch_size, seq_len, d_model)
        """
        B, L, D = x.shape
        
        # 路由决策
        gate_weights, selected_experts, load_balance_loss = self.router(x)
        self.last_load_balance_loss = load_balance_loss
        
        # 初始化输出
        output = torch.zeros_like(x)
        
        # 展平以便处理
        x_flat = x.view(-1, D)  # (B*L, D)
        gate_weights_flat = gate_weights.view(-1, self.top_k)  # (B*L, top_k)
        selected_experts_flat = selected_experts.view(-1, self.top_k)  # (B*L, top_k)
        
        # 对每个专家处理
        for expert_idx in range(self.num_experts):
            # 找到选择该专家的 Token
            expert_mask = (selected_experts_flat == expert_idx)  # (B*L, top_k)
            
            if expert_mask.any():
                # 获取对应的权重
                weights = gate_weights_flat[expert_mask]  # (num_selected,)
                
                # 获取选择该专家的 Token 索引
                token_indices = expert_mask.any(dim=1).nonzero(as_tuple=True)[0]
                
                # 处理这些 Token
                expert_input = x_flat[token_indices]  # (num_selected, D)
                expert_output = self.experts[expert_idx](expert_input)
                
                # 加权累加到输出
                for i, idx in enumerate(token_indices):
                    # 找到该 Token 中该专家的权重
                    k_idx = expert_mask[idx].nonzero(as_tuple=True)[0]
                    if len(k_idx) > 0:
                        weight = gate_weights_flat[idx, k_idx[0]]
                        output.view(-1, D)[idx] += weight * expert_output[i]
        
        return output


class SimplifiedMoE(nn.Module):
    """
    简化版 MoE (更高效的实现)
    
    使用批量操作代替循环，提升计算效率
    """
    
    def __init__(
        self,
        d_model: int,
        d_ff: int,
        num_experts: int = 16,
        top_k: int = 4,
        dropout: float = 0.1,
    ):
        super().__init__()
        
        self.num_experts = num_experts
        self.top_k = top_k
        
        # 所有专家共享的第一层权重 (用于批量计算)
        self.w1 = nn.Parameter(torch.randn(num_experts, d_model, d_ff) * 0.02)
        self.w2 = nn.Parameter(torch.randn(num_experts, d_ff, d_model) * 0.02)
        
        # 门控
        self.gate = nn.Linear(d_model, num_experts, bias=False)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, L, D = x.shape
        
        # 门控
        logits = self.gate(x)  # (B, L, num_experts)
        weights, indices = torch.topk(F.softmax(logits, dim=-1), self.top_k, dim=-1)
        weights = weights / weights.sum(dim=-1, keepdim=True)  # 归一化
        
        # 批量专家计算
        # 这里使用 einsum 进行高效计算
        # 每个 token 只计算 top_k 个专家
        
        output = torch.zeros_like(x)
        
        for k in range(self.top_k):
            expert_idx = indices[..., k]  # (B, L)
            expert_weight = weights[..., k:k+1]  # (B, L, 1)
            
            # 获取对应专家的权重
            w1_selected = self.w1[expert_idx]  # (B, L, D, d_ff)
            w2_selected = self.w2[expert_idx]  # (B, L, d_ff, D)
            
            # 前向传播
            hidden = torch.einsum('bld,bldf->blf', x, w1_selected)
            hidden = F.gelu(hidden)
            hidden = self.dropout(hidden)
            out = torch.einsum('blf,blfd->bld', hidden, w2_selected)
            
            output = output + expert_weight * out
        
        return output
```

### 2.7 完整 UGT 模型

#### 2.7.1 编码器层

```python
"""
UGT 编码器层实现
对应架构文档: 3.1 节
"""

class EncoderLayer(nn.Module):
    """
    UGT 编码器层
    
    结构: Dot-Product Aggregated Attention → FFN → GLN
    """
    
    def __init__(
        self,
        d_model: int,
        n_heads: int,
        d_ff: int,
        num_groups: int = 4,
        dropout: float = 0.1,
    ):
        super().__init__()
        
        # 点积聚合注意力
        self.self_attn = DotProductAggregatedAttention(
            d_model=d_model,
            n_heads=n_heads,
            dropout=dropout,
        )
        
        # 前馈网络 (普通 FFN，编码器不使用 MoE)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout),
        )
        
        # Group Layer Normalization
        self.norm1 = GroupLayerNorm(d_model, num_groups)
        self.norm2 = GroupLayerNorm(d_model, num_groups)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
    def forward(
        self, 
        x: torch.Tensor, 
        token_types: torch.Tensor,
        mask: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        """
        Args:
            x: 输入 (batch_size, seq_len, d_model)
            token_types: Token 类型 (batch_size, seq_len)
            mask: 注意力掩码
            
        Returns:
            输出 (batch_size, seq_len, d_model)
        """
        # Self-Attention + Residual
        attn_out = self.self_attn(x, mask=mask)
        x = x + self.dropout(attn_out)
        x = self.norm1(x, token_types)
        
        # FFN + Residual
        ffn_out = self.ffn(x)
        x = x + ffn_out
        x = self.norm2(x, token_types)
        
        return x
```

#### 2.7.2 解码器层

```python
class DecoderLayer(nn.Module):
    """
    UGT 解码器层
    
    结构: Causal Attention → Cross Attention → MoE FFN → GLN
    """
    
    def __init__(
        self,
        d_model: int,
        n_heads: int,
        d_ff: int,
        num_experts: int = 16,
        top_k: int = 4,
        num_groups: int = 4,
        dropout: float = 0.1,
    ):
        super().__init__()
        
        # 因果自注意力
        self.self_attn = CausalSelfAttention(
            d_model=d_model,
            n_heads=n_heads,
            dropout=dropout,
        )
        
        # 交叉注意力
        self.cross_attn = CrossAttention(
            d_model=d_model,
            n_heads=n_heads,
            dropout=dropout,
        )
        
        # MoE FFN
        self.moe_ffn = MoEFeedForward(
            d_model=d_model,
            d_ff=d_ff,
            num_experts=num_experts,
            top_k=top_k,
            dropout=dropout,
        )
        
        # Group Layer Normalization
        self.norm1 = GroupLayerNorm(d_model, num_groups)
        self.norm2 = GroupLayerNorm(d_model, num_groups)
        self.norm3 = GroupLayerNorm(d_model, num_groups)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(
        self,
        x: torch.Tensor,
        encoder_output: torch.Tensor,
        token_types: torch.Tensor,
        self_mask: Optional[torch.Tensor] = None,
        cross_mask: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        """
        Args:
            x: 解码器输入 (batch_size, tgt_len, d_model)
            encoder_output: 编码器输出 (batch_size, src_len, d_model)
            token_types: Token 类型
            self_mask: 自注意力掩码
            cross_mask: 交叉注意力掩码
            
        Returns:
            输出 (batch_size, tgt_len, d_model)
        """
        # 因果自注意力
        attn_out = self.self_attn(x, mask=self_mask)
        x = x + self.dropout(attn_out)
        x = self.norm1(x, token_types)
        
        # 交叉注意力
        cross_out = self.cross_attn(x, encoder_output, mask=cross_mask)
        x = x + self.dropout(cross_out)
        x = self.norm2(x, token_types)
        
        # MoE FFN
        ffn_out = self.moe_ffn(x)
        x = x + ffn_out
        x = self.norm3(x, token_types)
        
        return x
```

#### 2.7.3 完整 UGT 模型

```python
"""
完整 UGT 模型实现
对应架构文档: 第三章
"""

from dataclasses import dataclass
from typing import List, Optional, Tuple, Dict
import torch
import torch.nn as nn
import torch.nn.functional as F


@dataclass
class UGTConfig:
    """UGT 模型配置"""
    # 模型维度
    d_model: int = 512
    n_heads: int = 16
    n_enc_layers: int = 12
    n_dec_layers: int = 12
    d_ff: int = 2048
    
    # MoE 配置
    num_experts: int = 16
    top_k_experts: int = 4
    
    # 序列配置
    max_seq_len: int = 1024
    vocab_size: int = 500000  # 语义 ID 词表大小
    
    # Token 类型
    num_token_types: int = 4  # USER, ITEM, CONTEXT, CROSS
    num_groups: int = 4  # GLN 分组数
    
    # 正则化
    dropout: float = 0.1
    
    # 语义 ID 配置
    semantic_id_levels: int = 3
    codebook_sizes: Tuple[int, ...] = (1024, 4096, 16384)


class InputEmbedding(nn.Module):
    """
    统一输入嵌入层
    
    E_input = E_semantic + E_position + E_type + E_time + E_feature
    
    对应架构文档: 3.1 节 Unified Input Embedding
    """
    
    def __init__(self, config: UGTConfig):
        super().__init__()
        self.config = config
        
        # 语义 ID 嵌入 (分层)
        self.semantic_embeddings = nn.ModuleList([
            nn.Embedding(size, config.d_model)
            for size in config.codebook_sizes
        ])
        
        # 位置嵌入
        self.position_embedding = nn.Embedding(config.max_seq_len, config.d_model)
        
        # Token 类型嵌入
        self.type_embedding = nn.Embedding(config.num_token_types, config.d_model)
        
        # 时间嵌入 (周期性编码)
        self.time_embedding = nn.Linear(64, config.d_model)  # 64维时间特征
        
        # 特征嵌入投影
        self.feature_projection = nn.Linear(128, config.d_model)  # 附加特征
        
        # 融合层
        self.fusion = nn.Sequential(
            nn.Linear(config.d_model * 5, config.d_model),
            nn.LayerNorm(config.d_model),
            nn.Dropout(config.dropout),
        )
        
    def forward(
        self,
        semantic_ids: List[torch.Tensor],  # 各层语义 ID
        positions: torch.Tensor,
        token_types: torch.Tensor,
        time_features: Optional[torch.Tensor] = None,
        extra_features: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        """
        Args:
            semantic_ids: 语义 ID 列表 [(B, L), (B, L), (B, L)]
            positions: 位置索引 (B, L)
            token_types: Token 类型 (B, L)
            time_features: 时间特征 (B, L, 64)
            extra_features: 附加特征 (B, L, 128)
            
        Returns:
            嵌入张量 (B, L, d_model)
        """
        B, L = positions.shape
        
        # 语义嵌入 (多层累加)
        semantic_emb = torch.zeros(B, L, self.config.d_model, device=positions.device)
        for i, (ids, embedding) in enumerate(zip(semantic_ids, self.semantic_embeddings)):
            semantic_emb = semantic_emb + embedding(ids)
        
        # 位置嵌入
        position_emb = self.position_embedding(positions)
        
        # 类型嵌入
        type_emb = self.type_embedding(token_types)
        
        # 时间嵌入
        if time_features is not None:
            time_emb = self.time_embedding(time_features)
        else:
            time_emb = torch.zeros(B, L, self.config.d_model, device=positions.device)
        
        # 特征嵌入
        if extra_features is not None:
            feature_emb = self.feature_projection(extra_features)
        else:
            feature_emb = torch.zeros(B, L, self.config.d_model, device=positions.device)
        
        # 融合所有嵌入
        combined = torch.cat([
            semantic_emb, position_emb, type_emb, time_emb, feature_emb
        ], dim=-1)
        
        output = self.fusion(combined)
        
        return output


class UGTEncoder(nn.Module):
    """
    UGT 编码器
    
    多尺度用户行为建模:
    - Long-Term: 压缩的历史行为
    - Short-Term: 近期会话
    - Real-Time: 当前上下文
    """
    
    def __init__(self, config: UGTConfig):
        super().__init__()
        
        self.layers = nn.ModuleList([
            EncoderLayer(
                d_model=config.d_model,
                n_heads=config.n_heads,
                d_ff=config.d_ff,
                num_groups=config.num_groups,
                dropout=config.dropout,
            )
            for _ in range(config.n_enc_layers)
        ])
        
    def forward(
        self,
        x: torch.Tensor,
        token_types: torch.Tensor,
        mask: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        for layer in self.layers:
            x = layer(x, token_types, mask)
        return x


class UGTDecoder(nn.Module):
    """UGT 解码器"""
    
    def __init__(self, config: UGTConfig):
        super().__init__()
        
        self.layers = nn.ModuleList([
            DecoderLayer(
                d_model=config.d_model,
                n_heads=config.n_heads,
                d_ff=config.d_ff,
                num_experts=config.num_experts,
                top_k=config.top_k_experts,
                num_groups=config.num_groups,
                dropout=config.dropout,
            )
            for _ in range(config.n_dec_layers)
        ])
        
    def forward(
        self,
        x: torch.Tensor,
        encoder_output: torch.Tensor,
        token_types: torch.Tensor,
        self_mask: Optional[torch.Tensor] = None,
        cross_mask: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        for layer in self.layers:
            x = layer(x, encoder_output, token_types, self_mask, cross_mask)
        return x


class UGT(nn.Module):
    """
    Unified Generative Transformer
    
    完整的生成式推荐模型
    
    对应架构文档: 第三章
    """
    
    def __init__(self, config: UGTConfig):
        super().__init__()
        self.config = config
        
        # 输入嵌入
        self.input_embedding = InputEmbedding(config)
        
        # 编码器
        self.encoder = UGTEncoder(config)
        
        # 解码器
        self.decoder = UGTDecoder(config)
        
        # 输出头 (预测下一个语义 ID)
        self.output_heads = nn.ModuleList([
            nn.Linear(config.d_model, size)
            for size in config.codebook_sizes
        ])
        
        # 初始化权重
        self.apply(self._init_weights)
        
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            
    def forward(
        self,
        encoder_semantic_ids: List[torch.Tensor],
        encoder_positions: torch.Tensor,
        encoder_token_types: torch.Tensor,
        decoder_semantic_ids: List[torch.Tensor],
        decoder_positions: torch.Tensor,
        decoder_token_types: torch.Tensor,
        encoder_time_features: Optional[torch.Tensor] = None,
        decoder_time_features: Optional[torch.Tensor] = None,
    ) -> List[torch.Tensor]:
        """
        前向传播
        
        Returns:
            各层语义 ID 的 logits 列表
        """
        # 编码器嵌入
        encoder_input = self.input_embedding(
            encoder_semantic_ids,
            encoder_positions,
            encoder_token_types,
            encoder_time_features,
        )
        
        # 编码
        encoder_output = self.encoder(encoder_input, encoder_token_types)
        
        # 解码器嵌入
        decoder_input = self.input_embedding(
            decoder_semantic_ids,
            decoder_positions,
            decoder_token_types,
            decoder_time_features,
        )
        
        # 解码
        decoder_output = self.decoder(
            decoder_input,
            encoder_output,
            decoder_token_types,
        )
        
        # 输出各层语义 ID 的 logits
        logits_list = [
            head(decoder_output)
            for head in self.output_heads
        ]
        
        return logits_list
    
    def generate(
        self,
        encoder_semantic_ids: List[torch.Tensor],
        encoder_positions: torch.Tensor,
        encoder_token_types: torch.Tensor,
        max_length: int = 10,
        temperature: float = 1.0,
        top_k: int = 50,
    ) -> List[List[int]]:
        """
        自回归生成推荐序列
        
        Args:
            encoder_*: 编码器输入 (用户历史)
            max_length: 最大生成长度
            temperature: 采样温度
            top_k: Top-K 采样
            
        Returns:
            生成的语义 ID 序列列表
        """
        self.eval()
        
        with torch.no_grad():
            # 编码用户历史
            encoder_input = self.input_embedding(
                encoder_semantic_ids,
                encoder_positions,
                encoder_token_types,
            )
            encoder_output = self.encoder(encoder_input, encoder_token_types)
            
            # 初始化解码器输入 (BOS token)
            batch_size = encoder_output.size(0)
            device = encoder_output.device
            
            generated_ids = [[] for _ in range(self.config.semantic_id_levels)]
            
            for step in range(max_length):
                # 构建解码器输入
                if step == 0:
                    # 使用特殊的开始 token
                    dec_ids = [
                        torch.zeros(batch_size, 1, dtype=torch.long, device=device)
                        for _ in range(self.config.semantic_id_levels)
                    ]
                else:
                    dec_ids = [
                        torch.tensor(ids, device=device).unsqueeze(0)
                        for ids in generated_ids
                    ]
                
                dec_positions = torch.arange(step + 1, device=device).unsqueeze(0).expand(batch_size, -1)
                dec_token_types = torch.ones(batch_size, step + 1, dtype=torch.long, device=device)
                
                # 解码
                decoder_input = self.input_embedding(dec_ids, dec_positions, dec_token_types)
                decoder_output = self.decoder(decoder_input, encoder_output, dec_token_types)
                
                # 获取最后一个位置的 logits
                for level, head in enumerate(self.output_heads):
                    logits = head(decoder_output[:, -1, :]) / temperature
                    
                    # Top-K 采样
                    if top_k > 0:
                        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]
                        logits[indices_to_remove] = float('-inf')
                    
                    probs = F.softmax(logits, dim=-1)
                    next_id = torch.multinomial(probs, num_samples=1).item()
                    generated_ids[level].append(next_id)
            
            return generated_ids
    
    def get_moe_load_balance_loss(self) -> torch.Tensor:
        """获取所有 MoE 层的负载均衡损失"""
        total_loss = 0.0
        count = 0
        
        for layer in self.decoder.layers:
            if hasattr(layer.moe_ffn, 'last_load_balance_loss'):
                loss = layer.moe_ffn.last_load_balance_loss
                if loss is not None:
                    total_loss = total_loss + loss
                    count += 1
        
        return total_loss / max(count, 1)
```

---

## 3. 训练流程

### 3.1 三阶段训练策略

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         三阶段训练流程                                        │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Stage 1: Semantic ID 预训练 (2-3 天)                                        │
│  ┌───────────────────────────────────────────────────────────────────────┐ │
│  │  目标: 学习物品的语义表示                                               │ │
│  │  数据: 物品多模态特征                                                   │ │
│  │  损失: RQ-VAE 重建损失 + 对比学习损失                                    │ │
│  │  输出: 预训练的 Semantic ID 编码器                                       │ │
│  └───────────────────────────────────────────────────────────────────────┘ │
│                                    ↓                                        │
│  Stage 2: 生成模型预训练 (5-7 天)                                            │
│  ┌───────────────────────────────────────────────────────────────────────┐ │
│  │  目标: 学习用户行为序列模式                                             │ │
│  │  数据: 用户行为序列 (tokenized)                                         │ │
│  │  损失: Next Token Prediction (NTP) 损失                                 │ │
│  │  输出: 预训练的 UGT 模型                                                │ │
│  └───────────────────────────────────────────────────────────────────────┘ │
│                                    ↓                                        │
│  Stage 3: 偏好对齐 (1-2 天)                                                  │
│  ┌───────────────────────────────────────────────────────────────────────┐ │
│  │  目标: 对齐用户真实偏好                                                 │ │
│  │  数据: 偏好对 (chosen, rejected)                                        │ │
│  │  损失: DPO (Direct Preference Optimization) 损失                        │ │
│  │  输出: 最终的生产模型                                                   │ │
│  └───────────────────────────────────────────────────────────────────────┘ │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 3.2 损失函数

#### 3.2.1 统一训练损失

```python
"""
训练损失函数实现
对应架构文档: 8.2 节
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional


class UnifiedTrainingLoss(nn.Module):
    """
    统一训练损失函数
    
    L_total = L_ntp + λ₁ * L_contrastive + λ₂ * L_preference + λ₃ * L_load_balance
    
    对应架构文档: 8.2 节
    """
    
    def __init__(
        self,
        lambda_contrastive: float = 0.1,
        lambda_preference: float = 0.05,
        lambda_load_balance: float = 0.01,
        label_smoothing: float = 0.1,
    ):
        super().__init__()
        self.lambda_contrastive = lambda_contrastive
        self.lambda_preference = lambda_preference
        self.lambda_load_balance = lambda_load_balance
        
        self.ntp_criterion = nn.CrossEntropyLoss(
            ignore_index=-100,
            label_smoothing=label_smoothing,
        )
        
    def forward(
        self,
        logits_list: List[torch.Tensor],
        targets_list: List[torch.Tensor],
        embeddings: Optional[torch.Tensor] = None,
        chosen_logits: Optional[torch.Tensor] = None,
        rejected_logits: Optional[torch.Tensor] = None,
        load_balance_loss: Optional[torch.Tensor] = None,
    ) -> Dict[str, torch.Tensor]:
        """
        计算总损失
        
        Args:
            logits_list: 各层语义 ID 的 logits [(B, L, V1), (B, L, V2), ...]
            targets_list: 各层目标 [(B, L), (B, L), ...]
            embeddings: 用于对比学习的嵌入 (B, L, D)
            chosen_logits: 偏好对齐中的正例 logits
            rejected_logits: 偏好对齐中的负例 logits
            load_balance_loss: MoE 负载均衡损失
            
        Returns:
            损失字典 {"total", "ntp", "contrastive", "preference", "load_balance"}
        """
        losses = {}
        
        # 1. Next Token Prediction 损失 (核心损失)
        ntp_loss = 0.0
        for logits, targets in zip(logits_list, targets_list):
            # logits: (B, L, V), targets: (B, L)
            B, L, V = logits.shape
            ntp_loss = ntp_loss + self.ntp_criterion(
                logits.view(-1, V),
                targets.view(-1),
            )
        ntp_loss = ntp_loss / len(logits_list)
        losses["ntp"] = ntp_loss
        
        # 2. 对比学习损失 (可选)
        if embeddings is not None:
            contrastive_loss = self._compute_contrastive_loss(embeddings)
            losses["contrastive"] = contrastive_loss
        else:
            contrastive_loss = 0.0
            losses["contrastive"] = torch.tensor(0.0)
        
        # 3. 偏好对齐损失 (DPO, 可选)
        if chosen_logits is not None and rejected_logits is not None:
            preference_loss = self._compute_dpo_loss(chosen_logits, rejected_logits)
            losses["preference"] = preference_loss
        else:
            preference_loss = 0.0
            losses["preference"] = torch.tensor(0.0)
        
        # 4. MoE 负载均衡损失
        if load_balance_loss is not None:
            losses["load_balance"] = load_balance_loss
        else:
            load_balance_loss = 0.0
            losses["load_balance"] = torch.tensor(0.0)
        
        # 总损失
        total_loss = (
            ntp_loss
            + self.lambda_contrastive * contrastive_loss
            + self.lambda_preference * preference_loss
            + self.lambda_load_balance * load_balance_loss
        )
        losses["total"] = total_loss
        
        return losses
    
    def _compute_contrastive_loss(
        self, 
        embeddings: torch.Tensor,
        temperature: float = 0.07,
    ) -> torch.Tensor:
        """
        计算对比学习损失 (InfoNCE)
        
        Args:
            embeddings: 嵌入张量 (B, L, D)
            temperature: 温度参数
            
        Returns:
            对比损失
        """
        B, L, D = embeddings.shape
        
        # 取序列中间位置作为锚点
        anchor = embeddings[:, L // 2, :]  # (B, D)
        
        # 正例：同一序列的其他位置
        positive = embeddings[:, L // 2 + 1, :]  # (B, D)
        
        # 归一化
        anchor = F.normalize(anchor, dim=-1)
        positive = F.normalize(positive, dim=-1)
        
        # 计算相似度
        pos_sim = (anchor * positive).sum(dim=-1) / temperature  # (B,)
        
        # 负例：batch 内其他样本
        neg_sim = torch.mm(anchor, anchor.T) / temperature  # (B, B)
        
        # 移除对角线 (自身)
        mask = torch.eye(B, dtype=torch.bool, device=anchor.device)
        neg_sim = neg_sim.masked_fill(mask, float('-inf'))
        
        # InfoNCE 损失
        logits = torch.cat([pos_sim.unsqueeze(1), neg_sim], dim=1)  # (B, 1+B)
        labels = torch.zeros(B, dtype=torch.long, device=anchor.device)
        
        loss = F.cross_entropy(logits, labels)
        
        return loss
    
    def _compute_dpo_loss(
        self,
        chosen_logits: torch.Tensor,
        rejected_logits: torch.Tensor,
        beta: float = 0.1,
    ) -> torch.Tensor:
        """
        计算 DPO (Direct Preference Optimization) 损失
        
        Args:
            chosen_logits: 正例的 log 概率
            rejected_logits: 负例的 log 概率
            beta: DPO 温度参数
            
        Returns:
            DPO 损失
        """
        # DPO 损失公式: -log(sigmoid(beta * (log_p_chosen - log_p_rejected)))
        log_ratio = chosen_logits - rejected_logits
        loss = -F.logsigmoid(beta * log_ratio).mean()
        
        return loss


class ContrastiveLoss(nn.Module):
    """
    对比学习损失 (用于 Semantic ID 预训练)
    
    正例: 同一物品的不同视图 (augmentation)
    负例: batch 内其他物品
    
    对应架构文档: 8.2.1 节
    """
    
    def __init__(self, temperature: float = 0.07):
        super().__init__()
        self.temperature = temperature
        
    def forward(
        self, 
        z1: torch.Tensor, 
        z2: torch.Tensor,
    ) -> torch.Tensor:
        """
        Args:
            z1: 第一视图嵌入 (B, D)
            z2: 第二视图嵌入 (B, D)
            
        Returns:
            对比损失
        """
        B, D = z1.shape
        
        # 归一化
        z1 = F.normalize(z1, dim=-1)
        z2 = F.normalize(z2, dim=-1)
        
        # 合并所有嵌入
        embeddings = torch.cat([z1, z2], dim=0)  # (2B, D)
        
        # 计算相似度矩阵
        sim_matrix = torch.mm(embeddings, embeddings.T) / self.temperature  # (2B, 2B)
        
        # 创建标签: z1[i] 的正例是 z2[i]
        labels = torch.arange(B, device=z1.device)
        labels = torch.cat([labels + B, labels], dim=0)  # (2B,)
        
        # 移除对角线
        mask = torch.eye(2 * B, dtype=torch.bool, device=z1.device)
        sim_matrix = sim_matrix.masked_fill(mask, float('-inf'))
        
        # 交叉熵损失
        loss = F.cross_entropy(sim_matrix, labels)
        
        return loss


class DPOLoss(nn.Module):
    """
    Direct Preference Optimization 损失
    
    用于偏好对齐阶段，无需奖励模型
    
    对应架构文档: 8.2.3 节
    """
    
    def __init__(self, beta: float = 0.1, label_smoothing: float = 0.0):
        super().__init__()
        self.beta = beta
        self.label_smoothing = label_smoothing
        
    def forward(
        self,
        policy_chosen_logps: torch.Tensor,
        policy_rejected_logps: torch.Tensor,
        reference_chosen_logps: torch.Tensor,
        reference_rejected_logps: torch.Tensor,
    ) -> Dict[str, torch.Tensor]:
        """
        计算 DPO 损失
        
        Args:
            policy_chosen_logps: 策略模型对正例的 log 概率 (B,)
            policy_rejected_logps: 策略模型对负例的 log 概率 (B,)
            reference_chosen_logps: 参考模型对正例的 log 概率 (B,)
            reference_rejected_logps: 参考模型对负例的 log 概率 (B,)
            
        Returns:
            损失字典
        """
        # 计算 log ratio
        pi_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = reference_chosen_logps - reference_rejected_logps
        
        # DPO 损失
        logits = pi_logratios - ref_logratios
        
        if self.label_smoothing > 0:
            # 标签平滑
            losses = (
                -F.logsigmoid(self.beta * logits) * (1 - self.label_smoothing)
                - F.logsigmoid(-self.beta * logits) * self.label_smoothing
            )
        else:
            losses = -F.logsigmoid(self.beta * logits)
        
        loss = losses.mean()
        
        # 统计信息
        chosen_rewards = self.beta * (policy_chosen_logps - reference_chosen_logps).detach()
        rejected_rewards = self.beta * (policy_rejected_logps - reference_rejected_logps).detach()
        reward_margin = (chosen_rewards - rejected_rewards).mean()
        
        return {
            "loss": loss,
            "chosen_rewards": chosen_rewards.mean(),
            "rejected_rewards": rejected_rewards.mean(),
            "reward_margin": reward_margin,
            "accuracy": (chosen_rewards > rejected_rewards).float().mean(),
        }
```

### 3.3 训练配置

```python
"""
训练配置和训练器实现
对应架构文档: 8.1 节
"""

from dataclasses import dataclass, field
from typing import Optional, List
import torch
from torch.utils.data import DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
import os


@dataclass
class TrainingConfig:
    """训练配置"""
    
    # 基础配置
    output_dir: str = "./checkpoints"
    run_name: str = "ugt_training"
    
    # 训练阶段
    stage: str = "pretrain"  # "semantic_id", "pretrain", "dpo"
    
    # 批次配置
    batch_size: int = 256
    gradient_accumulation_steps: int = 4
    effective_batch_size: int = field(init=False)
    
    # 优化器配置
    learning_rate: float = 1e-4
    weight_decay: float = 0.01
    adam_beta1: float = 0.9
    adam_beta2: float = 0.999
    adam_epsilon: float = 1e-8
    max_grad_norm: float = 1.0
    
    # 学习率调度
    warmup_steps: int = 2000
    max_steps: int = 100000
    lr_scheduler_type: str = "cosine"
    
    # 训练配置
    num_epochs: int = 10
    logging_steps: int = 100
    save_steps: int = 1000
    eval_steps: int = 500
    
    # 分布式配置
    num_gpus: int = 8
    deepspeed_config: Optional[str] = None
    fp16: bool = True
    bf16: bool = False
    
    # 损失权重
    lambda_contrastive: float = 0.1
    lambda_preference: float = 0.05
    lambda_load_balance: float = 0.01
    
    def __post_init__(self):
        self.effective_batch_size = (
            self.batch_size * self.gradient_accumulation_steps * self.num_gpus
        )


class UGTTrainer:
    """
    UGT 模型训练器
    
    支持:
    - 三阶段训练
    - 分布式训练 (DeepSpeed)
    - 混合精度
    - 梯度累积
    - 检查点保存/恢复
    """
    
    def __init__(
        self,
        model: nn.Module,
        config: TrainingConfig,
        train_dataloader: DataLoader,
        eval_dataloader: Optional[DataLoader] = None,
    ):
        self.model = model
        self.config = config
        self.train_dataloader = train_dataloader
        self.eval_dataloader = eval_dataloader
        
        # 损失函数
        self.loss_fn = UnifiedTrainingLoss(
            lambda_contrastive=config.lambda_contrastive,
            lambda_preference=config.lambda_preference,
            lambda_load_balance=config.lambda_load_balance,
        )
        
        # 优化器
        self.optimizer = self._create_optimizer()
        
        # 学习率调度器
        self.scheduler = self._create_scheduler()
        
        # 训练状态
        self.global_step = 0
        self.epoch = 0
        self.best_eval_loss = float('inf')
        
    def _create_optimizer(self) -> torch.optim.Optimizer:
        """创建优化器，支持权重衰减分组"""
        # 不对 bias 和 LayerNorm 应用权重衰减
        no_decay = ["bias", "LayerNorm.weight", "layer_norm.weight"]
        
        optimizer_grouped_parameters = [
            {
                "params": [
                    p for n, p in self.model.named_parameters()
                    if not any(nd in n for nd in no_decay)
                ],
                "weight_decay": self.config.weight_decay,
            },
            {
                "params": [
                    p for n, p in self.model.named_parameters()
                    if any(nd in n for nd in no_decay)
                ],
                "weight_decay": 0.0,
            },
        ]
        
        optimizer = AdamW(
            optimizer_grouped_parameters,
            lr=self.config.learning_rate,
            betas=(self.config.adam_beta1, self.config.adam_beta2),
            eps=self.config.adam_epsilon,
        )
        
        return optimizer
    
    def _create_scheduler(self):
        """创建学习率调度器"""
        return CosineAnnealingWarmRestarts(
            self.optimizer,
            T_0=self.config.max_steps // 4,
            T_mult=2,
            eta_min=self.config.learning_rate * 0.01,
        )
    
    def train(self):
        """主训练循环"""
        print(f"开始训练: {self.config.run_name}")
        print(f"有效批次大小: {self.config.effective_batch_size}")
        print(f"训练阶段: {self.config.stage}")
        
        self.model.train()
        
        for epoch in range(self.config.num_epochs):
            self.epoch = epoch
            epoch_loss = self._train_epoch()
            
            print(f"Epoch {epoch + 1}/{self.config.num_epochs}, Loss: {epoch_loss:.4f}")
            
            # 评估
            if self.eval_dataloader is not None:
                eval_loss = self._evaluate()
                print(f"Eval Loss: {eval_loss:.4f}")
                
                # 保存最佳模型
                if eval_loss < self.best_eval_loss:
                    self.best_eval_loss = eval_loss
                    self._save_checkpoint("best")
            
            # 定期保存
            self._save_checkpoint(f"epoch_{epoch + 1}")
    
    def _train_epoch(self) -> float:
        """训练一个 epoch"""
        total_loss = 0.0
        num_batches = 0
        
        for batch_idx, batch in enumerate(self.train_dataloader):
            # 前向传播
            loss_dict = self._training_step(batch)
            loss = loss_dict["total"]
            
            # 梯度累积
            loss = loss / self.config.gradient_accumulation_steps
            loss.backward()
            
            if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:
                # 梯度裁剪
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.config.max_grad_norm,
                )
                
                # 优化器步进
                self.optimizer.step()
                self.scheduler.step()
                self.optimizer.zero_grad()
                
                self.global_step += 1
                
                # 日志
                if self.global_step % self.config.logging_steps == 0:
                    lr = self.scheduler.get_last_lr()[0]
                    print(
                        f"Step {self.global_step}, "
                        f"Loss: {loss.item() * self.config.gradient_accumulation_steps:.4f}, "
                        f"LR: {lr:.2e}"
                    )
                
                # 保存检查点
                if self.global_step % self.config.save_steps == 0:
                    self._save_checkpoint(f"step_{self.global_step}")
            
            total_loss += loss.item() * self.config.gradient_accumulation_steps
            num_batches += 1
        
        return total_loss / num_batches
    
    def _training_step(self, batch: dict) -> Dict[str, torch.Tensor]:
        """单步训练"""
        # 解析 batch
        encoder_semantic_ids = batch["encoder_semantic_ids"]
        encoder_positions = batch["encoder_positions"]
        encoder_token_types = batch["encoder_token_types"]
        decoder_semantic_ids = batch["decoder_semantic_ids"]
        decoder_positions = batch["decoder_positions"]
        decoder_token_types = batch["decoder_token_types"]
        targets = batch["targets"]
        
        # 前向传播
        logits_list = self.model(
            encoder_semantic_ids=encoder_semantic_ids,
            encoder_positions=encoder_positions,
            encoder_token_types=encoder_token_types,
            decoder_semantic_ids=decoder_semantic_ids,
            decoder_positions=decoder_positions,
            decoder_token_types=decoder_token_types,
        )
        
        # 获取 MoE 负载均衡损失
        load_balance_loss = self.model.get_moe_load_balance_loss()
        
        # 计算损失
        loss_dict = self.loss_fn(
            logits_list=logits_list,
            targets_list=targets,
            load_balance_loss=load_balance_loss,
        )
        
        return loss_dict
    
    def _evaluate(self) -> float:
        """评估"""
        self.model.eval()
        total_loss = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for batch in self.eval_dataloader:
                loss_dict = self._training_step(batch)
                total_loss += loss_dict["total"].item()
                num_batches += 1
        
        self.model.train()
        return total_loss / num_batches
    
    def _save_checkpoint(self, name: str):
        """保存检查点"""
        checkpoint_dir = os.path.join(self.config.output_dir, name)
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # 保存模型
        torch.save(
            self.model.state_dict(),
            os.path.join(checkpoint_dir, "model.pt"),
        )
        
        # 保存优化器状态
        torch.save(
            {
                "optimizer": self.optimizer.state_dict(),
                "scheduler": self.scheduler.state_dict(),
                "global_step": self.global_step,
                "epoch": self.epoch,
                "best_eval_loss": self.best_eval_loss,
            },
            os.path.join(checkpoint_dir, "training_state.pt"),
        )
        
        print(f"检查点已保存: {checkpoint_dir}")
    
    def load_checkpoint(self, checkpoint_dir: str):
        """加载检查点"""
        # 加载模型
        model_path = os.path.join(checkpoint_dir, "model.pt")
        self.model.load_state_dict(torch.load(model_path))
        
        # 加载训练状态
        state_path = os.path.join(checkpoint_dir, "training_state.pt")
        state = torch.load(state_path)
        
        self.optimizer.load_state_dict(state["optimizer"])
        self.scheduler.load_state_dict(state["scheduler"])
        self.global_step = state["global_step"]
        self.epoch = state["epoch"]
        self.best_eval_loss = state["best_eval_loss"]
        
        print(f"检查点已加载: {checkpoint_dir}")
```

### 3.4 分布式训练配置

```yaml
# deepspeed_config.yaml
# DeepSpeed ZeRO-2 配置 (推荐用于中大规模训练)

{
  "train_batch_size": "auto",
  "train_micro_batch_size_per_gpu": "auto",
  "gradient_accumulation_steps": "auto",
  
  "optimizer": {
    "type": "AdamW",
    "params": {
      "lr": "auto",
      "betas": [0.9, 0.999],
      "eps": 1e-8,
      "weight_decay": "auto"
    }
  },
  
  "scheduler": {
    "type": "WarmupCosineDecay",
    "params": {
      "warmup_min_lr": 0,
      "warmup_max_lr": "auto",
      "warmup_num_steps": "auto",
      "total_num_steps": "auto"
    }
  },
  
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
      "device": "none"
    },
    "allgather_partitions": true,
    "allgather_bucket_size": 5e8,
    "overlap_comm": true,
    "reduce_scatter": true,
    "reduce_bucket_size": 5e8,
    "contiguous_gradients": true
  },
  
  "fp16": {
    "enabled": true,
    "loss_scale": 0,
    "loss_scale_window": 1000,
    "initial_scale_power": 16,
    "hysteresis": 2,
    "min_loss_scale": 1
  },
  
  "gradient_clipping": 1.0,
  
  "steps_per_print": 100,
  
  "wall_clock_breakdown": false
}
```

---

## 4. 推理流程

### 4.1 M-FALCON 推理加速策略

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        M-FALCON 推理加速架构                                  │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │  Stage 1: 动态序列裁剪 (Dynamic Sequence Pruning)                    │   │
│  │                                                                     │   │
│  │  - 输入序列长度: 2048                                               │   │
│  │  - 根据注意力权重裁剪低重要性 Token                                   │   │
│  │  - 裁剪后长度: ~512 (75% 压缩)                                       │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                    ↓                                        │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │  Stage 2: 级联过滤 (Cascaded Filtering)                              │   │
│  │                                                                     │   │
│  │  Layer 1-4:   粗粒度过滤，保留 Top-1000 候选                          │   │
│  │  Layer 5-8:   中粒度过滤，保留 Top-200 候选                           │   │
│  │  Layer 9-12:  细粒度排序，输出 Top-50 推荐                            │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                    ↓                                        │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │  Stage 3: KV Cache 优化                                             │   │
│  │                                                                     │   │
│  │  - 增量 KV Cache 更新                                               │   │
│  │  - 多用户请求批处理                                                  │   │
│  │  - GPU 显存预分配                                                    │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  性能指标:                                                                   │
│  - 推理延迟: P50 < 15ms, P99 < 30ms                                        │
│  - 吞吐量: 单 GPU > 5000 QPS                                                │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 4.2 推理服务实现

```python
"""
推理服务实现
对应架构文档: 6.1-6.3 节
"""

import torch
import torch.nn.functional as F
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
import numpy as np
import time


@dataclass
class InferenceConfig:
    """推理配置"""
    # 模型配置
    model_path: str = "./checkpoints/best/model.pt"
    device: str = "cuda"
    
    # 推理参数
    max_seq_len: int = 2048
    max_output_len: int = 50
    batch_size: int = 64
    
    # M-FALCON 配置
    enable_m_falcon: bool = True
    prune_ratio: float = 0.75  # 序列裁剪比例
    cascade_topk: List[int] = None  # 级联过滤 Top-K
    
    # 采样配置
    temperature: float = 1.0
    top_k: int = 50
    top_p: float = 0.9
    
    # 缓存配置
    enable_kv_cache: bool = True
    cache_size_gb: float = 4.0
    
    def __post_init__(self):
        if self.cascade_topk is None:
            self.cascade_topk = [1000, 200, 50]


class KVCache:
    """
    KV Cache 管理器
    
    用于存储和复用注意力的 Key-Value 缓存
    """
    
    def __init__(
        self, 
        max_batch_size: int,
        max_seq_len: int,
        n_layers: int,
        n_heads: int,
        head_dim: int,
        device: str = "cuda",
    ):
        self.max_batch_size = max_batch_size
        self.max_seq_len = max_seq_len
        self.n_layers = n_layers
        
        # 预分配 GPU 显存
        cache_shape = (max_batch_size, n_layers, max_seq_len, n_heads, head_dim)
        
        self.k_cache = torch.zeros(cache_shape, device=device, dtype=torch.float16)
        self.v_cache = torch.zeros(cache_shape, device=device, dtype=torch.float16)
        
        # 当前缓存长度
        self.cache_lens = torch.zeros(max_batch_size, dtype=torch.long, device=device)
        
    def update(
        self,
        batch_indices: torch.Tensor,
        layer_idx: int,
        new_k: torch.Tensor,
        new_v: torch.Tensor,
    ):
        """更新缓存"""
        seq_len = new_k.size(1)
        
        for i, batch_idx in enumerate(batch_indices):
            start_pos = self.cache_lens[batch_idx].item()
            end_pos = start_pos + seq_len
            
            self.k_cache[batch_idx, layer_idx, start_pos:end_pos] = new_k[i]
            self.v_cache[batch_idx, layer_idx, start_pos:end_pos] = new_v[i]
        
        # 更新缓存长度
        self.cache_lens[batch_indices] += seq_len
        
    def get(
        self,
        batch_indices: torch.Tensor,
        layer_idx: int,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """获取缓存"""
        max_len = self.cache_lens[batch_indices].max().item()
        
        k = self.k_cache[batch_indices, layer_idx, :max_len]
        v = self.v_cache[batch_indices, layer_idx, :max_len]
        
        return k, v
    
    def clear(self, batch_indices: Optional[torch.Tensor] = None):
        """清除缓存"""
        if batch_indices is None:
            self.cache_lens.zero_()
        else:
            self.cache_lens[batch_indices] = 0


class SequencePruner:
    """
    动态序列裁剪器
    
    基于注意力权重裁剪低重要性 Token
    """
    
    def __init__(self, prune_ratio: float = 0.75):
        self.prune_ratio = prune_ratio
        
    def compute_importance(
        self, 
        attention_weights: torch.Tensor,
    ) -> torch.Tensor:
        """
        计算 Token 重要性
        
        Args:
            attention_weights: 注意力权重 (B, H, L, L)
            
        Returns:
            重要性分数 (B, L)
        """
        # 使用注意力权重的列和作为重要性
        # 被更多 Token 关注的 Token 更重要
        importance = attention_weights.sum(dim=(1, 2))  # (B, L)
        
        return importance
    
    def prune(
        self,
        hidden_states: torch.Tensor,
        importance_scores: torch.Tensor,
        keep_first_n: int = 10,  # 保留开头的 Token
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        裁剪序列
        
        Args:
            hidden_states: 隐藏状态 (B, L, D)
            importance_scores: 重要性分数 (B, L)
            keep_first_n: 保留开头的 Token 数
            
        Returns:
            裁剪后的隐藏状态和索引
        """
        B, L, D = hidden_states.shape
        
        # 计算保留数量
        keep_num = max(int(L * (1 - self.prune_ratio)), keep_first_n + 10)
        
        # 保留开头的 Token
        importance_scores[:, :keep_first_n] = float('inf')
        
        # 选择 Top-K
        _, indices = importance_scores.topk(keep_num, dim=1, sorted=True)
        indices = indices.sort(dim=1).values  # 按位置排序
        
        # 收集保留的 Token
        pruned_states = torch.gather(
            hidden_states,
            dim=1,
            index=indices.unsqueeze(-1).expand(-1, -1, D),
        )
        
        return pruned_states, indices


class CascadeFilter:
    """
    级联过滤器
    
    在不同层使用不同的候选集大小，逐步精细化
    """
    
    def __init__(
        self,
        topk_per_stage: List[int] = [1000, 200, 50],
        layers_per_stage: List[int] = [4, 4, 4],
    ):
        self.topk_per_stage = topk_per_stage
        self.layers_per_stage = layers_per_stage
        
        # 计算每层对应的 stage
        self.layer_to_stage = {}
        layer_idx = 0
        for stage, num_layers in enumerate(layers_per_stage):
            for _ in range(num_layers):
                self.layer_to_stage[layer_idx] = stage
                layer_idx += 1
                
    def filter(
        self,
        logits: torch.Tensor,
        layer_idx: int,
        current_candidates: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        """
        级联过滤
        
        Args:
            logits: 当前层输出 (B, V)
            layer_idx: 层索引
            current_candidates: 当前候选集 (B, K)
            
        Returns:
            过滤后的候选集
        """
        stage = self.layer_to_stage.get(layer_idx, len(self.topk_per_stage) - 1)
        topk = self.topk_per_stage[stage]
        
        if current_candidates is not None:
            # 在候选集内选择 Top-K
            candidate_logits = torch.gather(logits, dim=1, index=current_candidates)
            _, local_indices = candidate_logits.topk(min(topk, candidate_logits.size(1)), dim=1)
            new_candidates = torch.gather(current_candidates, dim=1, index=local_indices)
        else:
            # 全量选择 Top-K
            _, new_candidates = logits.topk(topk, dim=1)
        
        return new_candidates


class InferenceEngine:
    """
    推理引擎
    
    集成 M-FALCON 加速策略的完整推理实现
    
    对应架构文档: 第六章
    """
    
    def __init__(self, model: nn.Module, config: InferenceConfig):
        self.model = model
        self.config = config
        self.device = config.device
        
        # 将模型移动到设备
        self.model.to(self.device)
        self.model.eval()
        
        # 初始化组件
        if config.enable_kv_cache:
            self.kv_cache = KVCache(
                max_batch_size=config.batch_size,
                max_seq_len=config.max_seq_len,
                n_layers=model.config.n_dec_layers,
                n_heads=model.config.n_heads,
                head_dim=model.config.d_model // model.config.n_heads,
                device=self.device,
            )
        
        if config.enable_m_falcon:
            self.pruner = SequencePruner(config.prune_ratio)
            self.cascade_filter = CascadeFilter(config.cascade_topk)
        
        # 预热
        self._warmup()
        
    def _warmup(self):
        """预热 GPU"""
        print("预热推理引擎...")
        dummy_input = {
            "encoder_semantic_ids": [
                torch.zeros(1, 10, dtype=torch.long, device=self.device)
                for _ in range(3)
            ],
            "encoder_positions": torch.arange(10, device=self.device).unsqueeze(0),
            "encoder_token_types": torch.zeros(1, 10, dtype=torch.long, device=self.device),
        }
        
        for _ in range(3):
            with torch.no_grad():
                self.generate(**dummy_input, max_length=5)
        
        print("预热完成")
        
    @torch.inference_mode()
    def generate(
        self,
        encoder_semantic_ids: List[torch.Tensor],
        encoder_positions: torch.Tensor,
        encoder_token_types: torch.Tensor,
        max_length: int = 10,
        temperature: Optional[float] = None,
        top_k: Optional[int] = None,
        top_p: Optional[float] = None,
    ) -> List[List[int]]:
        """
        生成推荐序列
        
        Args:
            encoder_*: 编码器输入 (用户历史)
            max_length: 最大生成长度
            temperature: 采样温度
            top_k: Top-K 采样
            top_p: Top-P 采样
            
        Returns:
            生成的语义 ID 序列
        """
        temperature = temperature or self.config.temperature
        top_k = top_k or self.config.top_k
        top_p = top_p or self.config.top_p
        
        batch_size = encoder_positions.size(0)
        
        # 编码用户历史
        encoder_input = self.model.input_embedding(
            encoder_semantic_ids,
            encoder_positions,
            encoder_token_types,
        )
        encoder_output = self.model.encoder(encoder_input, encoder_token_types)
        
        # M-FALCON: 序列裁剪
        if self.config.enable_m_falcon:
            # 计算注意力权重 (简化版，实际应从模型中获取)
            importance = torch.ones(batch_size, encoder_output.size(1), device=self.device)
            encoder_output, _ = self.pruner.prune(encoder_output, importance)
        
        # 自回归生成
        generated_ids = [[] for _ in range(self.model.config.semantic_id_levels)]
        
        for step in range(max_length):
            # 构建解码器输入
            if step == 0:
                dec_ids = [
                    torch.zeros(batch_size, 1, dtype=torch.long, device=self.device)
                    for _ in range(self.model.config.semantic_id_levels)
                ]
            else:
                dec_ids = [
                    torch.tensor([ids], device=self.device)
                    for ids in generated_ids
                ]
            
            dec_positions = torch.arange(step + 1, device=self.device).unsqueeze(0).expand(batch_size, -1)
            dec_token_types = torch.ones(batch_size, step + 1, dtype=torch.long, device=self.device)
            
            # 解码
            decoder_input = self.model.input_embedding(dec_ids, dec_positions, dec_token_types)
            decoder_output = self.model.decoder(decoder_input, encoder_output, dec_token_types)
            
            # 采样下一个 Token
            for level, head in enumerate(self.model.output_heads):
                logits = head(decoder_output[:, -1, :]) / temperature
                
                # Top-K 过滤
                if top_k > 0:
                    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]
                    logits[indices_to_remove] = float('-inf')
                
                # Top-P 过滤
                if top_p < 1.0:
                    sorted_logits, sorted_indices = torch.sort(logits, descending=True)
                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                    
                    sorted_indices_to_remove = cumulative_probs > top_p
                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                    sorted_indices_to_remove[..., 0] = 0
                    
                    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
                    logits[indices_to_remove] = float('-inf')
                
                probs = F.softmax(logits, dim=-1)
                next_id = torch.multinomial(probs, num_samples=1).item()
                generated_ids[level].append(next_id)
        
        return generated_ids
    
    @torch.inference_mode()
    def batch_generate(
        self,
        requests: List[Dict],
        max_length: int = 10,
    ) -> List[List[List[int]]]:
        """
        批量生成
        
        Args:
            requests: 请求列表，每个请求包含用户历史
            max_length: 最大生成长度
            
        Returns:
            每个请求的生成结果
        """
        results = []
        
        # 分批处理
        for i in range(0, len(requests), self.config.batch_size):
            batch_requests = requests[i:i + self.config.batch_size]
            
            # 构建批次输入 (需要 padding)
            # ... 省略 padding 逻辑
            
            batch_results = self.generate(
                encoder_semantic_ids=batch_requests[0]["encoder_semantic_ids"],
                encoder_positions=batch_requests[0]["encoder_positions"],
                encoder_token_types=batch_requests[0]["encoder_token_types"],
                max_length=max_length,
            )
            
            results.extend(batch_results)
        
        return results


class RecommendationService:
    """
    推荐服务封装
    
    提供高层 API 供业务调用
    """
    
    def __init__(
        self,
        model: nn.Module,
        semantic_id_encoder: SemanticIDEncoder,
        config: InferenceConfig,
    ):
        self.engine = InferenceEngine(model, config)
        self.semantic_id_encoder = semantic_id_encoder
        self.config = config
        
    def recommend(
        self,
        user_history: List[Dict],
        num_recommendations: int = 10,
    ) -> List[Dict]:
        """
        生成推荐
        
        Args:
            user_history: 用户历史行为列表
            num_recommendations: 推荐数量
            
        Returns:
            推荐物品列表
        """
        start_time = time.time()
        
        # 1. 将用户历史转换为语义 ID 序列
        encoder_semantic_ids = self._encode_history(user_history)
        
        # 2. 生成推荐
        generated_ids = self.engine.generate(
            encoder_semantic_ids=encoder_semantic_ids["semantic_ids"],
            encoder_positions=encoder_semantic_ids["positions"],
            encoder_token_types=encoder_semantic_ids["token_types"],
            max_length=num_recommendations,
        )
        
        # 3. 解码语义 ID 为物品
        recommendations = self._decode_items(generated_ids)
        
        # 4. 记录延迟
        latency_ms = (time.time() - start_time) * 1000
        print(f"推荐延迟: {latency_ms:.2f}ms")
        
        return recommendations
    
    def _encode_history(self, user_history: List[Dict]) -> Dict:
        """编码用户历史"""
        semantic_ids_list = [[] for _ in range(3)]
        
        for item in user_history:
            # 获取物品的语义 ID
            item_semantic_ids = self.semantic_id_encoder.encode(item)
            
            for level, sid in enumerate(item_semantic_ids):
                semantic_ids_list[level].append(sid)
        
        seq_len = len(user_history)
        
        return {
            "semantic_ids": [
                torch.tensor([ids], dtype=torch.long, device=self.config.device)
                for ids in semantic_ids_list
            ],
            "positions": torch.arange(seq_len, device=self.config.device).unsqueeze(0),
            "token_types": torch.zeros(1, seq_len, dtype=torch.long, device=self.config.device),
        }
    
    def _decode_items(self, generated_ids: List[List[int]]) -> List[Dict]:
        """解码语义 ID 为物品"""
        recommendations = []
        
        num_items = len(generated_ids[0])
        
        for i in range(num_items):
            semantic_id = [generated_ids[level][i] for level in range(len(generated_ids))]
            
            # 从语义 ID 查找物品
            # 实际实现需要维护语义 ID 到物品的映射
            item = {
                "semantic_id": semantic_id,
                "rank": i + 1,
            }
            recommendations.append(item)
        
        return recommendations
```

### 4.3 延迟分解与优化

| 阶段 | 目标延迟 | 优化手段 |
|------|----------|----------|
| 特征获取 | < 2ms | Redis 缓存热门特征 |
| 用户编码 | < 5ms | 增量编码 + KV Cache |
| 序列生成 | < 15ms | M-FALCON 加速 |
| 后处理 | < 3ms | 批量去重 + 业务规则 |
| **总延迟** | **< 25ms** | - |

---

## 5. 特征工程方案

### 5.1 事件 Token 化

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          事件 Token 化流程                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  原始事件:                                                                   │
│  {                                                                          │
│    "user_id": "U123",                                                       │
│    "event_type": "click",                                                   │
│    "item_id": "I456",                                                       │
│    "timestamp": 1704067200,                                                 │
│    "context": {"device": "mobile", "location": "Beijing"}                   │
│  }                                                                          │
│                                    ↓                                        │
│  Token 序列:                                                                 │
│  [USER_U123] [ACTION_click] [ITEM_I456] [TIME_morning] [CTX_mobile]         │
│  [CTX_Beijing] [SEMANTIC_1_23] [SEMANTIC_2_456] [SEMANTIC_3_7890]           │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 5.2 Token 词表设计

```python
"""
Token 词表管理
对应架构文档: 4.1 节
"""

from dataclasses import dataclass, field
from typing import Dict, List, Optional
from enum import IntEnum
import json


class TokenType(IntEnum):
    """Token 类型枚举"""
    PAD = 0
    BOS = 1  # 序列开始
    EOS = 2  # 序列结束
    UNK = 3  # 未知
    
    # 用户相关
    USER = 10
    
    # 行为相关
    ACTION_CLICK = 20
    ACTION_VIEW = 21
    ACTION_PURCHASE = 22
    ACTION_CART = 23
    ACTION_FAVORITE = 24
    ACTION_SHARE = 25
    ACTION_COMMENT = 26
    
    # 物品相关
    ITEM = 30
    
    # 语义 ID
    SEMANTIC_L1 = 40  # 第一层语义 ID
    SEMANTIC_L2 = 41  # 第二层语义 ID
    SEMANTIC_L3 = 42  # 第三层语义 ID
    
    # 上下文相关
    CONTEXT_DEVICE = 50
    CONTEXT_LOCATION = 51
    CONTEXT_CHANNEL = 52
    
    # 时间相关
    TIME_HOUR = 60
    TIME_DAY = 61
    TIME_WEEK = 62
    TIME_MONTH = 63


@dataclass
class VocabularyConfig:
    """词表配置"""
    # 特殊 Token
    pad_token: str = "[PAD]"
    bos_token: str = "[BOS]"
    eos_token: str = "[EOS]"
    unk_token: str = "[UNK]"
    
    # 各类 Token 的保留空间
    num_special_tokens: int = 100
    num_user_tokens: int = 10_000_000    # 1000万用户
    num_action_tokens: int = 100
    num_item_tokens: int = 100_000_000   # 1亿物品
    num_semantic_tokens: int = 2_000_000  # 200万语义 ID
    num_context_tokens: int = 10_000
    num_time_tokens: int = 1000


class Vocabulary:
    """
    统一词表管理
    
    管理所有类型的 Token 及其映射关系
    """
    
    def __init__(self, config: VocabularyConfig):
        self.config = config
        
        # Token 到 ID 的映射
        self.token_to_id: Dict[str, int] = {}
        self.id_to_token: Dict[int, str] = {}
        
        # 各类型的 ID 范围
        self.type_ranges: Dict[str, tuple] = {}
        
        # 初始化
        self._init_vocabulary()
        
    def _init_vocabulary(self):
        """初始化词表"""
        current_id = 0
        
        # 1. 特殊 Token
        special_tokens = [
            self.config.pad_token,
            self.config.bos_token,
            self.config.eos_token,
            self.config.unk_token,
        ]
        for token in special_tokens:
            self.token_to_id[token] = current_id
            self.id_to_token[current_id] = token
            current_id += 1
        
        current_id = self.config.num_special_tokens
        
        # 2. 用户 Token
        self.type_ranges["user"] = (current_id, current_id + self.config.num_user_tokens)
        current_id += self.config.num_user_tokens
        
        # 3. 行为 Token
        self.type_ranges["action"] = (current_id, current_id + self.config.num_action_tokens)
        action_names = ["click", "view", "purchase", "cart", "favorite", "share", "comment"]
        for i, name in enumerate(action_names):
            token = f"[ACTION_{name}]"
            self.token_to_id[token] = self.type_ranges["action"][0] + i
            self.id_to_token[self.type_ranges["action"][0] + i] = token
        current_id += self.config.num_action_tokens
        
        # 4. 物品 Token
        self.type_ranges["item"] = (current_id, current_id + self.config.num_item_tokens)
        current_id += self.config.num_item_tokens
        
        # 5. 语义 ID Token
        self.type_ranges["semantic"] = (current_id, current_id + self.config.num_semantic_tokens)
        current_id += self.config.num_semantic_tokens
        
        # 6. 上下文 Token
        self.type_ranges["context"] = (current_id, current_id + self.config.num_context_tokens)
        current_id += self.config.num_context_tokens
        
        # 7. 时间 Token
        self.type_ranges["time"] = (current_id, current_id + self.config.num_time_tokens)
        current_id += self.config.num_time_tokens
        
        self.vocab_size = current_id
        
    def encode_user(self, user_id: str) -> int:
        """编码用户 ID"""
        # 使用哈希映射到用户 Token 范围
        hash_value = hash(user_id) % self.config.num_user_tokens
        return self.type_ranges["user"][0] + hash_value
    
    def encode_item(self, item_id: str) -> int:
        """编码物品 ID"""
        hash_value = hash(item_id) % self.config.num_item_tokens
        return self.type_ranges["item"][0] + hash_value
    
    def encode_semantic_id(self, semantic_id: List[int], level: int) -> int:
        """编码语义 ID"""
        # 为每层分配不同的偏移
        level_offset = level * (self.config.num_semantic_tokens // 3)
        return self.type_ranges["semantic"][0] + level_offset + semantic_id[level]
    
    def encode_action(self, action: str) -> int:
        """编码行为"""
        token = f"[ACTION_{action}]"
        return self.token_to_id.get(token, self.token_to_id[self.config.unk_token])
    
    def encode_time(self, timestamp: int) -> List[int]:
        """编码时间戳为多个时间 Token"""
        from datetime import datetime
        dt = datetime.fromtimestamp(timestamp)
        
        tokens = []
        
        # 小时 (0-23)
        hour_id = self.type_ranges["time"][0] + dt.hour
        tokens.append(hour_id)
        
        # 星期 (0-6)
        weekday_id = self.type_ranges["time"][0] + 24 + dt.weekday()
        tokens.append(weekday_id)
        
        # 月份 (1-12)
        month_id = self.type_ranges["time"][0] + 31 + dt.month
        tokens.append(month_id)
        
        return tokens
    
    def decode(self, token_id: int) -> str:
        """解码 Token ID"""
        if token_id in self.id_to_token:
            return self.id_to_token[token_id]
        
        # 根据范围判断类型
        for type_name, (start, end) in self.type_ranges.items():
            if start <= token_id < end:
                offset = token_id - start
                return f"[{type_name.upper()}_{offset}]"
        
        return self.config.unk_token
    
    def get_token_type(self, token_id: int) -> int:
        """获取 Token 类型 (用于 GLN)"""
        for type_name, (start, end) in self.type_ranges.items():
            if start <= token_id < end:
                type_mapping = {
                    "user": 0,
                    "action": 0,
                    "item": 1,
                    "semantic": 1,
                    "context": 2,
                    "time": 2,
                }
                return type_mapping.get(type_name, 3)
        return 3


class EventTokenizer:
    """
    事件 Token 化器
    
    将原始事件转换为 Token 序列
    """
    
    def __init__(self, vocabulary: Vocabulary, semantic_encoder: SemanticIDEncoder):
        self.vocab = vocabulary
        self.semantic_encoder = semantic_encoder
        
    def tokenize(self, event: Dict) -> Dict:
        """
        Token 化单个事件
        
        Args:
            event: 原始事件字典
            
        Returns:
            Token 化结果
        """
        token_ids = []
        token_types = []
        
        # 1. 用户 Token
        if "user_id" in event:
            user_token = self.vocab.encode_user(event["user_id"])
            token_ids.append(user_token)
            token_types.append(0)  # USER type
        
        # 2. 行为 Token
        if "event_type" in event:
            action_token = self.vocab.encode_action(event["event_type"])
            token_ids.append(action_token)
            token_types.append(0)  # ACTION type
        
        # 3. 物品 Token
        if "item_id" in event:
            item_token = self.vocab.encode_item(event["item_id"])
            token_ids.append(item_token)
            token_types.append(1)  # ITEM type
        
        # 4. 物品语义 ID
        if "item_features" in event:
            semantic_ids = self.semantic_encoder.encode(event["item_features"])
            for level, sid in enumerate(semantic_ids):
                sem_token = self.vocab.encode_semantic_id(semantic_ids, level)
                token_ids.append(sem_token)
                token_types.append(1)  # SEMANTIC type
        
        # 5. 时间 Token
        if "timestamp" in event:
            time_tokens = self.vocab.encode_time(event["timestamp"])
            token_ids.extend(time_tokens)
            token_types.extend([2] * len(time_tokens))  # TIME type
        
        # 6. 上下文 Token
        if "context" in event:
            for key, value in event["context"].items():
                ctx_token = self.vocab.type_ranges["context"][0] + hash(f"{key}:{value}") % 10000
                token_ids.append(ctx_token)
                token_types.append(2)  # CONTEXT type
        
        return {
            "token_ids": token_ids,
            "token_types": token_types,
        }
    
    def tokenize_sequence(self, events: List[Dict]) -> Dict:
        """
        Token 化事件序列
        
        Args:
            events: 事件列表
            
        Returns:
            Token 化的序列
        """
        all_token_ids = [self.vocab.token_to_id[self.vocab.config.bos_token]]
        all_token_types = [0]
        
        for event in events:
            result = self.tokenize(event)
            all_token_ids.extend(result["token_ids"])
            all_token_types.extend(result["token_types"])
        
        all_token_ids.append(self.vocab.token_to_id[self.vocab.config.eos_token])
        all_token_types.append(0)
        
        return {
            "token_ids": all_token_ids,
            "token_types": all_token_types,
            "positions": list(range(len(all_token_ids))),
        }
```

---

## 6. 大模型接入方案

### 6.1 LLM 辅助冷启动

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                       LLM 辅助冷启动架构                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  新物品入库流程:                                                              │
│                                                                             │
│  ┌─────────────┐     ┌─────────────┐     ┌─────────────┐                   │
│  │  物品信息   │────→│   LLM API   │────→│ 语义理解结果 │                    │
│  │ (标题/描述) │     │ (GPT-4/...)  │     │  (结构化)   │                    │
│  └─────────────┘     └─────────────┘     └──────┬──────┘                    │
│                                                  │                          │
│                                                  ▼                          │
│                      ┌───────────────────────────────────────────┐          │
│                      │           Semantic ID 生成               │          │
│                      │                                           │          │
│                      │  - 类目预测: [C_electronics, ...]         │          │
│                      │  - 属性提取: [品牌, 价格区间, 风格, ...]    │          │
│                      │  - 相似物品: 匹配已有物品的语义空间         │          │
│                      └───────────────────────────────────────────┘          │
│                                                  │                          │
│                                                  ▼                          │
│                      ┌───────────────────────────────────────────┐          │
│                      │           初始嵌入生成                    │          │
│                      │                                           │          │
│                      │  E_new = α * E_llm + (1-α) * E_similar    │          │
│                      │                                           │          │
│                      │  α: 衰减因子，随曝光逐渐降低                │          │
│                      └───────────────────────────────────────────┘          │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 6.2 LLM 接入实现

```python
"""
LLM 接入实现
对应架构文档: 5.1-5.2 节
"""

import asyncio
import aiohttp
from dataclasses import dataclass
from typing import Dict, List, Optional, Any
import json
import numpy as np
from abc import ABC, abstractmethod


@dataclass
class LLMConfig:
    """LLM 配置"""
    # API 配置
    provider: str = "openai"  # openai, anthropic, local
    api_key: str = ""
    api_base: str = "https://api.openai.com/v1"
    model_name: str = "gpt-4"
    
    # 请求配置
    max_tokens: int = 1024
    temperature: float = 0.7
    timeout: int = 30
    max_retries: int = 3
    
    # 缓存配置
    enable_cache: bool = True
    cache_ttl: int = 86400  # 24小时


class LLMClient(ABC):
    """LLM 客户端抽象基类"""
    
    @abstractmethod
    async def complete(self, prompt: str, **kwargs) -> str:
        """生成补全"""
        pass
    
    @abstractmethod
    async def embed(self, text: str) -> List[float]:
        """生成嵌入"""
        pass


class OpenAIClient(LLMClient):
    """OpenAI API 客户端"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.session = None
        
    async def _get_session(self) -> aiohttp.ClientSession:
        if self.session is None:
            self.session = aiohttp.ClientSession(
                headers={
                    "Authorization": f"Bearer {self.config.api_key}",
                    "Content-Type": "application/json",
                }
            )
        return self.session
    
    async def complete(self, prompt: str, **kwargs) -> str:
        """调用 Chat Completion API"""
        session = await self._get_session()
        
        payload = {
            "model": self.config.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": kwargs.get("max_tokens", self.config.max_tokens),
            "temperature": kwargs.get("temperature", self.config.temperature),
        }
        
        for attempt in range(self.config.max_retries):
            try:
                async with session.post(
                    f"{self.config.api_base}/chat/completions",
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=self.config.timeout),
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        return result["choices"][0]["message"]["content"]
                    elif response.status == 429:
                        # 限流，等待后重试
                        await asyncio.sleep(2 ** attempt)
                    else:
                        error = await response.text()
                        raise Exception(f"API 错误: {response.status} - {error}")
            except asyncio.TimeoutError:
                if attempt < self.config.max_retries - 1:
                    await asyncio.sleep(1)
                else:
                    raise
        
        raise Exception("达到最大重试次数")
    
    async def embed(self, text: str) -> List[float]:
        """调用 Embedding API"""
        session = await self._get_session()
        
        payload = {
            "model": "text-embedding-3-small",
            "input": text,
        }
        
        async with session.post(
            f"{self.config.api_base}/embeddings",
            json=payload,
            timeout=aiohttp.ClientTimeout(total=self.config.timeout),
        ) as response:
            if response.status == 200:
                result = await response.json()
                return result["data"][0]["embedding"]
            else:
                error = await response.text()
                raise Exception(f"Embedding API 错误: {response.status} - {error}")
    
    async def close(self):
        if self.session:
            await self.session.close()


class ColdStartAssistant:
    """
    冷启动辅助器
    
    使用 LLM 辅助新物品的语义理解和初始嵌入生成
    
    对应架构文档: 5.1-5.2 节
    """
    
    # 物品理解 Prompt 模板
    ITEM_UNDERSTANDING_PROMPT = """
请分析以下物品信息，并提取结构化的语义特征：

物品标题: {title}
物品描述: {description}
类目路径: {category}

请以 JSON 格式输出以下信息：
{{
    "primary_category": "一级类目",
    "secondary_category": "二级类目",
    "attributes": {{
        "品牌": "...",
        "价格区间": "低/中/高/奢侈",
        "风格": ["..."],
        "适用人群": ["..."],
        "关键特征": ["..."]
    }},
    "semantic_tags": ["语义标签1", "语义标签2", ...],
    "similar_to": ["可能相似的物品类型"]
}}

只输出 JSON，不要其他内容。
"""
    
    # 用户画像 Prompt 模板
    USER_PROFILING_PROMPT = """
根据以下用户行为，生成用户画像：

最近浏览: {recent_views}
最近购买: {recent_purchases}
收藏物品: {favorites}

请以 JSON 格式输出：
{{
    "interests": ["兴趣标签"],
    "price_preference": "低/中/高",
    "style_preference": ["风格偏好"],
    "activity_level": "低活跃/中等/高活跃",
    "potential_categories": ["可能感兴趣的类目"]
}}

只输出 JSON，不要其他内容。
"""
    
    def __init__(
        self,
        llm_client: LLMClient,
        semantic_encoder: SemanticIDEncoder,
        vocabulary: Vocabulary,
    ):
        self.llm = llm_client
        self.semantic_encoder = semantic_encoder
        self.vocab = vocabulary
        
        # 缓存
        self._cache: Dict[str, Any] = {}
        
    async def process_new_item(
        self,
        item_id: str,
        title: str,
        description: str,
        category: str = "",
        image_features: Optional[np.ndarray] = None,
    ) -> Dict:
        """
        处理新物品
        
        1. LLM 语义理解
        2. 生成 Semantic ID
        3. 生成初始嵌入
        
        Args:
            item_id: 物品 ID
            title: 物品标题
            description: 物品描述
            category: 类目路径
            image_features: 图像特征 (可选)
            
        Returns:
            处理结果
        """
        # 检查缓存
        cache_key = f"item:{item_id}"
        if cache_key in self._cache:
            return self._cache[cache_key]
        
        # 1. LLM 语义理解
        prompt = self.ITEM_UNDERSTANDING_PROMPT.format(
            title=title,
            description=description[:500],  # 限制长度
            category=category,
        )
        
        try:
            llm_response = await self.llm.complete(prompt, temperature=0.3)
            semantic_info = json.loads(llm_response)
        except Exception as e:
            print(f"LLM 分析失败: {e}")
            semantic_info = self._fallback_semantic_info(title, category)
        
        # 2. 生成文本嵌入
        text_for_embedding = f"{title}. {semantic_info.get('semantic_tags', [])}"
        try:
            llm_embedding = await self.llm.embed(text_for_embedding)
            llm_embedding = np.array(llm_embedding)
        except Exception as e:
            print(f"嵌入生成失败: {e}")
            llm_embedding = np.zeros(256)
        
        # 3. 生成 Semantic ID
        # 融合 LLM 嵌入和图像特征
        if image_features is not None:
            fused_features = np.concatenate([
                llm_embedding[:128],
                image_features[:128],
            ])
        else:
            fused_features = llm_embedding
        
        # 使用 Semantic ID 编码器 (简化版)
        # 实际实现需要将 numpy 转换为 torch tensor
        semantic_ids = [
            hash(semantic_info.get("primary_category", "")) % 1024,
            hash(str(semantic_info.get("attributes", {}))) % 4096,
            hash(item_id) % 16384,
        ]
        
        # 4. 构建结果
        result = {
            "item_id": item_id,
            "semantic_ids": semantic_ids,
            "semantic_info": semantic_info,
            "initial_embedding": fused_features.tolist(),
            "cold_start": True,
            "confidence": 0.7,  # 初始置信度
        }
        
        # 缓存结果
        self._cache[cache_key] = result
        
        return result
    
    async def process_new_user(
        self,
        user_id: str,
        initial_preferences: Optional[Dict] = None,
    ) -> Dict:
        """
        处理新用户
        
        Args:
            user_id: 用户 ID
            initial_preferences: 初始偏好 (注册时填写)
            
        Returns:
            用户初始化结果
        """
        if initial_preferences:
            # 基于显式偏好生成画像
            profile = {
                "user_id": user_id,
                "interests": initial_preferences.get("interests", []),
                "cold_start": True,
                "exploration_rate": 0.3,  # 新用户探索率较高
            }
        else:
            # 使用默认的热门类目
            profile = {
                "user_id": user_id,
                "interests": ["热门", "推荐"],
                "cold_start": True,
                "exploration_rate": 0.5,  # 完全新用户探索率更高
            }
        
        return profile
    
    async def update_cold_start_embedding(
        self,
        item_id: str,
        interactions: List[Dict],
        current_embedding: np.ndarray,
        initial_embedding: np.ndarray,
    ) -> np.ndarray:
        """
        更新冷启动物品的嵌入
        
        随着交互增加，逐渐从 LLM 嵌入过渡到协同嵌入
        
        Args:
            item_id: 物品 ID
            interactions: 交互记录
            current_embedding: 当前协同嵌入
            initial_embedding: 初始 LLM 嵌入
            
        Returns:
            更新后的嵌入
        """
        num_interactions = len(interactions)
        
        # 计算衰减因子
        # α 从 1.0 开始，随交互增加衰减
        alpha = max(0.1, 1.0 - num_interactions / 100)
        
        # 加权融合
        updated_embedding = alpha * initial_embedding + (1 - alpha) * current_embedding
        
        # 归一化
        updated_embedding = updated_embedding / (np.linalg.norm(updated_embedding) + 1e-8)
        
        return updated_embedding
    
    def _fallback_semantic_info(self, title: str, category: str) -> Dict:
        """LLM 失败时的回退策略"""
        return {
            "primary_category": category.split("/")[0] if category else "未知",
            "secondary_category": category.split("/")[1] if "/" in category else "未知",
            "attributes": {},
            "semantic_tags": title.split()[:5],
            "similar_to": [],
        }


class LLMPromptManager:
    """
    Prompt 模板管理器
    
    管理不同场景的 Prompt 模板
    """
    
    TEMPLATES = {
        "item_understanding": ColdStartAssistant.ITEM_UNDERSTANDING_PROMPT,
        "user_profiling": ColdStartAssistant.USER_PROFILING_PROMPT,
        
        "recommendation_explanation": """
根据用户的历史行为，解释为什么推荐以下物品：

用户近期行为: {user_history}
推荐物品: {recommended_item}

请用一句话解释推荐理由：
""",
        
        "query_understanding": """
分析用户的搜索意图：

搜索词: {query}
用户画像: {user_profile}

请输出：
{{
    "intent": "购买/浏览/比较/...",
    "category": "可能的类目",
    "attributes": ["关键属性"],
    "expanded_queries": ["扩展查询"]
}}
""",
    }
    
    @classmethod
    def get_prompt(cls, template_name: str, **kwargs) -> str:
        """获取填充后的 Prompt"""
        template = cls.TEMPLATES.get(template_name)
        if template is None:
            raise ValueError(f"未知的模板: {template_name}")
        return template.format(**kwargs)
```

### 6.3 上下文管理

```python
"""
LLM 上下文管理
对应架构文档: 5.3 节
"""

from dataclasses import dataclass, field
from typing import List, Dict, Optional
from collections import deque
import tiktoken


@dataclass
class ContextConfig:
    """上下文配置"""
    max_tokens: int = 4096          # 最大 Token 数
    system_tokens: int = 500        # 系统提示预留
    response_tokens: int = 1024     # 响应预留
    available_tokens: int = field(init=False)
    
    def __post_init__(self):
        self.available_tokens = self.max_tokens - self.system_tokens - self.response_tokens


class ContextWindow:
    """
    上下文窗口管理
    
    管理 LLM 调用的上下文，确保不超过 Token 限制
    """
    
    def __init__(self, config: ContextConfig, model: str = "gpt-4"):
        self.config = config
        
        # Token 计数器
        try:
            self.tokenizer = tiktoken.encoding_for_model(model)
        except:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
        
        # 上下文内容
        self.system_prompt: str = ""
        self.messages: deque = deque()
        self.current_tokens: int = 0
        
    def set_system_prompt(self, prompt: str):
        """设置系统提示"""
        self.system_prompt = prompt
        
    def add_message(self, role: str, content: str) -> bool:
        """
        添加消息
        
        Args:
            role: 角色 (user/assistant)
            content: 消息内容
            
        Returns:
            是否成功添加
        """
        tokens = self._count_tokens(content)
        
        # 检查是否超出限制
        while self.current_tokens + tokens > self.config.available_tokens:
            if len(self.messages) == 0:
                return False  # 单条消息就超出限制
            
            # 移除最旧的消息
            old_message = self.messages.popleft()
            self.current_tokens -= self._count_tokens(old_message["content"])
        
        # 添加消息
        self.messages.append({"role": role, "content": content})
        self.current_tokens += tokens
        
        return True
    
    def get_messages(self) -> List[Dict]:
        """获取所有消息"""
        messages = []
        
        if self.system_prompt:
            messages.append({"role": "system", "content": self.system_prompt})
        
        messages.extend(list(self.messages))
        
        return messages
    
    def clear(self):
        """清空上下文"""
        self.messages.clear()
        self.current_tokens = 0
    
    def _count_tokens(self, text: str) -> int:
        """计算 Token 数"""
        return len(self.tokenizer.encode(text))
    
    def get_stats(self) -> Dict:
        """获取统计信息"""
        return {
            "current_tokens": self.current_tokens,
            "available_tokens": self.config.available_tokens,
            "num_messages": len(self.messages),
            "utilization": self.current_tokens / self.config.available_tokens,
        }
```

---

## 7. 附录

### 7.1 模型参数量估算

| 模型规模 | 参数量 | 显存占用 (FP16) | 推荐场景 |
|---------|--------|----------------|---------|
| 小规模 | ~50M | ~100MB | 实验/POC |
| 中规模 | ~500M | ~1GB | 中小型业务 |
| 大规模 | ~1B | ~2GB | 大型业务 |
| 超大规模 | ~10B | ~20GB | 万亿级数据 |

### 7.2 训练资源需求

| 阶段 | GPU 配置 | 训练时间 | 数据量 |
|------|----------|----------|--------|
| Semantic ID 预训练 | 8 × A100 (80GB) | 2-3 天 | 1亿物品 |
| 生成模型预训练 | 32 × A100 (80GB) | 5-7 天 | 1000亿 Token |
| DPO 对齐 | 8 × A100 (80GB) | 1-2 天 | 100万偏好对 |

### 7.3 关键依赖版本

```
# requirements.txt
torch>=2.0.0
transformers>=4.30.0
deepspeed>=0.10.0
flash-attn>=2.0.0
tiktoken>=0.5.0
aiohttp>=3.8.0
numpy>=1.24.0
```

### 7.4 核心算法复杂度

| 操作 | 时间复杂度 | 空间复杂度 |
|------|-----------|-----------|
| 自注意力 | O(L²·D) | O(L²) |
| M-FALCON 剪枝后 | O(L·K·D) | O(L·K) |
| MoE 路由 | O(L·E) | O(E) |
| Semantic ID 编码 | O(D·C) | O(C) |

其中：L=序列长度，D=隐藏维度，K=剪枝后长度，E=专家数，C=码本大小

### 7.5 监控指标

```python
"""
关键监控指标定义
"""

MONITORING_METRICS = {
    # 模型指标
    "model": {
        "inference_latency_p50": "推理延迟 P50 (ms)",
        "inference_latency_p99": "推理延迟 P99 (ms)",
        "throughput_qps": "吞吐量 (QPS)",
        "moe_load_balance": "MoE 负载均衡度",
        "kv_cache_hit_rate": "KV Cache 命中率",
    },
    
    # 业务指标
    "business": {
        "ctr": "点击率",
        "cvr": "转化率",
        "cold_start_ctr": "冷启动物品点击率",
        "diversity": "推荐多样性",
        "coverage": "物品覆盖率",
    },
    
    # 系统指标
    "system": {
        "gpu_utilization": "GPU 利用率",
        "gpu_memory_used": "GPU 显存使用",
        "api_success_rate": "API 成功率",
        "llm_latency": "LLM 调用延迟",
    },
}
```

---

> **文档版本历史**
> - v1.0 (2026-01-03): 初始版本，包含完整的算法架构和大模型接入方案