{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def read_gz_file(file_path, low_memory=True):\n",
    "    dataset = pd.read_csv(file_path, sep='\\t', low_memory=low_memory)\n",
    "    return dataset\n",
    "def save_dataframe_to_file(df, filename):\n",
    "    df.to_csv(filename, index=False)\n",
    "def save_ndarry_to_file(data, filename):\n",
    "    np.savetxt(filename, data, delimiter=',')\n",
    "def load_file_to_dataframe(filename):\n",
    "    return pd.read_csv(filename)\n",
    "def load_file_to_ndarry(filename):\n",
    "    return pd.read_csv(filename).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data, process each file, clean up null values, cut out unnecessary features, and clean up data and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame object to read the compressed file\n",
    "title_basics_tsv_df = read_gz_file('title.basics.tsv',low_memory=False)\n",
    "# Remove noise\n",
    "title_basics_tsv_df = title_basics_tsv_df.drop(columns=['startYear', 'endYear', 'runtimeMinutes'])\n",
    "# Remove rows where the title column has missing values\n",
    "title_basics_tsv_df = title_basics_tsv_df.dropna(subset=['titleType','primaryTitle','originalTitle','isAdult','genres'])\n",
    "# Filter for movies' tconst\n",
    "title_basics_tsv_df = title_basics_tsv_df[title_basics_tsv_df['titleType'] == 'movie']\n",
    "title_basics_tsv_df = title_basics_tsv_df.drop(columns=['titleType'])\n",
    "# Display the first few rows of data\n",
    "print(title_basics_tsv_df.head(10))\n",
    "# Build a tconst set\n",
    "movie_tconsts = set(title_basics_tsv_df['tconst'])\n",
    "# Build a filter\n",
    "def filter_by_tconst(df, tconst_column):\n",
    "    return df[df[tconst_column].isin(movie_tconsts)]\n",
    "# Build a filter\n",
    "def clean_known_for_titles(df, movie_tconsts):\n",
    "    def filter_known_for_titles(row):\n",
    "        # Split the string into an array\n",
    "        titles = row['knownForTitles'].split(',')\n",
    "        # Filter out tconsts that are out of range\n",
    "        filtered_titles = [title for title in titles if title in movie_tconsts]\n",
    "        # Overwrite the new data back into knownForTitles\n",
    "        row['knownForTitles'] = ','.join(filtered_titles)\n",
    "        # Return this row\n",
    "        return row\n",
    "\n",
    "    # Filter and clean each row of data\n",
    "    df = df.apply(filter_known_for_titles, axis=1)\n",
    "    # Remove rows where knownForTitles is empty, which indicates that the row is completely out of range\n",
    "    df = df[df['knownForTitles'] != '']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame object to read the compressed file\n",
    "name_basics_tsv_df = read_gz_file('name.basics.tsv')\n",
    "# Remove noise\n",
    "name_basics_tsv_df = name_basics_tsv_df.drop(columns=['primaryName', 'birthYear', 'deathYear'])\n",
    "name_basics_tsv_df = name_basics_tsv_df.dropna(subset=['nconst','primaryProfession','knownForTitles'])\n",
    "# Filter and re-clean\n",
    "name_basics_tsv_df = clean_known_for_titles(name_basics_tsv_df, movie_tconsts)\n",
    "# Build an nconst set\n",
    "name_nconsts = set(name_basics_tsv_df['nconst'])\n",
    "# Display the first few rows of data\n",
    "print(name_basics_tsv_df.head(10))\n",
    "# Build a filter\n",
    "def filter_by_nconst(df, nconst_column):\n",
    "    return df[df[nconst_column].isin(name_nconsts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame object to read the compressed file\n",
    "title_akas_tsv_df = read_gz_file('title.akas.tsv')\n",
    "# Remove noise\n",
    "title_akas_tsv_df = title_akas_tsv_df.drop(columns=['ordering', 'language', 'attributes'])\n",
    "# Remove rows where the title column has missing values\n",
    "title_akas_tsv_df = title_akas_tsv_df.dropna(subset=['title','titleId'])\n",
    "# Re-clean the data\n",
    "title_akas_tsv_df = filter_by_tconst(title_akas_tsv_df, 'titleId')\n",
    "title_akas_tsv_df = title_akas_tsv_df.rename(columns={'titleId': 'tconst'})\n",
    "# Display the first few rows of data\n",
    "print(title_akas_tsv_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_nconst_directors(row):\n",
    "    if row['directors'] == '\\\\N':\n",
    "        return row['directors']\n",
    "    names = row['directors'].split(',')\n",
    "    filtered_names = [name for name in names if name in name_nconsts]\n",
    "    return ','.join(filtered_names) if filtered_names else '\\\\N'\n",
    "def filter_nconst_writers(row):\n",
    "    if row['writers'] == '\\\\N':\n",
    "        return row['writers']\n",
    "    names = row['writers'].split(',')\n",
    "    filtered_names = [name for name in names if name in name_nconsts]\n",
    "    return ','.join(filtered_names) if filtered_names else '\\\\N'\n",
    "# Create a data frame object to read the compressed file\n",
    "title_crew_tsv_df= read_gz_file('title.crew.tsv')\n",
    "# Re-clean the data\n",
    "title_crew_tsv_df = filter_by_tconst(title_crew_tsv_df, 'tconst')\n",
    "title_crew_tsv_df['directors'] = title_crew_tsv_df.apply(filter_nconst_directors, axis=1)\n",
    "title_crew_tsv_df['writers'] = title_crew_tsv_df.apply(filter_nconst_writers, axis=1)\n",
    "\n",
    "title_crew_tsv_df = title_crew_tsv_df.rename(columns={'directors': 'directors_nconst'})\n",
    "title_crew_tsv_df = title_crew_tsv_df.rename(columns={'writers': 'writers_nconst'})\n",
    "# Display the first few rows of data\n",
    "print(title_crew_tsv_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame object to read the compressed file\n",
    "title_ratings_tsv_df= read_gz_file('title.ratings.tsv')\n",
    "# Remove rows where the directors column has missing values\n",
    "title_ratings_tsv_df = title_ratings_tsv_df.dropna(subset=['averageRating','numVotes'])\n",
    "# Re-clean the data\n",
    "title_ratings_tsv_df = filter_by_tconst(title_ratings_tsv_df, 'tconst')\n",
    "# Display the first few rows of data\n",
    "print(title_ratings_tsv_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save these DataFrame data to files for future reading\n",
    "save_dataframe_to_file(title_basics_tsv_df,'title_basics_tsv_df.csv')\n",
    "save_dataframe_to_file(name_basics_tsv_df,'name_basics_tsv_df.csv')\n",
    "save_dataframe_to_file(title_akas_tsv_df,'title_akas_tsv_df.csv')\n",
    "save_dataframe_to_file(title_crew_tsv_df,'title_crew_tsv_df.csv')\n",
    "save_dataframe_to_file(title_ratings_tsv_df,'title_ratings_tsv_df.csv')\n",
    "del title_basics_tsv_df\n",
    "del name_basics_tsv_df\n",
    "del title_akas_tsv_df\n",
    "del title_crew_tsv_df\n",
    "del title_ratings_tsv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link tables to create data tables with tighter information density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code here finally merges three new tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "title_basics_tsv_df = load_file_to_dataframe('title_basics_tsv_df.csv')\n",
    "title_basics_tsv_df = title_basics_tsv_df.drop(columns=['primaryTitle', 'originalTitle'])\n",
    "name_basics_tsv_df = load_file_to_dataframe('name_basics_tsv_df.csv')\n",
    "title_akas_tsv_df = load_file_to_dataframe('title_akas_tsv_df.csv')\n",
    "title_crew_tsv_df = load_file_to_dataframe('title_crew_tsv_df.csv')\n",
    "title_ratings_tsv_df = load_file_to_dataframe('title_ratings_tsv_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive movie information feature table\n",
    "# Filter original titles\n",
    "original_titles_df = title_akas_tsv_df[title_akas_tsv_df['isOriginalTitle'] == 1]\n",
    "\n",
    "# Merge the title_basics_tsv_df table and the Original Titles table\n",
    "movies_info_df = pd.merge(title_basics_tsv_df, original_titles_df[['tconst', 'title']], on='tconst', how='left')\n",
    "\n",
    "# Merge the title_ratings_tsv_df table\n",
    "movies_info_df = pd.merge(movies_info_df, title_ratings_tsv_df[['tconst', 'averageRating', 'numVotes']], on='tconst', how='left')\n",
    "\n",
    "# Replace missing values in averageRating and numVotes with the median\n",
    "average_rating_median = movies_info_df['averageRating'].median()\n",
    "num_votes_median = movies_info_df['numVotes'].median()\n",
    "\n",
    "movies_info_df['averageRating'].fillna(average_rating_median, inplace=True)\n",
    "movies_info_df['numVotes'].fillna(num_votes_median, inplace=True)\n",
    "\n",
    "# Round averageRating to an integer\n",
    "movies_info_df['averageRating'] = movies_info_df['averageRating'].round().astype(int)\n",
    "\n",
    "# Convert numVotes to the nearest multiple of 10\n",
    "movies_info_df['numVotes'] = movies_info_df['numVotes'].apply(lambda x: int(np.round(x / 10) * 10))\n",
    "\n",
    "# Select the required columns\n",
    "movies_info_df[['genres1', 'genres2', 'genres3']] = movies_info_df['genres'].str.split(',', expand=True).fillna('\\\\N')\n",
    "movies_info_df = movies_info_df[['tconst','title','genres1','genres2','genres3','isAdult','averageRating','numVotes']]\n",
    "\n",
    "# Convert isAdult, averageRating, numVotes columns to string type\n",
    "movies_info_df['averageRating'] = movies_info_df['averageRating'].astype(str)\n",
    "movies_info_df['numVotes'] = movies_info_df['numVotes'].astype(str)\n",
    "\n",
    "print(movies_info_df.head())\n",
    "\n",
    "save_dataframe_to_file(movies_info_df,'movies_info_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have loaded the data into the following DataFrame\n",
    "name_basics_tsv_df = pd.read_csv('name_basics_tsv_df.csv')\n",
    "title_crew_tsv_df = pd.read_csv('title_crew_tsv_df.csv')\n",
    "\n",
    "# Split the primaryProfession column and the knownForTitles column by commas and expand them into multiple columns\n",
    "name_basics_tsv_df[['primaryProfession_top1', 'primaryProfession_top2', 'primaryProfession_top3']] = name_basics_tsv_df['primaryProfession'].str.split(',', expand=True).fillna('\\\\N')\n",
    "name_basics_tsv_df[['knownForTitle1', 'knownForTitle2', 'knownForTitle3', 'knownForTitle4']] = name_basics_tsv_df['knownForTitles'].str.split(',', expand=True).fillna('\\\\N')\n",
    "\n",
    "# Initialize isDirectors and isWriters columns as 0\n",
    "name_basics_tsv_df['isDirectors'] = 0\n",
    "name_basics_tsv_df['isWriters'] = 0\n",
    "\n",
    "# Set the isDirectors column\n",
    "directors = title_crew_tsv_df['directors_nconst'].str.split(',').explode().unique()\n",
    "name_basics_tsv_df.loc[name_basics_tsv_df['nconst'].isin(directors), 'isDirectors'] = 1\n",
    "\n",
    "# Set the isWriters column\n",
    "writers = title_crew_tsv_df['writers_nconst'].str.split(',').explode().unique()\n",
    "name_basics_tsv_df.loc[name_basics_tsv_df['nconst'].isin(writers), 'isWriters'] = 1\n",
    "\n",
    "# Keep the required columns\n",
    "staff_df = name_basics_tsv_df[['nconst', 'primaryProfession_top1', 'primaryProfession_top2', 'primaryProfession_top3', 'knownForTitle1', 'knownForTitle2', 'knownForTitle3', 'knownForTitle4', 'isDirectors', 'isWriters']]\n",
    "\n",
    "print(staff_df.head())\n",
    "\n",
    "save_dataframe_to_file(staff_df,'staff_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the required columns\n",
    "regional_titles_df = title_akas_tsv_df[['tconst', 'title', 'region', 'types']]\n",
    "print(regional_titles_df.head())\n",
    "save_dataframe_to_file(regional_titles_df,'regional_titles_df.csv')\n",
    "del title_basics_tsv_df\n",
    "del name_basics_tsv_df\n",
    "del title_akas_tsv_df\n",
    "del title_crew_tsv_df\n",
    "del title_ratings_tsv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dictionary and perform one hot encoding (vectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_info_df = load_file_to_dataframe('movies_info_df.csv')\n",
    "regional_titles_df = load_file_to_dataframe('regional_titles_df.csv')\n",
    "staff_df = load_file_to_dataframe('staff_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "vocab = {}\n",
    "global_counter = 0\n",
    "vocab['0'] = 0\n",
    "vocab['1'] = 1\n",
    "global_counter = 1\n",
    "\n",
    "int_list_record = set([0,1])\n",
    "movies_info_df_averageRating_list = set(movies_info_df['averageRating'].tolist())\n",
    "movies_info_df_numVotes_list = set(movies_info_df['numVotes'].tolist())\n",
    "unique_int_set = int_list_record.union(movies_info_df_averageRating_list.union(movies_info_df_numVotes_list))\n",
    "\n",
    "def get_valid_counter_num():\n",
    "    global global_counter\n",
    "    while global_counter in unique_int_set:\n",
    "        global_counter += 1\n",
    "    return global_counter\n",
    "\n",
    "def register_vocab(data_list):\n",
    "    global global_counter\n",
    "    for item in data_list:\n",
    "        if item not in vocab:\n",
    "            vocab[item] = get_valid_counter_num()\n",
    "            global_counter += 1\n",
    "\n",
    "def record(df, col_name):\n",
    "    col_list = df[col_name].tolist()\n",
    "    random.shuffle(col_list)\n",
    "    register_vocab(col_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This recording process has a sequential requirement, so nconst and tconst are sequential in the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Therefore, in this step, I did not record nconst and tconst first, but first recorded some information that can represent classification information, such as region and type, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, we record tconst and nconst, and some very isolated data such as title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In all the above recording processes, random.shuffle is used to ensure that the data does not have any sequential representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record(movies_info_df, 'genres1')\n",
    "record(movies_info_df, 'genres2')\n",
    "record(movies_info_df, 'genres3')\n",
    "record(regional_titles_df, 'region')\n",
    "record(regional_titles_df, 'types')\n",
    "record(staff_df, 'primaryProfession_top1')\n",
    "record(staff_df, 'primaryProfession_top2')\n",
    "record(staff_df, 'primaryProfession_top3')\n",
    "record(movies_info_df, 'tconst')\n",
    "record(staff_df, 'nconst')\n",
    "record(movies_info_df, 'title')\n",
    "record(regional_titles_df, 'title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# Save\n",
    "with open('vocab.csv', 'w', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for key, value in vocab.items():\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linking the movies and regional tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import tempfile\n",
    "\n",
    "# Assuming staff_df and regional_titles_df have been loaded and are available\n",
    "# staff_df = pd.read_csv('path_to_staff_df.csv')\n",
    "# regional_titles_df = pd.read_csv('path_to_regional_titles_df.csv')\n",
    "\n",
    "def process_batch(temp_file_name, movies_info_batch, regional_titles_df):\n",
    "    regional_titles_batch = regional_titles_df.iloc[temp_file_name]\n",
    "    \n",
    "    for index, row in regional_titles_batch.iterrows():\n",
    "        tconst = row['tconst']\n",
    "        region = row['region']\n",
    "        movie_type = row['types']\n",
    "\n",
    "        if tconst in movies_info_batch['tconst'].values:\n",
    "            movies_info_batch.loc[movies_info_batch['tconst'] == tconst, f'region_class_{region}'] = 1\n",
    "            movies_info_batch.loc[movies_info_batch['tconst'] == tconst, f'movie_type_{movie_type}'] = 1\n",
    "\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.pkl')\n",
    "    movies_info_batch.to_pickle(temp_file.name)\n",
    "    return temp_file.name\n",
    "\n",
    "def combine_temp_files(temp_files):\n",
    "    combined_df = pd.concat([pd.read_pickle(temp_file) for temp_file in temp_files], ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "def batch_transform(data, batch_size=10000):\n",
    "    temp_files = []\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for i in range(0, data.shape[0], batch_size):\n",
    "            batch_indices = range(i, min(i + batch_size, data.shape[0]))\n",
    "            futures.append(executor.submit(process_batch, batch_indices, data.iloc[batch_indices], regional_titles_df))\n",
    "\n",
    "        for future in futures:\n",
    "            temp_files.append(future.result())\n",
    "\n",
    "    return temp_files\n",
    "\n",
    "# Initialize new columns\n",
    "regional_titles_df['region'] = regional_titles_df['region'].astype(str)\n",
    "regional_titles_df['types'] = regional_titles_df['types'].astype(str)\n",
    "unique_regions = sorted(regional_titles_df['region'].unique())\n",
    "unique_types = sorted(regional_titles_df['types'].unique())\n",
    "\n",
    "# Create a DataFrame with all the new columns\n",
    "new_columns = {f'region_class_{region}': 0 for region in unique_regions}\n",
    "new_columns.update({f'movie_type_{movie_type}': 0 for movie_type in unique_types})\n",
    "new_columns_df = pd.DataFrame(new_columns, index=movies_info_df.index)\n",
    "\n",
    "# Merge new columns into movies_info_df\n",
    "movies_info_df = pd.concat([movies_info_df, new_columns_df], axis=1)\n",
    "\n",
    "# Batch process and save to disk\n",
    "temp_files = batch_transform(movies_info_df)\n",
    "\n",
    "# Load from disk and merge results\n",
    "final_df = combine_temp_files(temp_files)\n",
    " \n",
    "print(final_df.head())\n",
    "save_dataframe_to_file(final_df, 'movies_info_regional_combine_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link movies_info_regional_combine and staff tables (get final table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staff = load_file_to_dataframe('staff_df.csv')\n",
    "\n",
    "# Expand the four knownForTitle columns\n",
    "staff_melted = staff.melt(id_vars=['nconst'], \n",
    "                          value_vars=['knownForTitle1', 'knownForTitle2', 'knownForTitle3', 'knownForTitle4'],\n",
    "                          var_name='knownForTitleNumber', \n",
    "                          value_name='knownForTitle')\n",
    "\n",
    "# Drop rows with missing values\n",
    "staff_melted = staff_melted.dropna(subset=['knownForTitle'])\n",
    "\n",
    "# Select the required columns\n",
    "staff_expanded = staff_melted[['nconst', 'knownForTitle']]\n",
    "\n",
    "# Read the data\n",
    "movies_info_regional_combine_df = load_file_to_dataframe('movies_info_regional_combine_df.csv')\n",
    "merged_df = staff_expanded.merge(movies_info_regional_combine_df, left_on='knownForTitle', right_on='tconst', how='inner')\n",
    "\n",
    "# Save the final table\n",
    "merged_df = merged_df.drop(['knownForTitle'], axis=1)\n",
    "print(merged_df.head())\n",
    "save_dataframe_to_file(merged_df, 'final_merged_staff_movies_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_df = pd.read_csv('vocab.csv', header=None)  # Ensure no header row\n",
    "vocab_df.columns = ['key', 'value']  # Add column names for easier handling\n",
    "# Convert DataFrame to dictionary\n",
    "vocab = pd.Series(vocab_df['value'].values, index=vocab_df['key']).to_dict()\n",
    "\n",
    "merged_df = load_file_to_dataframe('final_merged_staff_movies_info.csv')\n",
    "\n",
    "# Define a function to map the columns\n",
    "def vectorize_column(column, vocab):\n",
    "    return column.apply(lambda x: vocab.get(x, x))\n",
    "\n",
    "# Iterate over all columns of merged_df\n",
    "for col in merged_df.columns:\n",
    "    if merged_df[col].dtype != 'int64':\n",
    "        merged_df[col] = vectorize_column(merged_df[col], vocab)\n",
    "\n",
    "# Print the result\n",
    "print(merged_df.head())\n",
    "# Save the vectorized table\n",
    "save_dataframe_to_file(merged_df, 'final_mapped_vec.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because the internal features of the original data are unknown after vectorization, such as their clustering, their feature correlation, etc., through the unsupervised learning of the neural network, the features of all vectors are fused and the final fused features are obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Read the CSV file\n",
    "final_mapped_vec_df = pd.read_csv('final_mapped_vec.csv')\n",
    "\n",
    "# Ensure final_mapped_vec is a numpy array\n",
    "final_mapped_vec = final_mapped_vec_df.values\n",
    "\n",
    "# Check and adjust data shape\n",
    "if final_mapped_vec.ndim == 1:\n",
    "    final_mapped_vec = final_mapped_vec.reshape(-1, 1)\n",
    "\n",
    "print(f\"Array dimensions: {final_mapped_vec.ndim}\")\n",
    "print(f\"Array shape: {final_mapped_vec.shape}\")\n",
    "\n",
    "# Print the first few rows of data to confirm\n",
    "print(final_mapped_vec[:5])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "final_mapped_vec = scaler.fit_transform(final_mapped_vec)\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Build the model\n",
    "input_layer = Input(shape=(final_mapped_vec.shape[1],))\n",
    "dense_layer_1 = Dense(512, activation='relu')(input_layer)  # High-dimensional features\n",
    "dropout_layer1 = Dropout(0.25)(dense_layer_1)\n",
    "dense_layer_2 = Dense(256, activation='relu')(dropout_layer1)  # High-dimensional features\n",
    "dropout_layer2 = Dropout(0.25)(dense_layer_2)\n",
    "dense_layer_3 = Dense(128, activation='relu')(dropout_layer2)  # High-dimensional features\n",
    "dropout_layer3 = Dropout(0.25)(dense_layer_3)\n",
    "output_layer = Dense(final_mapped_vec.shape[1], activation='relu')(dropout_layer3)  # Fused features\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(final_mapped_vec,\n",
    "                    final_mapped_vec,\n",
    "                    epochs=100,\n",
    "                    batch_size=2048,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping, checkpoint])\n",
    "\n",
    "# Load the best model weights\n",
    "model.load_weights('best_model.keras')\n",
    "\n",
    "# Get the fused features\n",
    "fused_features = model.predict(final_mapped_vec)\n",
    "\n",
    "# Save the fused features to a Parquet file\n",
    "fused_features_df = pd.DataFrame(fused_features)\n",
    "fused_features_df.to_parquet('fused_features.parquet', index=False)\n",
    "print(\"Fused features saved to fused_features.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The fused features are used as input into Wrod2Vec to obtain the final word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict, Counter\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import re\n",
    "import string\n",
    "import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "    targets, contexts, labels = [], [], []\n",
    "\n",
    "    # Build the sampling table for `vocab_size` tokens.\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "    for sequence in tqdm.tqdm(sequences):\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "            sequence,\n",
    "            vocabulary_size=vocab_size,\n",
    "            sampling_table=sampling_table,\n",
    "            window_size=window_size,\n",
    "            negative_samples=0)\n",
    "\n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "            context_class = tf.expand_dims(\n",
    "                tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "                true_classes=context_class,\n",
    "                num_true=1,\n",
    "                num_sampled=num_ns,\n",
    "                unique=True,\n",
    "                range_max=vocab_size,\n",
    "                seed=seed,\n",
    "                name=\"negative_sampling\")\n",
    "\n",
    "            context = tf.concat([tf.squeeze(context_class, 1), negative_sampling_candidates], 0)\n",
    "            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "\n",
    "    return targets, contexts, labels\n",
    "\n",
    "class Word2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = layers.Embedding(vocab_size, embedding_dim, name=\"w2v_embedding\")\n",
    "        self.context_embedding = layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        if len(target.shape) == 2:\n",
    "            target = tf.squeeze(target, axis=1)\n",
    "        word_emb = self.target_embedding(target)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "        return dots\n",
    "\n",
    "def export(word2vec, vocab):\n",
    "    weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "\n",
    "    out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "    out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "    for index, word in enumerate(vocab):\n",
    "        if index == 0:\n",
    "            continue  # skip 0, it's padding.\n",
    "        vec = weights[index]\n",
    "        out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "        out_m.write(word + \"\\n\")\n",
    "    out_v.close()\n",
    "    out_m.close()\n",
    "\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        files.download('vectors.tsv')\n",
    "        files.download('metadata.tsv')\n",
    "    except Exception:\n",
    "        pass\n",
    "    return weights\n",
    "\n",
    "def movie_recommendation_word2vec(fused_features, max_features=20000, window_size=2, num_ns=20, BATCH_SIZE=1024, BUFFER_SIZE=10000, embedding_dim=150, epochs=25, SEED=42):\n",
    "    sequences = fused_features\n",
    "    vocab_size = max_features\n",
    "\n",
    "    # Generate training data\n",
    "    targets, contexts, labels = generate_training_data(\n",
    "        sequences=sequences,\n",
    "        window_size=window_size,\n",
    "        num_ns=num_ns,\n",
    "        vocab_size=vocab_size,\n",
    "        seed=SEED)\n",
    "    \n",
    "    targets = np.array(targets)\n",
    "    contexts = np.array(contexts)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    print(f\"targets.shape: {targets.shape}\")\n",
    "    print(f\"contexts.shape: {contexts.shape}\")\n",
    "    print(f\"labels.shape: {labels.shape}\")\n",
    "\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "    dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "    word2vec.compile(optimizer='adam',\n",
    "                     loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                     metrics=['accuracy'])\n",
    "    \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "    word2vec.fit(dataset, epochs=epochs, callbacks=[tensorboard_callback])\n",
    "\n",
    "    # Export word vectors\n",
    "    vocab = [str(i) for i in range(vocab_size)]  # Construct a simple vocabulary\n",
    "    weights = export(word2vec, vocab)\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Assuming fused_features are the fused features\n",
    "fused_features = pd.read_parquet('fused_features.parquet').values\n",
    "print(\"Read fused features done\")\n",
    "fused_features = tf.data.Dataset.from_tensor_slices(fused_features.astype(int))\n",
    "fused_features = list(fused_features.as_numpy_iterator())\n",
    "\n",
    "# Train Word2Vec model\n",
    "weights = movie_recommendation_word2vec(fused_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
