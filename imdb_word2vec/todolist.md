# ğŸ“‹ IMDb Word2Vec é¡¹ç›®ä¼˜ç¼ºç‚¹å®Œæ•´æ¸…å•

---

## âœ… ä¼˜ç‚¹æ¸…å•ï¼ˆåº”ä¿ç•™ï¼‰

---

### 1. æµå¼è®­ç»ƒæ¶æ„

**æè¿°**: å°†å¤§è§„æ¨¡æ•°æ®åˆ†å—å¤„ç†ï¼Œæ¯æ¬¡åªåŠ è½½ä¸€éƒ¨åˆ†åˆ°å†…å­˜ï¼Œè®­ç»ƒåé‡Šæ”¾ã€‚

**ä»£ç ç¤ºä¾‹**:
```python
# training.py
for global_epoch in range(global_epochs):
    for chunk_idx in chunk_indices:
        targets, contexts = _load_positive_pairs_from_disk(chunk_path)
        # è®­ç»ƒ
        model.fit(dataset, ...)
        # é‡Šæ”¾å†…å­˜
        del targets, contexts
        gc.collect()
```

**ä¿ç•™ç†ç”±**:
| æ–¹é¢ | è¯´æ˜ |
|------|------|
| **å†…å­˜æ•ˆç‡** | å°† 337 GB æ•°æ®å‹ç¼©åˆ° 20 GB ç£ç›˜ + 6 GB å†…å­˜è¿è¡Œ |
| **å¯æ‰©å±•æ€§** | æ•°æ®é‡å¢é•¿ 10 å€ä¹Ÿèƒ½å¤„ç†ï¼Œåªæ˜¯æ—¶é—´å¢åŠ  |
| **æ–­ç‚¹ç»­è®­** | æ¯ä¸ª chunk ç‹¬ç«‹ï¼Œè®­ç»ƒä¸­æ–­å¯ä»ä¸Šæ¬¡ä½ç½®ç»§ç»­ |
| **å·¥ä¸šå®è·µ** | å¤§å‚è®­ç»ƒå¤§æ¨¡å‹éƒ½é‡‡ç”¨ç±»ä¼¼çš„åˆ†å—ç­–ç•¥ |

**æœªæ¥è¿­ä»£å»ºè®®**: ä¿ç•™å¹¶å¢å¼ºï¼Œæ·»åŠ  checkpoint æœºåˆ¶ä¿å­˜è®­ç»ƒè¿›åº¦ã€‚

---

### 2. é…ç½®é›†ä¸­ç®¡ç†

**æè¿°**: ä½¿ç”¨ dataclass å°†æ‰€æœ‰é…ç½®é¡¹é›†ä¸­åœ¨ `config.py`ï¼Œé¿å…ç¡¬ç¼–ç ã€‚

**ä»£ç ç¤ºä¾‹**:
```python
# config.py
@dataclass
class TrainConfig:
    window_size: int = 5
    num_negative_samples: int = 5
    embedding_dim: int = 128
    # ...

@dataclass  
class Config:
    paths: PathConfig
    data: DataConfig
    train: TrainConfig
```

**ä¿ç•™ç†ç”±**:
| æ–¹é¢ | è¯´æ˜ |
|------|------|
| **å¯ç»´æŠ¤æ€§** | ä¿®æ”¹å‚æ•°åªéœ€æ”¹ä¸€å¤„ï¼Œä¸ç”¨å…¨å±€æœç´¢ |
| **å¯è¯»æ€§** | æ–°äººä¸€çœ‹ config.py å°±çŸ¥é“æœ‰å“ªäº›å¯é…ç½®é¡¹ |
| **ç¯å¢ƒé€‚é…** | å¯è½»æ¾æ‰©å±•ä¸ºä»ç¯å¢ƒå˜é‡æˆ– YAML æ–‡ä»¶åŠ è½½ |
| **ç±»å‹å®‰å…¨** | dataclass æä¾›ç±»å‹æç¤ºï¼ŒIDE å¯ä»¥è‡ªåŠ¨è¡¥å…¨ |

**æœªæ¥è¿­ä»£å»ºè®®**: ä¿ç•™ï¼Œè€ƒè™‘æ”¯æŒä» `config.yaml` åŠ è½½é…ç½®ã€‚

---

### 3. å¤šè®¾å¤‡è‡ªåŠ¨æ£€æµ‹

**æè¿°**: è‡ªåŠ¨æ£€æµ‹ NVIDIA CUDAã€Apple Metalã€CPUï¼Œæ— éœ€ç”¨æˆ·æ‰‹åŠ¨é…ç½®ã€‚

**ä»£ç ç¤ºä¾‹**:
```python
# config.py
def detect_device() -> Tuple[str, str]:
    gpus = tf.config.list_physical_devices("GPU")
    if gpus:
        if "NVIDIA" in device_name.upper():
            return "/GPU:0", "NVIDIA"
        if sys.platform == "darwin":
            return "/GPU:0", "Metal"
    return "/CPU:0", "CPU"
```

**ä¿ç•™ç†ç”±**:
| æ–¹é¢ | è¯´æ˜ |
|------|------|
| **ç”¨æˆ·å‹å¥½** | ç”¨æˆ·ä¸éœ€è¦å…³å¿ƒåº•å±‚ç¡¬ä»¶ |
| **è·¨å¹³å°** | Windows/Linux/macOS éƒ½èƒ½è¿è¡Œ |
| **ä¼˜é›…é™çº§** | æ²¡æœ‰ GPU è‡ªåŠ¨å›é€€åˆ° CPUï¼Œä¸ä¼šæŠ¥é”™ |
| **é›¶é…ç½®** | å³æ’å³ç”¨ï¼Œé™ä½ä½¿ç”¨é—¨æ§› |

**æœªæ¥è¿­ä»£å»ºè®®**: ä¿ç•™ï¼Œæ·»åŠ å¯¹ AMD ROCm çš„æ”¯æŒã€‚

---

### 4. å®ä½“ç±»å‹å‰ç¼€è®¾è®¡

**æè¿°**: ä¸ºä¸åŒç±»å‹çš„å®ä½“æ·»åŠ å‰ç¼€ï¼Œä½¿ Word2Vec èƒ½åŒºåˆ†è¯­ä¹‰ã€‚

**ä»£ç ç¤ºä¾‹**:
```python
# feature_engineering.py
ENTITY_PREFIXES = {
    "movie": "MOV_",    # MOV_tt0111161
    "actor": "ACT_",    # ACT_nm0000151
    "genre": "GEN_",    # GEN_Drama
    "era": "ERA_",      # ERA_1990s
}
```

**ä¿ç•™ç†ç”±**:
| æ–¹é¢ | è¯´æ˜ |
|------|------|
| **è¯­ä¹‰åŒºåˆ†** | "Drama" ä½œä¸ºç±»å‹ vs ä½œä¸ºç”µå½±åï¼Œå‰ç¼€æ˜ç¡®åŒºåˆ† |
| **é¿å…ç¢°æ’** | ä¸åŒç±»å‹çš„ ID å¯èƒ½ç›¸åŒï¼Œå‰ç¼€é¿å…æ··æ·† |
| **å¯è§£é‡Šæ€§** | çœ‹åˆ° `MOV_tt0111161` ç«‹åˆ»çŸ¥é“æ˜¯ç”µå½± |
| **ä¸‹æ¸¸å‹å¥½** | æ¨èç³»ç»Ÿå¯ä»¥æŒ‰å‰ç¼€è¿‡æ»¤å€™é€‰é›† |

**æœªæ¥è¿­ä»£å»ºè®®**: ä¿ç•™ï¼Œä½†éœ€è¦ç»Ÿä¸€åŒä¸€å®ä½“çš„å‰ç¼€ï¼ˆä¸€äººä¸€å‰ç¼€ï¼‰ã€‚

---

### 5. åªå­˜æ­£æ ·æœ¬çš„ä¼˜åŒ–ç­–ç•¥

**æè¿°**: é¢„ç”Ÿæˆé˜¶æ®µåªå­˜å‚¨æ­£æ ·æœ¬å¯¹ï¼Œè´Ÿæ ·æœ¬åœ¨è®­ç»ƒæ—¶åŠ¨æ€é‡‡æ ·ã€‚

**ä»£ç ç¤ºä¾‹**:
```python
# pretraining.py - åªå­˜ 8 bytes/æ ·æœ¬
np.savez_compressed(chunk_path, targets=targets, contexts=contexts)

# training.py - åŠ¨æ€æ·»åŠ è´Ÿæ ·æœ¬
negatives = rng.choice(len(neg_prob), size=(n, num_ns), p=neg_prob)
```

**ä¿ç•™ç†ç”±**:
| æ–¹é¢ | è¯´æ˜ |
|------|------|
| **ç©ºé—´æ•ˆç‡** | ä» 172 bytes/æ ·æœ¬é™åˆ° 8 bytes/æ ·æœ¬ï¼ŒèŠ‚çœ 95% |
| **çµæ´»æ€§** | å¯éšæ—¶è°ƒæ•´è´Ÿæ ·æœ¬æ•°ï¼Œæ— éœ€é‡æ–°ç”Ÿæˆæ•°æ® |
| **éšæœºæ€§** | æ¯ä¸ª epoch ä½¿ç”¨ä¸åŒè´Ÿæ ·æœ¬ï¼Œæ³›åŒ–æ›´å¥½ |
| **ç ”ç©¶è¡¨æ˜** | åŠ¨æ€è´Ÿé‡‡æ ·æ•ˆæœä¸æ¯”é™æ€å·® |

**æœªæ¥è¿­ä»£å»ºè®®**: ä¿ç•™ï¼Œè¿™æ˜¯æ ¸å¿ƒä¼˜åŒ–ã€‚

---

### 6. å¤šç§åºåˆ—ç±»å‹è®¾è®¡

**æè¿°**: è®¾è®¡ 9 ç§ä¸åŒçš„åºåˆ—ç±»å‹ï¼Œæ•è·å¤šç»´åº¦å…³ç³»ã€‚

**ä»£ç ç¤ºä¾‹**:
```python
# feature_engineering.py
all_sequences.extend(_generate_person_movie_sequences(...))    # äºº-ç”µå½±
all_sequences.extend(_generate_movie_context_sequences(...))   # ç”µå½±ä¸Šä¸‹æ–‡
all_sequences.extend(_generate_coactor_sequences(...))         # åˆä½œæ¼”å‘˜
all_sequences.extend(_generate_era_movie_sequences(...))       # åŒå¹´ä»£
all_sequences.extend(_generate_rating_movie_sequences(...))    # åŒè¯„åˆ†
all_sequences.extend(_generate_director_genre_sequences(...))  # å¯¼æ¼”åå¥½
all_sequences.extend(_generate_actor_genre_sequences(...))     # æ¼”å‘˜åå¥½
```

**ä¿ç•™ç†ç”±**:
| æ–¹é¢ | è¯´æ˜ |
|------|------|
| **å…³ç³»ä¸°å¯Œ** | æ•è·äºº-ç‰©ã€ç‰©-ç‰©ã€äºº-äººå¤šç§å…³ç³» |
| **ç‰¹å¾å·¥ç¨‹** | è¿™æ˜¯æ¨èç³»ç»Ÿæœ€æ ¸å¿ƒçš„éƒ¨åˆ† |
| **å¯æ‰©å±•** | å®¹æ˜“æ·»åŠ æ–°çš„åºåˆ—ç±»å‹ |
| **åˆ›æ–°ç‚¹** | å¹´ä»£/è¯„åˆ†åºåˆ—æ˜¯æœ‰ä»·å€¼çš„åˆ›æ–° |

**æœªæ¥è¿­ä»£å»ºè®®**: ä¿ç•™å¹¶æ‰©å±•ï¼Œè€ƒè™‘æ·»åŠ ç”¨æˆ·è¡Œä¸ºåºåˆ—ã€‚

---

### 7. å®Œå–„çš„å¯¼å‡ºæ¨¡å—

**æè¿°**: æ”¯æŒå¤šç§æ ¼å¼å¯¼å‡ºï¼Œä¾¿äºä¸åŒåœºæ™¯ä½¿ç”¨ã€‚

**ä»£ç ç¤ºä¾‹**:
```python
# export.py
export_tsv()           # TensorFlow Projector
export_onnx()          # åœ¨çº¿æ¨ç†
export_json_embeddings()  # ç½‘é¡µå¯è§†åŒ–
export_clustering_visualization()  # t-SNE èšç±»
export_html_visualization()  # äº¤äº’å¼ç½‘é¡µ
export_recommendation_config()  # æ¨èç³»ç»Ÿé…ç½®
```

**ä¿ç•™ç†ç”±**:
| æ–¹é¢ | è¯´æ˜ |
|------|------|
| **éƒ¨ç½²å‹å¥½** | ONNX å¯éƒ¨ç½²åˆ°ä»»ä½•æ¨ç†å¼•æ“ |
| **å¯è§†åŒ–** | HTML å¯ç›´æ¥å±•ç¤ºæˆæœ |
| **è°ƒè¯•æ–¹ä¾¿** | JSON æ ¼å¼ä¾¿äºæ£€æŸ¥å’Œè°ƒè¯• |
| **æ ‡å‡†å…¼å®¹** | TSV å…¼å®¹ TensorFlow Embedding Projector |

**æœªæ¥è¿­ä»£å»ºè®®**: ä¿ç•™ï¼Œè€ƒè™‘æ·»åŠ  TensorRT å¯¼å‡ºã€‚

---

### 8. CLI å‘½ä»¤è¡Œè®¾è®¡

**æè¿°**: æä¾›å®Œæ•´çš„å‘½ä»¤è¡Œæ¥å£ï¼Œæ”¯æŒåˆ†æ­¥æ‰§è¡Œå’Œå®Œæ•´æµç¨‹ã€‚

**ä»£ç ç¤ºä¾‹**:
```bash
python -m imdb_word2vec.cli download
python -m imdb_word2vec.cli preprocess
python -m imdb_word2vec.cli fe
python -m imdb_word2vec.cli pretrain
python -m imdb_word2vec.cli train --use-cache
python -m imdb_word2vec.cli export
python -m imdb_word2vec.cli all  # ä¸€é”®æ‰§è¡Œ
```

**ä¿ç•™ç†ç”±**:
| æ–¹é¢ | è¯´æ˜ |
|------|------|
| **çµæ´»æ€§** | å¯åˆ†æ­¥æ‰§è¡Œï¼Œä¾¿äºè°ƒè¯•å•ä¸ªæ­¥éª¤ |
| **è‡ªåŠ¨åŒ–** | å¯é›†æˆåˆ° shell è„šæœ¬æˆ– CI/CD |
| **ç”¨æˆ·å‹å¥½** | æ¯”ä¿®æ”¹ä»£ç æ›´ç›´è§‚ |
| **å¯ç»„åˆ** | ä¸åŒå‘½ä»¤å¯è‡ªç”±ç»„åˆ |

**æœªæ¥è¿­ä»£å»ºè®®**: ä¿ç•™ï¼Œæ·»åŠ  `--help` çš„ä¸­æ–‡è¯´æ˜ã€‚

---

### 9. æ¨¡å‹å‚æ•°ç»Ÿè®¡åŠŸèƒ½

**æè¿°**: è®­ç»ƒå‰æ‰“å°æ¨¡å‹å‚æ•°é‡å’Œå¤§å°ã€‚

**ä»£ç ç¤ºä¾‹**:
```python
# training.py
def _log_model_stats(model: Word2Vec):
    logger.info("æ€»å‚æ•°é‡: %s (%s)", f"{total:,}", param_str)
    logger.info("æ¨¡å‹å¤§å°: %.2f MB (float32)", stats["model_size_mb"])

# è¾“å‡º:
# æ€»å‚æ•°é‡: 12,800,000 (12.80M)
# æ¨¡å‹å¤§å°: 48.83 MB
```

**ä¿ç•™ç†ç”±**:
| æ–¹é¢ | è¯´æ˜ |
|------|------|
| **é€æ˜åº¦** | ç”¨æˆ·æ¸…æ¥šçŸ¥é“æ¨¡å‹è§„æ¨¡ |
| **å¯¹æ¯”åŸºå‡†** | ä¾¿äºä¸å…¶ä»–æ¨¡å‹å¯¹æ¯” |
| **èµ„æºä¼°ç®—** | å¯æ®æ­¤ä¼°ç®—è®­ç»ƒ/æ¨ç†èµ„æº |
| **ä¸“ä¸šæ€§** | è¿™æ˜¯å¤§æ¨¡å‹æ—¶ä»£çš„æ ‡å‡†åšæ³• |

**æœªæ¥è¿­ä»£å»ºè®®**: ä¿ç•™ï¼Œæ·»åŠ  FLOPs ä¼°ç®—ã€‚

---

### 10. æ—¥å¿—ç³»ç»Ÿè§„èŒƒ

**æè¿°**: ç»Ÿä¸€çš„æ—¥å¿—æ ¼å¼ï¼Œå¸¦æ—¶é—´æˆ³å’Œçº§åˆ«ã€‚

**ä»£ç ç¤ºä¾‹**:
```python
# logging_utils.py
def setup_logging(logs_dir: Path) -> logging.Logger:
    formatter = logging.Formatter(
        "[%(asctime)s][%(levelname)s] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )
    # ...

# è¾“å‡º:
# [2025-12-23 17:14:24][INFO] è¯è¡¨è§„æ¨¡: 50000
```

**ä¿ç•™ç†ç”±**:
| æ–¹é¢ | è¯´æ˜ |
|------|------|
| **å¯è¿½æº¯** | æ—¶é—´æˆ³ä¾¿äºå®šä½é—®é¢˜ |
| **ç»Ÿä¸€æ ¼å¼** | æ‰€æœ‰æ¨¡å—æ—¥å¿—é£æ ¼ä¸€è‡´ |
| **æŒä¹…åŒ–** | æ—¥å¿—ä¿å­˜åˆ°æ–‡ä»¶ï¼Œè®­ç»ƒå®Œå¯å›æº¯ |
| **è°ƒè¯•å‹å¥½** | INFO/WARNING/ERROR åˆ†çº§æ¸…æ™° |

**æœªæ¥è¿­ä»£å»ºè®®**: ä¿ç•™ï¼Œè€ƒè™‘æ·»åŠ  JSON æ ¼å¼æ—¥å¿—é€‰é¡¹ã€‚

---

## âŒ ç¼ºç‚¹æ¸…å•ï¼ˆåº”ä¿®å¤ï¼‰

---

### 1. ç›®æ ‡ä¸å®ç°ä¸åŒ¹é…

**é—®é¢˜æè¿°**: é¡¹ç›®å£°ç§°æ˜¯"ç”µå½±æ¨èç³»ç»Ÿ"ï¼Œå®é™…åªå®ç°äº†åµŒå…¥è®­ç»ƒã€‚

**å½±å“**:
```
ç”¨æˆ·æœŸæœ›: è¾“å…¥ç”¨æˆ· ID â†’ è¾“å‡ºæ¨èç”µå½±åˆ—è¡¨
å®é™…èƒ½åš: è¾“å…¥ç”µå½± ID â†’ è¾“å‡ºç›¸ä¼¼ç”µå½±ï¼ˆéœ€è¦é¢å¤–ä»£ç ï¼‰

ç¼ºå¤±ç»„ä»¶:
â”œâ”€â”€ ç”¨æˆ·ç”»åƒ
â”œâ”€â”€ å¬å›å±‚
â”œâ”€â”€ ç²¾æ’å±‚
â”œâ”€â”€ åœ¨çº¿æœåŠ¡
â””â”€â”€ è¯„ä¼°ä½“ç³»
```

**ä¿®å¤æ–¹æ¡ˆ**:
```python
# æ–¹æ¡ˆ A: æ˜ç¡®ç›®æ ‡ï¼Œæ”¹åä¸º "IMDb Embedding Trainer"

# æ–¹æ¡ˆ B: è¡¥å…¨æ¨èç³»ç»Ÿç»„ä»¶
# recommender/
# â”œâ”€â”€ retrieval.py      # å¬å›ï¼šANN æœ€è¿‘é‚»æœç´¢
# â”œâ”€â”€ ranking.py        # ç²¾æ’ï¼šCTR é¢„ä¼°æ¨¡å‹
# â”œâ”€â”€ serving.py        # åœ¨çº¿æœåŠ¡ API
# â””â”€â”€ evaluation.py     # ç¦»çº¿è¯„ä¼° (Recall@K, NDCG)
```

**ä¿®å¤ä¼˜å…ˆçº§**: ğŸ”´ P0ï¼ˆå¿…é¡»ï¼‰

---

### 2. æµ‹è¯•è¦†ç›–ä¸ºé›¶

**é—®é¢˜æè¿°**: `tests/` ç›®å½•åªæœ‰ import æµ‹è¯•å’Œè·¯å¾„æµ‹è¯•ï¼Œæ ¸å¿ƒé€»è¾‘æ— æµ‹è¯•ã€‚

**å½±å“**:
```
é£é™©:
â”œâ”€â”€ é‡æ„æ—¶ä¸çŸ¥é“æ˜¯å¦ç ´åäº†åŠŸèƒ½
â”œâ”€â”€ è¾¹ç•Œæ¡ä»¶æœªéªŒè¯
â”œâ”€â”€ Bug åªèƒ½åœ¨è¿è¡Œæ—¶å‘ç°
â””â”€â”€ ä»£ç è´¨é‡æ— æ³•ä¿è¯
```

**ä¿®å¤æ–¹æ¡ˆ**:
```python
# tests/test_feature_engineering.py
def test_rating_to_bucket():
    assert _rating_to_bucket(7.3) == "7.5"
    assert _rating_to_bucket(7.2) == "7.0"
    assert _rating_to_bucket("invalid") == "UNKNOWN"

def test_add_prefix():
    assert _add_prefix("tt0111161", "movie") == "MOV_tt0111161"
    assert _add_prefix("\\N", "movie") is None

# tests/test_training.py
def test_add_negative_samples():
    targets = np.array([1, 2, 3])
    contexts = np.array([4, 5, 6])
    # éªŒè¯è¾“å‡ºå½¢çŠ¶ã€ç±»å‹ã€å€¼èŒƒå›´
```

**ä¿®å¤ä¼˜å…ˆçº§**: ğŸ”´ P0ï¼ˆå¿…é¡»ï¼‰

---

### 3. Autoencoder æ¨¡å—å­¤ç«‹

**é—®é¢˜æè¿°**: Autoencoder ä¸ Word2Vec å®Œå…¨ç‹¬ç«‹ï¼Œç”Ÿæˆçš„ç‰¹å¾æ— ä¸‹æ¸¸æ¶ˆè´¹è€…ã€‚

**å½±å“**:
```python
# å½“å‰æ•°æ®æµ:
# feature_engineering â†’ tabular_features.csv â†’ autoencoder â†’ fused_features.parquet â†’ â“

# fused_features.parquet æ²¡äººç”¨ï¼
```

**ä¿®å¤æ–¹æ¡ˆ**:
```python
# æ–¹æ¡ˆ A: åˆ é™¤ Autoencoderï¼ˆå¦‚æœä¸éœ€è¦ï¼‰

# æ–¹æ¡ˆ B: ä¸ Word2Vec è”åˆä½¿ç”¨
def get_item_embedding(item_id):
    w2v_emb = word2vec.get_embedding(item_id)  # 128 ç»´
    ae_emb = autoencoder.get_embedding(item_id)  # 64 ç»´
    return np.concatenate([w2v_emb, ae_emb])  # 192 ç»´

# æ–¹æ¡ˆ C: ç”¨äºç²¾æ’æ¨¡å‹
class RankingModel:
    def __init__(self):
        self.w2v_embeddings = load_word2vec()
        self.ae_features = load_autoencoder()
    
    def predict(self, user_id, item_id):
        features = concat(self.w2v_embeddings[item_id], 
                         self.ae_features[item_id])
        return self.mlp(features)
```

**ä¿®å¤ä¼˜å…ˆçº§**: ğŸŸ¡ P1ï¼ˆé‡è¦ï¼‰

---

### 4. å®ä½“å‰ç¼€ä¸ä¸€è‡´

**é—®é¢˜æè¿°**: åŒä¸€ä¸ªäººå¯èƒ½æœ‰å¤šä¸ªå‰ç¼€ï¼ˆPER_ã€ACT_ã€DIR_ï¼‰ã€‚

**å½±å“**:
```python
# åŒä¸€ä¸ªäºº:
# PER_nm0000151  â† ä½œä¸ºæ™®é€šäººå‘˜
# ACT_nm0000151  â† ä½œä¸ºæ¼”å‘˜å‡ºç°
# DIR_nm0000151  â† ä½œä¸ºå¯¼æ¼”å‡ºç°

# é—®é¢˜: è¿™ä¸‰ä¸ªè¢«è§†ä¸ºä¸åŒå®ä½“ï¼Œä½†å®é™…æ˜¯åŒä¸€ä¸ªäºº
# å¯¼è‡´: åµŒå…¥ç©ºé—´åˆ†è£‚ï¼Œæ— æ³•æ­£ç¡®è®¡ç®—ç›¸ä¼¼åº¦
```

**ä¿®å¤æ–¹æ¡ˆ**:
```python
# æ–¹æ¡ˆ: ç»Ÿä¸€ä½¿ç”¨ PER_ å‰ç¼€ï¼Œç”¨å±æ€§åŒºåˆ†è§’è‰²
ENTITY_PREFIXES = {
    "movie": "MOV_",
    "person": "PER_",   # æ‰€æœ‰äººéƒ½ç”¨è¿™ä¸ª
    "genre": "GEN_",
    "era": "ERA_",
}

# è§’è‰²ä¿¡æ¯æ”¾åˆ°åºåˆ—ä¸Šä¸‹æ–‡ä¸­:
# [PER_nm001, ROLE_actor, MOV_tt001]
# [PER_nm001, ROLE_director, MOV_tt002]
```

**ä¿®å¤ä¼˜å…ˆçº§**: ğŸŸ¡ P1ï¼ˆé‡è¦ï¼‰

---

### 5. feature_engineering.py æ€§èƒ½ä½ä¸‹

**é—®é¢˜æè¿°**: å¤§é‡ä½¿ç”¨ `iterrows()`ï¼Œè¿™æ˜¯ pandas æœ€æ…¢çš„éå†æ–¹å¼ã€‚

**å½±å“**:
```python
# å½“å‰ä»£ç  (æ…¢):
for _, row in tqdm(staff_df.iterrows(), ...):  # O(n) æ…¢
    # å¤„ç†é€»è¾‘

# 21M è¡Œ Ã— å¤šä¸ªå¾ªç¯ = æ•°å°æ—¶
```

**ä¿®å¤æ–¹æ¡ˆ**:
```python
# æ–¹æ¡ˆ A: ä½¿ç”¨ groupby + apply
person_movies = (
    title_principals_df
    .groupby("nconst")["tconst"]
    .apply(list)
    .to_dict()
)

# æ–¹æ¡ˆ B: ä½¿ç”¨å‘é‡åŒ–æ“ä½œ
staff_df["prefixed_tconst"] = "MOV_" + staff_df["tconst"]

# æ–¹æ¡ˆ C: ä½¿ç”¨ polars (æ¯” pandas å¿« 10-100 å€)
import polars as pl
df = pl.read_csv("data.csv")
result = df.groupby("nconst").agg(pl.col("tconst").alias("movies"))
```

**ä¿®å¤ä¼˜å…ˆçº§**: ğŸŸ¡ P1ï¼ˆé‡è¦ï¼‰

---

### 6. ç¼ºä¹è¯„ä¼°æŒ‡æ ‡

**é—®é¢˜æè¿°**: åªæœ‰ loss å’Œ accuracyï¼Œæ— æ¨èç³»ç»Ÿä¸“ç”¨æŒ‡æ ‡ã€‚

**å½±å“**:
```
å½“å‰æŒ‡æ ‡:
â”œâ”€â”€ loss: 3.2451      â† ä¸çŸ¥é“å¥½å
â”œâ”€â”€ accuracy: 0.43    â† ä¸çŸ¥é“å¥½å
â””â”€â”€ positive_recall: 0.56  â† ä¸çŸ¥é“å¥½å

æ— æ³•å›ç­”:
â”œâ”€â”€ æ¨èçš„ç”µå½±ç”¨æˆ·ä¼šå–œæ¬¢å—ï¼Ÿ
â”œâ”€â”€ ç›¸ä¼¼ç”µå½±çœŸçš„ç›¸ä¼¼å—ï¼Ÿ
â””â”€â”€ åµŒå…¥è´¨é‡å¦‚ä½•ï¼Ÿ
```

**ä¿®å¤æ–¹æ¡ˆ**:
```python
# evaluation.py
def evaluate_embeddings(embeddings, test_pairs):
    """è¯„ä¼°åµŒå…¥è´¨é‡"""
    # 1. ç›¸ä¼¼åº¦ä»»åŠ¡
    hit_rate_10 = calc_hit_rate(embeddings, test_pairs, k=10)
    mrr = calc_mrr(embeddings, test_pairs)
    
    # 2. èšç±»è´¨é‡
    silhouette = calc_silhouette_score(embeddings)
    
    # 3. ä¸‹æ¸¸ä»»åŠ¡
    genre_classification_acc = eval_genre_classification(embeddings)
    
    return {
        "Hit@10": hit_rate_10,
        "MRR": mrr,
        "Silhouette": silhouette,
        "Genre_Acc": genre_classification_acc,
    }
```

**ä¿®å¤ä¼˜å…ˆçº§**: ğŸŸ¡ P1ï¼ˆé‡è¦ï¼‰

---

### 7. å¼‚å¸¸å¤„ç†ç¼ºå¤±

**é—®é¢˜æè¿°**: å…³é”®æ“ä½œç¼ºå°‘ try-exceptï¼Œé”™è¯¯æ—¶ç›´æ¥å´©æºƒã€‚

**å½±å“**:
```python
# å½“å‰ä»£ç :
data = np.load(chunk_path)  # æ–‡ä»¶æŸåï¼Ÿç›´æ¥å´©æºƒ
return data["targets"]       # key ä¸å­˜åœ¨ï¼Ÿç›´æ¥å´©æºƒ

# è®­ç»ƒåˆ°ä¸€åŠå´©æºƒï¼Œæ‰€æœ‰è¿›åº¦ä¸¢å¤±
```

**ä¿®å¤æ–¹æ¡ˆ**:
```python
def _load_positive_pairs_from_disk(chunk_path: Path):
    try:
        data = np.load(chunk_path)
        targets = data["targets"]
        contexts = data["contexts"]
        return targets, contexts
    except FileNotFoundError:
        logger.error("æ‰¾ä¸åˆ°æ–‡ä»¶: %s", chunk_path)
        raise
    except KeyError as e:
        logger.error("æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘ key: %s", e)
        raise
    except Exception as e:
        logger.error("åŠ è½½å¤±è´¥: %s, é”™è¯¯: %s", chunk_path, e)
        raise
```

**ä¿®å¤ä¼˜å…ˆçº§**: ğŸŸ  P2ï¼ˆå»ºè®®ï¼‰

---

### 8. é­”æ³•æ•°å­—æ— æ–‡æ¡£

**é—®é¢˜æè¿°**: é…ç½®æ–‡ä»¶ä¸­çš„æ•°å€¼æ²¡æœ‰è§£é‡Šä¸ºä»€ä¹ˆé€‰è¿™ä¸ªå€¼ã€‚

**å½±å“**:
```python
# config.py
subsample_t: float = 1e-4     # ä¸ºä»€ä¹ˆæ˜¯ 1e-4ï¼Ÿ
window_size: int = 5          # ä¸ºä»€ä¹ˆæ˜¯ 5ï¼Ÿ
num_negative_samples: int = 5 # ä¸ºä»€ä¹ˆæ˜¯ 5ï¼Ÿ
embedding_dim: int = 128      # ä¸ºä»€ä¹ˆæ˜¯ 128ï¼Ÿ

# æ–°äººæ¥æ‰‹æ—¶å®Œå…¨ä¸çŸ¥é“èƒ½ä¸èƒ½æ”¹ã€æ”¹äº†ä¼šæ€æ ·
```

**ä¿®å¤æ–¹æ¡ˆ**:
```python
@dataclass
class TrainConfig:
    # é«˜é¢‘å­é‡‡æ ·é˜ˆå€¼
    # å‚è€ƒ: Mikolov et al. 2013, æ¨èå€¼ 1e-3 åˆ° 1e-5
    # è¶Šå°è¶Šæ¿€è¿›ï¼Œä¸¢å¼ƒæ›´å¤šé«˜é¢‘è¯
    subsample_t: float = 1e-4
    
    # Skip-gram çª—å£å¤§å°
    # å‚è€ƒ: ä¸€èˆ¬ 5-10ï¼Œçª—å£è¶Šå¤§æ•è·è¶Šè¿œçš„è¯­ä¹‰å…³ç³»
    # ä½†è®¡ç®—é‡ä¹Ÿè¶Šå¤§
    window_size: int = 5
    
    # è´Ÿæ ·æœ¬æ•°é‡
    # å‚è€ƒ: åŸè®ºæ–‡æ¨è 5-20ï¼Œå°æ•°æ®é›†ç”¨ 5-10 è¶³å¤Ÿ
    # å¢åŠ å¯æå‡è´¨é‡ä½†è®­ç»ƒå˜æ…¢
    num_negative_samples: int = 5
```

**ä¿®å¤ä¼˜å…ˆçº§**: ğŸŸ  P2ï¼ˆå»ºè®®ï¼‰

---

### 9. é‡å¤ä»£ç 

**é—®é¢˜æè¿°**: `TqdmProgressCallback` åœ¨ä¸¤ä¸ªæ–‡ä»¶ä¸­é‡å¤å®šä¹‰ã€‚

**å½±å“**:
```python
# training.py ç¬¬ 39-80 è¡Œ
class TqdmProgressCallback(tf.keras.callbacks.Callback):
    # ... 42 è¡Œä»£ç 

# autoencoder.py ç¬¬ 34-55 è¡Œ
class TqdmProgressCallback(tf.keras.callbacks.Callback):
    # ... 22 è¡Œä»£ç 

# é—®é¢˜: æ”¹ä¸€ä¸ªå¿˜äº†æ”¹å¦ä¸€ä¸ªï¼Œè¡Œä¸ºä¸ä¸€è‡´
```

**ä¿®å¤æ–¹æ¡ˆ**:
```python
# utils.py (æ–°æ–‡ä»¶)
class TqdmProgressCallback(tf.keras.callbacks.Callback):
    """ç»Ÿä¸€çš„è¿›åº¦æ¡å›è°ƒ"""
    # ... å®Œæ•´å®ç°

# training.py
from .utils import TqdmProgressCallback

# autoencoder.py
from .utils import TqdmProgressCallback
```

**ä¿®å¤ä¼˜å…ˆçº§**: ğŸŸ  P2ï¼ˆå»ºè®®ï¼‰

---

### 10. ç¼ºå°‘ CI/CD å’Œä»£ç è´¨é‡æ£€æŸ¥

**é—®é¢˜æè¿°**: æ²¡æœ‰è‡ªåŠ¨åŒ–æµ‹è¯•ã€æ ¼å¼æ£€æŸ¥ã€ç±»å‹æ£€æŸ¥ã€‚

**å½±å“**:
```
é£é™©:
â”œâ”€â”€ PR åˆå¹¶å¯èƒ½å¼•å…¥ bug
â”œâ”€â”€ ä»£ç é£æ ¼ä¸ç»Ÿä¸€
â”œâ”€â”€ ç±»å‹é”™è¯¯åœ¨è¿è¡Œæ—¶æ‰å‘ç°
â””â”€â”€ ä¾èµ–ç‰ˆæœ¬ä¸ä¸€è‡´
```

**ä¿®å¤æ–¹æ¡ˆ**:
```yaml
# .github/workflows/ci.yml
name: CI
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      - run: pip install -r requirements.txt
      - run: pip install pytest ruff mypy
      - run: ruff check .          # ä»£ç æ ¼å¼
      - run: mypy imdb_word2vec/   # ç±»å‹æ£€æŸ¥
      - run: pytest tests/ -v      # å•å…ƒæµ‹è¯•
```

```toml
# pyproject.toml
[tool.ruff]
line-length = 100
select = ["E", "F", "W", "I"]

[tool.mypy]
python_version = "3.12"
warn_return_any = true
```

**ä¿®å¤ä¼˜å…ˆçº§**: ğŸŸ  P2ï¼ˆå»ºè®®ï¼‰

---

### 11. Word2Vec ç®—æ³•è¿‡æ—¶

**é—®é¢˜æè¿°**: 2013 å¹´çš„ç®—æ³•ï¼Œæ— æ³•å¤„ç†å¤šä¹‰è¯å’Œ OOVã€‚

**å½±å“**:
```
å±€é™æ€§:
â”œâ”€â”€ "è‹¹æœ" åªæœ‰ä¸€ä¸ªå‘é‡ï¼Œæ— æ³•åŒºåˆ†æ°´æœ/å…¬å¸
â”œâ”€â”€ æ–°ç”µå½±/æ–°æ¼”å‘˜æ— æ³•è·å¾—åµŒå…¥
â”œâ”€â”€ æ— æ³•å¤„ç†é•¿æ–‡æœ¬è¯­ä¹‰
â””â”€â”€ è´¨é‡ä¸å¦‚é¢„è®­ç»ƒæ¨¡å‹
```

**ä¿®å¤æ–¹æ¡ˆ**:
```python
# æ–¹æ¡ˆ A: å‡çº§åˆ° FastText (æ”¯æŒå­è¯)
from gensim.models import FastText
model = FastText(sentences, vector_size=128, window=5)

# æ–¹æ¡ˆ B: ä½¿ç”¨ Sentence-BERT (æ›´å¼ºè¯­ä¹‰)
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(["The Shawshank Redemption"])

# æ–¹æ¡ˆ C: ä½¿ç”¨ Two-Tower æ¨¡å‹ (ä¸“ä¸ºæ¨èè®¾è®¡)
# å‚è€ƒ Google çš„ YouTube æ¨èç³»ç»Ÿæ¶æ„
```

**ä¿®å¤ä¼˜å…ˆçº§**: ğŸ”µ P3ï¼ˆé•¿æœŸï¼‰

---

## ğŸ“Š ä¼˜å…ˆçº§æ€»ç»“

| ä¼˜å…ˆçº§ | é¡¹ç›® | é¢„è®¡å·¥æ—¶ |
|--------|------|----------|
| ğŸ”´ P0 | æ˜ç¡®é¡¹ç›®ç›®æ ‡ | 1 å¤© |
| ğŸ”´ P0 | æ·»åŠ å•å…ƒæµ‹è¯• | 3 å¤© |
| ğŸŸ¡ P1 | æ•´åˆ/åˆ é™¤ Autoencoder | 1 å¤© |
| ğŸŸ¡ P1 | ç»Ÿä¸€å®ä½“å‰ç¼€ | 0.5 å¤© |
| ğŸŸ¡ P1 | ä¼˜åŒ– feature_engineering æ€§èƒ½ | 2 å¤© |
| ğŸŸ¡ P1 | æ·»åŠ è¯„ä¼°æŒ‡æ ‡ | 2 å¤© |
| ğŸŸ  P2 | æ·»åŠ å¼‚å¸¸å¤„ç† | 1 å¤© |
| ğŸŸ  P2 | æ–‡æ¡£åŒ–é­”æ³•æ•°å­— | 0.5 å¤© |
| ğŸŸ  P2 | æŠ½å–é‡å¤ä»£ç  | 0.5 å¤© |
| ğŸŸ  P2 | æ·»åŠ  CI/CD | 1 å¤© |
| ğŸ”µ P3 | å‡çº§åµŒå…¥ç®—æ³• | 5 å¤© |

**æ€»è®¡**: çº¦ 17.5 å¤©å·¥ä½œé‡