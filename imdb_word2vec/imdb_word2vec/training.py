"""
Word2Vec è®­ç»ƒæ¨¡å—

æ”¯æŒä¸¤ç§è®­ç»ƒæ¨¡å¼ï¼š
1. ä»é¢„ç”Ÿæˆçš„æ­£æ ·æœ¬å¯¹è®­ç»ƒï¼ˆæ¨èï¼Œè´Ÿæ ·æœ¬åŠ¨æ€é‡‡æ ·ï¼‰
2. æµå¼è®­ç»ƒï¼šè¾¹ç”Ÿæˆè¾¹è®­ç»ƒï¼ˆé€‚åˆé¦–æ¬¡è¿è¡Œï¼‰

ä½¿ç”¨æ–¹æ³•:
    # é¢„ç”Ÿæˆ + è®­ç»ƒï¼ˆæ¨èï¼‰
    python -m imdb_word2vec.cli pretrain
    python -m imdb_word2vec.cli train --use-cache
    
    # æµå¼è®­ç»ƒ
    python -m imdb_word2vec.cli train
"""
from __future__ import annotations

import gc
import json
from pathlib import Path
from typing import Callable, List, Optional, Tuple

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, mixed_precision
from tensorflow.keras.callbacks import ReduceLROnPlateau
from tqdm import tqdm

from .config import CONFIG
from .logging_utils import setup_logging


logger = setup_logging(CONFIG.paths.logs_dir)


# ========== è¿›åº¦æ¡å›è°ƒ ==========

class TqdmProgressCallback(tf.keras.callbacks.Callback):
    """ä½¿ç”¨ tqdm æ˜¾ç¤ºè®­ç»ƒè¿›åº¦çš„å›è°ƒã€‚"""

    def __init__(self, epochs: int, desc: str = "è®­ç»ƒä¸­"):
        super().__init__()
        self.epochs = epochs
        self.desc = desc
        self.epoch_bar = None
        self.batch_bar = None

    def on_train_begin(self, logs=None):
        self.epoch_bar = tqdm(total=self.epochs, desc=self.desc, position=0)

    def on_epoch_begin(self, epoch, logs=None):
        steps = self.params.get("steps", 0)
        self.batch_bar = tqdm(total=steps, desc=f"Epoch {epoch+1}", position=1, leave=False)

    def on_batch_end(self, batch, logs=None):
        if self.batch_bar:
            self.batch_bar.update(1)
            if logs:
                self.batch_bar.set_postfix({
                    "loss": f"{logs.get('loss', 0):.4f}",
                    "acc": f"{logs.get('accuracy', 0):.4f}",
                })

    def on_epoch_end(self, epoch, logs=None):
        if self.batch_bar:
            self.batch_bar.close()
        if self.epoch_bar:
            self.epoch_bar.update(1)
            if logs:
                self.epoch_bar.set_postfix({
                    "loss": f"{logs.get('loss', 0):.4f}",
                    "acc": f"{logs.get('accuracy', 0):.4f}",
                })

    def on_train_end(self, logs=None):
        if self.batch_bar:
            self.batch_bar.close()
        if self.epoch_bar:
            self.epoch_bar.close()


# ========== æ­£æ ·æœ¬å¬å›ç‡ ==========

def positive_recall(y_true, y_pred):
    """æ­£æ ·æœ¬å¬å›ç‡ï¼šæ­£æ ·æœ¬ä½ç½®çš„ logit æ˜¯å¦æ˜¯æ‰€æœ‰ä½ç½®ä¸­æœ€å¤§çš„"""
    pred_max_idx = tf.argmax(y_pred, axis=-1)
    correct = tf.cast(tf.equal(pred_max_idx, 0), tf.float32)
    return tf.reduce_mean(correct)


# ========== è¾…åŠ©å‡½æ•° ==========

def _get_strategy() -> tf.distribute.Strategy:
    """æŒ‰éœ€åˆ›å»ºåˆ†å¸ƒå¼ç­–ç•¥ã€‚"""
    if CONFIG.train.enable_distribute:
        gpus = tf.config.list_physical_devices("GPU")
        if len(gpus) > 1:
            try:
                return tf.distribute.MirroredStrategy()
            except Exception:
                pass
    return tf.distribute.get_strategy()


# ========== Word2Vec æ¨¡å‹ ==========

class Word2Vec(tf.keras.Model):
    """Skip-gram è¯å‘é‡æ¨¡å‹ã€‚"""

    def __init__(self, vocab_size: int, embedding_dim: int):
        super().__init__()
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.target_embedding = layers.Embedding(vocab_size, embedding_dim, name="w2v_embedding")
        self.context_embedding = layers.Embedding(vocab_size, embedding_dim)

    def call(self, inputs):
        target, context = inputs
        target_emb = self.target_embedding(target)
        context_emb = self.context_embedding(context)
        dots = tf.einsum("be,bce->bc", target_emb, context_emb)
        return dots
    
    def get_model_stats(self) -> dict:
        """è·å–æ¨¡å‹å‚æ•°ç»Ÿè®¡ä¿¡æ¯ã€‚"""
        # è®¡ç®—å‚æ•°é‡
        target_params = self.vocab_size * self.embedding_dim
        context_params = self.vocab_size * self.embedding_dim
        total_params = target_params + context_params
        
        # è®¡ç®—æ¨¡å‹å¤§å°ï¼ˆfloat32ï¼Œæ¯ä¸ªå‚æ•° 4 bytesï¼‰
        model_size_bytes = total_params * 4
        model_size_mb = model_size_bytes / (1024 ** 2)
        
        return {
            "vocab_size": self.vocab_size,
            "embedding_dim": self.embedding_dim,
            "target_embedding_params": target_params,
            "context_embedding_params": context_params,
            "total_params": total_params,
            "model_size_mb": model_size_mb,
        }


def _log_model_stats(model: Word2Vec) -> None:
    """æ‰“å°æ¨¡å‹å‚æ•°ç»Ÿè®¡ä¿¡æ¯ã€‚"""
    stats = model.get_model_stats()
    total = stats["total_params"]
    
    # æ ¼å¼åŒ–å‚æ•°é‡
    if total >= 1e9:
        param_str = f"{total / 1e9:.2f}B"
    elif total >= 1e6:
        param_str = f"{total / 1e6:.2f}M"
    elif total >= 1e3:
        param_str = f"{total / 1e3:.2f}K"
    else:
        param_str = str(total)
    
    logger.info("========== æ¨¡å‹å‚æ•°ç»Ÿè®¡ ==========")
    logger.info("è¯è¡¨è§„æ¨¡: %d", stats["vocab_size"])
    logger.info("åµŒå…¥ç»´åº¦: %d", stats["embedding_dim"])
    logger.info("Target Embedding: %s å‚æ•°", f"{stats['target_embedding_params']:,}")
    logger.info("Context Embedding: %s å‚æ•°", f"{stats['context_embedding_params']:,}")
    logger.info("æ€»å‚æ•°é‡: %s (%s)", f"{total:,}", param_str)
    logger.info("æ¨¡å‹å¤§å°: %.2f MB (float32)", stats["model_size_mb"])


# ========== è´Ÿé‡‡æ ·ç›¸å…³ ==========

def _prepare_negative_sampling_probs(freq: np.ndarray, power: float = 0.75) -> np.ndarray:
    """å‡†å¤‡è´Ÿé‡‡æ ·çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆ3/4 æ¬¡å¹‚å¹³æ»‘ï¼‰ã€‚"""
    powered = np.power(freq.astype(np.float64), power)
    total = powered.sum()
    return powered / (total + 1e-10)


def _compute_subsample_probs(freq: np.ndarray, t: float) -> np.ndarray:
    """è®¡ç®—é«˜é¢‘å­é‡‡æ ·çš„ä¿ç•™æ¦‚ç‡ã€‚"""
    total = freq.sum()
    f = freq / (total + 1e-10)
    probs = np.sqrt(t / (f + 1e-10)) + t / (f + 1e-10)
    probs = np.clip(probs, 0, 1)
    probs[freq == 0] = 0
    return probs


# ========== å¯¼å‡ºå‡½æ•° ==========

def _export_all_formats(model: Word2Vec, tokens: List[str], output_dir: Path):
    """å¯¼å‡ºæ‰€æœ‰æ ¼å¼çš„è¯å‘é‡æ–‡ä»¶ã€‚"""
    from .export import export_all
    
    weights = model.target_embedding.get_weights()[0]
    export_all(weights, tokens, output_dir)


def _load_vocab_tokens(vocab_path: Path, vocab_size: int) -> List[str]:
    """ä»è¯è¡¨æ–‡ä»¶åŠ è½½ token åˆ—è¡¨ã€‚"""
    vocab_df = pd.read_csv(vocab_path, header=None, names=["key", "value"])
    inv = vocab_df.set_index("value")["key"].to_dict()
    tokens = [inv.get(i, f"<ID_{i}>") for i in range(vocab_size)]
    return tokens


# ========== ä»ç£ç›˜åŠ è½½æ­£æ ·æœ¬å¯¹ ==========

def _load_positive_pairs_from_disk(chunk_path: Path) -> Tuple[np.ndarray, np.ndarray]:
    """ä»ç£ç›˜åŠ è½½ä¸€ä¸ªæ­£æ ·æœ¬å¯¹æ–‡ä»¶ã€‚"""
    data = np.load(chunk_path)
    return data["targets"], data["contexts"]


def _create_gpu_negative_sampler(
    neg_prob: np.ndarray,
    num_ns: int,
    vocab_size: int,
) -> Callable:
    """
    åˆ›å»ºä¸€ä¸ªåœ¨ GPU ä¸Šæ‰§è¡Œè´Ÿé‡‡æ ·çš„ tf.functionã€‚
    
    ä½¿ç”¨ tf.random.categorical åœ¨ GPU ä¸Šé«˜æ•ˆé‡‡æ ·ï¼Œ
    ç›¸æ¯” CPU numpy é‡‡æ ·é€Ÿåº¦æå‡ 100-500 å€ã€‚
    
    Args:
        neg_prob: è´Ÿé‡‡æ ·æ¦‚ç‡åˆ†å¸ƒ (vocab_size,)
        num_ns: æ¯ä¸ªæ­£æ ·æœ¬å¯¹åº”çš„è´Ÿæ ·æœ¬æ•°
        vocab_size: è¯è¡¨å¤§å°
    
    Returns:
        ä¸€ä¸ª tf.functionï¼Œè¾“å…¥ (target, context)ï¼Œè¾“å‡º ((target, contexts_with_neg), labels)
    """
    # å°†æ¦‚ç‡è½¬æ¢ä¸º log æ¦‚ç‡ï¼ˆtf.random.categorical éœ€è¦ logitsï¼‰
    # æ·»åŠ å°å€¼é¿å… log(0)
    log_probs = tf.constant(
        np.log(neg_prob + 1e-10).astype(np.float32),
        dtype=tf.float32
    )
    
    # é¢„è®¡ç®—é™æ€ label æ¨¡æ¿
    label_template = tf.constant(
        np.array([1.0] + [0.0] * num_ns, dtype=np.float32),
        dtype=tf.float32
    )
    
    @tf.function
    def add_negative_samples_gpu(target: tf.Tensor, context: tf.Tensor) -> Tuple:
        """
        ä¸ºä¸€ä¸ª batch çš„æ­£æ ·æœ¬å¯¹æ·»åŠ è´Ÿæ ·æœ¬ï¼ˆåœ¨ GPU ä¸Šæ‰§è¡Œï¼‰ã€‚
        
        Args:
            target: shape (batch_size,) ç›®æ ‡è¯ ID
            context: shape (batch_size,) ä¸Šä¸‹æ–‡è¯ IDï¼ˆæ­£æ ·æœ¬ï¼‰
        
        Returns:
            ((target, contexts_with_neg), labels)
        """
        batch_size = tf.shape(target)[0]
        
        # åœ¨ GPU ä¸Šé‡‡æ ·è´Ÿæ ·æœ¬
        # tf.random.categorical è¾“å…¥æ˜¯ (batch_size, num_classes) çš„ logits
        # è¾“å‡ºæ˜¯ (batch_size, num_samples)
        log_probs_broadcast = tf.broadcast_to(
            log_probs[tf.newaxis, :],  # (1, vocab_size)
            [batch_size, vocab_size]    # (batch_size, vocab_size)
        )
        
        # é‡‡æ · num_ns ä¸ªè´Ÿæ ·æœ¬
        negatives = tf.random.categorical(
            log_probs_broadcast,
            num_samples=num_ns,
            dtype=tf.int32
        )  # (batch_size, num_ns)
        
        # ç»„åˆæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬
        context_expanded = tf.expand_dims(tf.cast(context, tf.int32), axis=1)  # (batch_size, 1)
        contexts_with_neg = tf.concat([context_expanded, negatives], axis=1)  # (batch_size, 1 + num_ns)
        
        # åˆ›å»ºæ ‡ç­¾ï¼ˆå¹¿æ’­ï¼‰
        labels = tf.broadcast_to(
            label_template[tf.newaxis, :],  # (1, 1 + num_ns)
            [batch_size, num_ns + 1]         # (batch_size, 1 + num_ns)
        )
        
        return (tf.cast(target, tf.int32), contexts_with_neg), labels
    
    return add_negative_samples_gpu


def _add_negative_samples(
    targets: np.ndarray,
    contexts: np.ndarray,
    neg_prob: np.ndarray,
    num_ns: int,
    rng: np.random.Generator,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    ä¸ºæ­£æ ·æœ¬å¯¹åŠ¨æ€æ·»åŠ è´Ÿæ ·æœ¬ï¼ˆCPU ç‰ˆæœ¬ï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼‰ã€‚
    
    æ³¨æ„ï¼šæ­¤å‡½æ•°å·²è¢« GPU ç‰ˆæœ¬ _create_gpu_negative_sampler å–ä»£ï¼Œ
    ä½†ä¿ç•™ä»¥æ”¯æŒæ—§ä»£ç è·¯å¾„ã€‚
    
    Args:
        targets: ç›®æ ‡è¯ ID æ•°ç»„
        contexts: æ­£æ ·æœ¬ä¸Šä¸‹æ–‡è¯ ID æ•°ç»„
        neg_prob: è´Ÿé‡‡æ ·æ¦‚ç‡åˆ†å¸ƒ
        num_ns: è´Ÿæ ·æœ¬æ•°é‡
        rng: éšæœºæ•°ç”Ÿæˆå™¨
    
    Returns:
        (targets, contexts_with_neg, labels)
        contexts_with_neg: shape (n, 1 + num_ns)ï¼Œç¬¬ä¸€åˆ—æ˜¯æ­£æ ·æœ¬
        labels: shape (n, 1 + num_ns)ï¼Œç¬¬ä¸€åˆ—æ˜¯ 1ï¼Œå…¶ä½™æ˜¯ 0
    """
    n = len(targets)
    
    # é‡‡æ ·è´Ÿæ ·æœ¬
    negatives = rng.choice(len(neg_prob), size=(n, num_ns), p=neg_prob, replace=True)
    
    # ç»„åˆæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬
    contexts_with_neg = np.concatenate([contexts.reshape(-1, 1), negatives], axis=1).astype(np.int32)
    
    # åˆ›å»ºæ ‡ç­¾ï¼ˆç¬¬ä¸€åˆ—æ˜¯ 1ï¼Œå…¶ä½™æ˜¯ 0ï¼‰
    labels = np.zeros((n, 1 + num_ns), dtype=np.float32)
    labels[:, 0] = 1.0
    
    return targets, contexts_with_neg, labels


# ========== ä»ç¼“å­˜è®­ç»ƒ ==========

def train_word2vec_from_cache(
    samples_dir: Path,
    vocab_path: Path,
) -> Tuple[Path, Path]:
    """
    ä»é¢„ç”Ÿæˆçš„æ­£æ ·æœ¬å¯¹ç›®å½•è®­ç»ƒ Word2Vec æ¨¡å‹ã€‚
    
    è´Ÿæ ·æœ¬åœ¨è®­ç»ƒæ—¶åŠ¨æ€ç”Ÿæˆï¼Œæ¯ä¸ª epoch ä½¿ç”¨ä¸åŒçš„è´Ÿæ ·æœ¬ã€‚
    """
    import time
    
    logger.info("=" * 60)
    logger.info("ä»ç¼“å­˜æ ·æœ¬è®­ç»ƒ Word2Vec")
    logger.info("=" * 60)
    
    total_start_time = time.time()
    
    # ========== æ­¥éª¤ 1: åŠ è½½å…ƒæ•°æ® ==========
    logger.info("[1/5] åŠ è½½å…ƒæ•°æ®...")
    step_start = time.time()
    
    meta_path = samples_dir / "meta.json"
    if not meta_path.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°é¢„ç”Ÿæˆæ ·æœ¬: {meta_path}")
    
    with open(meta_path, "r") as f:
        meta = json.load(f)
    
    vocab_size = meta["vocab_size"]
    num_chunks = meta["num_chunks"]
    total_pairs = meta["total_pairs"]
    
    logger.info("  â”œâ”€ è¯è¡¨è§„æ¨¡: %d", vocab_size)
    logger.info("  â”œâ”€ æ•°æ®å—æ•°: %d", num_chunks)
    logger.info("  â”œâ”€ æ€»æ­£æ ·æœ¬å¯¹: %d (%.1fM)", total_pairs, total_pairs / 1e6)
    logger.info("  â””â”€ è€—æ—¶: %.2f ç§’", time.time() - step_start)
    
    # ========== æ­¥éª¤ 2: åŠ è½½è¯é¢‘åˆ†å¸ƒ ==========
    logger.info("[2/5] åŠ è½½è¯é¢‘åˆ†å¸ƒ...")
    step_start = time.time()
    
    freq_path = samples_dir / "freq.npy"
    if freq_path.exists():
        freq = np.load(freq_path)
        logger.info("  â”œâ”€ ä»æ–‡ä»¶åŠ è½½: %s", freq_path.name)
    else:
        freq = np.ones(vocab_size, dtype=np.float64)
        logger.info("  â”œâ”€ ä½¿ç”¨é»˜è®¤å‡åŒ€åˆ†å¸ƒ")
    
    # ç¡®ä¿ freq å¤§å°ä¸ vocab_size ä¸€è‡´
    if len(freq) != vocab_size:
        logger.warning("  â”œâ”€ âš ï¸ è¯é¢‘æ•°ç»„å¤§å° (%d) ä¸è¯è¡¨å¤§å° (%d) ä¸ä¸€è‡´ï¼Œè¿›è¡Œæˆªæ–­/å¡«å……", len(freq), vocab_size)
        if len(freq) > vocab_size:
            freq = freq[:vocab_size]
        else:
            padded = np.ones(vocab_size, dtype=freq.dtype)
            padded[:len(freq)] = freq
            freq = padded
    
    neg_prob = _prepare_negative_sampling_probs(freq)
    logger.info("  â”œâ”€ è´Ÿé‡‡æ ·æ¦‚ç‡åˆ†å¸ƒå·²è®¡ç®— (å¤§å°: %d)", len(neg_prob))
    logger.info("  â””â”€ è€—æ—¶: %.2f ç§’", time.time() - step_start)
    
    # ========== æ­¥éª¤ 3: åˆå§‹åŒ–æ¨¡å‹ ==========
    logger.info("[3/5] åˆå§‹åŒ–æ¨¡å‹...")
    step_start = time.time()
    
    strategy = _get_strategy()
    batch_size = CONFIG.train.batch_size_word2vec
    num_ns = CONFIG.train.num_negative_samples
    
    logger.info("  â”œâ”€ åˆ†å¸ƒå¼ç­–ç•¥: %s", strategy.__class__.__name__)
    logger.info("  â”œâ”€ Batch Size: %d", batch_size)
    logger.info("  â”œâ”€ è´Ÿæ ·æœ¬æ•°: %d (GPU åŠ¨æ€é‡‡æ ·)", num_ns)
    
    # åˆ›å»º GPU è´Ÿé‡‡æ ·å‡½æ•°
    gpu_negative_sampler = _create_gpu_negative_sampler(neg_prob, num_ns, vocab_size)
    logger.info("  â”œâ”€ GPU è´Ÿé‡‡æ ·å™¨å·²åˆå§‹åŒ–")
    
    with strategy.scope():
        model = Word2Vec(vocab_size=vocab_size, embedding_dim=CONFIG.train.embedding_dim)
        loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)
        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
        
        if mixed_precision.global_policy().compute_dtype == "float16":
            optimizer = mixed_precision.LossScaleOptimizer(optimizer)
            logger.info("  â”œâ”€ æ··åˆç²¾åº¦: å·²å¯ç”¨ (float16)")
        
        model.compile(
            optimizer=optimizer,
            loss=loss_fn,
            metrics=["accuracy", positive_recall],
            jit_compile=CONFIG.train.enable_jit_compile
        )
    
    logger.info("  â”œâ”€ æ¨¡å‹å·²ç¼–è¯‘")
    logger.info("  â””â”€ è€—æ—¶: %.2f ç§’", time.time() - step_start)
    
    # æ‰“å°æ¨¡å‹å‚æ•°ç»Ÿè®¡
    _log_model_stats(model)
    
    # ========== æ­¥éª¤ 4: å¼€å§‹è®­ç»ƒ ==========
    logger.info("[4/5] å¼€å§‹è®­ç»ƒ...")
    
    global_epochs = CONFIG.train.global_epochs
    epochs_per_chunk = CONFIG.train.epochs_per_chunk
    
    logger.info("  â”œâ”€ å…¨å±€è½®æ•°: %d", global_epochs)
    logger.info("  â”œâ”€ æ¯å—è®­ç»ƒè½®æ•°: %d", epochs_per_chunk)
    logger.info("  â”œâ”€ æ€»è®­ç»ƒå•å…ƒ: %d (= %d Ã— %d)", global_epochs * num_chunks, global_epochs, num_chunks)
    
    # é¢„ä¼°è®­ç»ƒæ—¶é—´
    avg_pairs_per_chunk = total_pairs // num_chunks
    steps_per_chunk = avg_pairs_per_chunk // batch_size * epochs_per_chunk
    total_steps = steps_per_chunk * num_chunks * global_epochs
    logger.info("  â”œâ”€ é¢„ä¼°æ€»æ­¥æ•°: %d (%.1fM)", total_steps, total_steps / 1e6)
    
    lr_callback = ReduceLROnPlateau(
        monitor="loss",
        factor=0.7,
        patience=2,
        min_lr=1e-6,
        verbose=0,  # å…³é—­ keras çš„ verboseï¼Œç”¨æˆ‘ä»¬è‡ªå·±çš„æ—¥å¿—
    )
    
    # è·å–æ‰€æœ‰ chunk æ–‡ä»¶
    chunk_files = sorted(samples_dir.glob("chunk_*.npz"))
    num_chunks = len(chunk_files)
    
    # å…¨å±€è¿›åº¦æ¡
    global_bar = tqdm(
        total=global_epochs * num_chunks, 
        desc="ğŸš€ è®­ç»ƒæ€»è¿›åº¦",
        unit="chunk",
        position=0,
        leave=True,
    )
    
    training_start = time.time()
    chunk_times = []
    
    for global_epoch in range(global_epochs):
        epoch_start = time.time()
        logger.info("=" * 50)
        logger.info("å…¨å±€è½®æ¬¡ %d/%d å¼€å§‹", global_epoch + 1, global_epochs)
        logger.info("=" * 50)
        
        # æ¯è½®æ‰“ä¹±æ•°æ®å—é¡ºåº
        chunk_indices = np.random.permutation(num_chunks)
        
        epoch_loss_sum = 0.0
        epoch_acc_sum = 0.0
        
        for i, chunk_idx in enumerate(chunk_indices):
            chunk_start = time.time()
            chunk_path = chunk_files[chunk_idx]
            
            # ===== åŠ è½½æ•°æ® =====
            load_start = time.time()
            targets, contexts = _load_positive_pairs_from_disk(chunk_path)
            chunk_pairs = len(targets)
            load_time = time.time() - load_start
            
            if chunk_pairs == 0:
                global_bar.update(1)
                logger.warning("  âš ï¸ Chunk %d ä¸ºç©ºï¼Œè·³è¿‡", chunk_idx)
                continue
            
            # ===== æ„å»ºæ•°æ®ç®¡é“ =====
            pipeline_start = time.time()
            steps_per_epoch = max(1, chunk_pairs // batch_size)
            shuffle_buffer = min(CONFIG.train.shuffle_buffer_size, chunk_pairs)
            
            dataset = tf.data.Dataset.from_tensor_slices((targets, contexts))
            dataset = dataset.shuffle(shuffle_buffer)
            dataset = dataset.batch(batch_size, drop_remainder=True)
            dataset = dataset.map(
                gpu_negative_sampler,
                num_parallel_calls=tf.data.AUTOTUNE
            )
            dataset = dataset.repeat()
            dataset = dataset.prefetch(tf.data.AUTOTUNE)
            pipeline_time = time.time() - pipeline_start
            
            # ===== è®­ç»ƒ =====
            train_start = time.time()
            
            # ä½¿ç”¨ TqdmCallback æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
            chunk_bar = tqdm(
                total=steps_per_epoch * epochs_per_chunk,
                desc=f"  ğŸ“¦ Chunk {i+1}/{num_chunks}",
                unit="batch",
                position=1,
                leave=False,
                bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"
            )
            
            class ChunkProgressCallback(tf.keras.callbacks.Callback):
                def on_batch_end(self, batch, logs=None):
                    chunk_bar.update(1)
                    if logs:
                        chunk_bar.set_postfix({
                            "loss": f"{logs.get('loss', 0):.4f}",
                            "acc": f"{logs.get('accuracy', 0):.4f}",
                        })
            
            model.fit(
                dataset,
                epochs=epochs_per_chunk,
                steps_per_epoch=steps_per_epoch,
                verbose=0,
                callbacks=[lr_callback, ChunkProgressCallback()],
            )
            
            chunk_bar.close()
            train_time = time.time() - train_start
            
            # è·å–è®­ç»ƒç»“æœ
            current_loss = model.history.history.get("loss", [0])[-1] if model.history else 0
            current_acc = model.history.history.get("accuracy", [0])[-1] if model.history else 0
            current_recall = model.history.history.get("positive_recall", [0])[-1] if model.history else 0
            
            epoch_loss_sum += current_loss
            epoch_acc_sum += current_acc
            
            # é‡Šæ”¾å†…å­˜
            del targets, contexts, dataset
            gc.collect()
            
            chunk_total_time = time.time() - chunk_start
            chunk_times.append(chunk_total_time)
            
            # è®¡ç®—å‰©ä½™æ—¶é—´
            avg_chunk_time = np.mean(chunk_times)
            remaining_chunks = (global_epochs - global_epoch - 1) * num_chunks + (num_chunks - i - 1)
            eta_seconds = avg_chunk_time * remaining_chunks
            eta_str = f"{int(eta_seconds // 3600)}h {int((eta_seconds % 3600) // 60)}m"
            
            # æ›´æ–°å…¨å±€è¿›åº¦æ¡
            global_bar.update(1)
            global_bar.set_postfix({
                "epoch": f"{global_epoch+1}/{global_epochs}",
                "loss": f"{current_loss:.4f}",
                "acc": f"{current_acc:.3f}",
                "recall": f"{current_recall:.3f}",
                "ETA": eta_str,
            })
            
            # æ¯ 5 ä¸ª chunk è¾“å‡ºè¯¦ç»†æ—¥å¿—
            if (i + 1) % 5 == 0 or i == 0:
                logger.info(
                    "  Chunk %d/%d: loss=%.4f acc=%.3f recall=%.3f | "
                    "åŠ è½½=%.1fs ç®¡é“=%.1fs è®­ç»ƒ=%.1fs æ€»=%.1fs",
                    i + 1, num_chunks, current_loss, current_acc, current_recall,
                    load_time, pipeline_time, train_time, chunk_total_time
                )
        
        # è½®æ¬¡ç»“æŸç»Ÿè®¡
        epoch_time = time.time() - epoch_start
        avg_epoch_loss = epoch_loss_sum / num_chunks
        avg_epoch_acc = epoch_acc_sum / num_chunks
        
        logger.info("-" * 50)
        logger.info(
            "è½®æ¬¡ %d å®Œæˆ | å¹³å‡ Loss: %.4f | å¹³å‡ Acc: %.3f | è€—æ—¶: %.1f åˆ†é’Ÿ",
            global_epoch + 1, avg_epoch_loss, avg_epoch_acc, epoch_time / 60
        )
        logger.info("-" * 50)
    
    global_bar.close()
    
    training_time = time.time() - training_start
    logger.info("  â””â”€ è®­ç»ƒæ€»è€—æ—¶: %.1f åˆ†é’Ÿ (%.1f å°æ—¶)", training_time / 60, training_time / 3600)
    
    # ========== æ­¥éª¤ 5: å¯¼å‡ºæ¨¡å‹ ==========
    logger.info("[5/5] å¯¼å‡ºæ¨¡å‹æ–‡ä»¶...")
    step_start = time.time()
    
    tokens = _load_vocab_tokens(vocab_path, vocab_size)
    _export_all_formats(model, tokens, CONFIG.paths.artifacts_dir)
    
    logger.info("  â””â”€ è€—æ—¶: %.2f ç§’", time.time() - step_start)
    
    # ========== å®Œæˆ ==========
    total_time = time.time() - total_start_time
    logger.info("=" * 60)
    logger.info("âœ… è®­ç»ƒå®Œæˆ!")
    logger.info("  â”œâ”€ æ€»è€—æ—¶: %.1f åˆ†é’Ÿ (%.2f å°æ—¶)", total_time / 60, total_time / 3600)
    logger.info("  â”œâ”€ å¹³å‡æ¯ chunk: %.1f ç§’", np.mean(chunk_times) if chunk_times else 0)
    logger.info("  â””â”€ è¾“å‡ºç›®å½•: %s", CONFIG.paths.artifacts_dir)
    logger.info("=" * 60)
    
    return CONFIG.paths.vectors_path, CONFIG.paths.metadata_path


# ========== æµå¼è®­ç»ƒï¼ˆè¾¹ç”Ÿæˆè¾¹è®­ç»ƒï¼‰ ==========

def _generate_samples_for_chunk(
    sequences: np.ndarray,
    window_size: int,
    num_ns: int,
    neg_prob: np.ndarray,
    subsample_probs: Optional[np.ndarray],
    rng_seed: int,
) -> Tuple[List, List, List]:
    """ä¸ºä¸€ä¸ªåºåˆ—å—ç”Ÿæˆ skip-gram æ ·æœ¬ï¼ˆå«è´Ÿé‡‡æ ·ï¼‰ã€‚"""
    rng = np.random.default_rng(rng_seed)
    
    targets = []
    contexts = []
    labels = []
    
    for seq in sequences:
        valid_tokens = [t for t in seq if t > 1]
        
        if subsample_probs is not None:
            filtered = []
            for tok in valid_tokens:
                if tok < len(subsample_probs) and rng.random() < subsample_probs[tok]:
                    filtered.append(tok)
            valid_tokens = filtered
        
        seq_len = len(valid_tokens)
        if seq_len < 2:
            continue
        
        for i, target_word in enumerate(valid_tokens):
            actual_window = rng.integers(1, window_size + 1)
            left = max(0, i - actual_window)
            right = min(seq_len, i + actual_window + 1)
            
            for j in range(left, right):
                if j == i:
                    continue
                
                context_word = valid_tokens[j]
                negatives = rng.choice(len(neg_prob), size=num_ns, p=neg_prob, replace=True)
                
                context = np.concatenate(([context_word], negatives)).astype(np.int32)
                label = np.concatenate(([1], np.zeros(num_ns, dtype=np.float32)))
                
                targets.append(np.int32(target_word))
                contexts.append(context)
                labels.append(label)
    
    return targets, contexts, labels


def train_word2vec(
    sequences_path: Path,
    vocab_path: Path,
    vocab_limit: int,
    max_sequences: Optional[int] = None,
    use_cache: bool = False,
) -> Tuple[Path, Path]:
    """
    è®­ç»ƒ Word2Vec æ¨¡å‹ã€‚
    
    Args:
        sequences_path: åºåˆ—æ•°æ® CSV è·¯å¾„
        vocab_path: è¯æ±‡è¡¨è·¯å¾„
        vocab_limit: è¯è¡¨è§„æ¨¡ä¸Šé™
        max_sequences: å¯é€‰çš„åºåˆ—æ•°ä¸Šé™
        use_cache: æ˜¯å¦ä½¿ç”¨é¢„ç”Ÿæˆçš„æ ·æœ¬ç¼“å­˜
    
    Returns:
        (vectors_path, metadata_path)
    """
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç¼“å­˜
    samples_dir = CONFIG.paths.cache_dir / "samples"
    if use_cache or (samples_dir / "meta.json").exists():
        if (samples_dir / "meta.json").exists():
            logger.info("å‘ç°é¢„ç”Ÿæˆæ ·æœ¬ç¼“å­˜ï¼Œä½¿ç”¨ç¼“å­˜è®­ç»ƒ")
            return train_word2vec_from_cache(samples_dir, vocab_path)
        else:
            logger.warning("æœªæ‰¾åˆ°é¢„ç”Ÿæˆæ ·æœ¬ï¼Œåˆ‡æ¢åˆ°æµå¼è®­ç»ƒæ¨¡å¼")
    
    logger.info("========== å¼€å§‹ Word2Vec æµå¼è®­ç»ƒ ==========")
    logger.info("è¯»å–åºåˆ—æ–‡ä»¶: %s", sequences_path)
    
    # ========== 1. è¯»å–åºåˆ—æ•°æ® ==========
    df = pd.read_csv(sequences_path)
    sequences = df.values.astype(np.int32)
    del df
    gc.collect()
    
    if max_sequences is not None:
        sequences = sequences[:max_sequences]
    
    num_sequences = len(sequences)
    seq_len = sequences.shape[1]
    logger.info("åºåˆ—æ•°: %d, åºåˆ—é•¿åº¦: %d", num_sequences, seq_len)
    
    # ========== 2. è®¡ç®—è¯é¢‘å’Œé‡‡æ ·åˆ†å¸ƒ ==========
    flat = sequences.reshape(-1)
    flat = flat[flat > 1]
    vocab_size = int(flat.max()) + 1 if flat.size else vocab_limit
    vocab_size = min(vocab_size, vocab_limit)
    
    freq = np.bincount(flat, minlength=vocab_size)
    del flat
    gc.collect()
    
    logger.info("è¯è¡¨è§„æ¨¡: %d, æœ‰æ•ˆ token æ•°: %d", vocab_size, (freq > 0).sum())
    
    subsample_probs = None
    if CONFIG.train.subsample_t > 0:
        subsample_probs = _compute_subsample_probs(freq, CONFIG.train.subsample_t)
    
    neg_prob = _prepare_negative_sampling_probs(freq)
    
    # ========== 3. è®¡ç®—åˆ†å—ç­–ç•¥ ==========
    window_size = CONFIG.train.window_size
    num_ns = CONFIG.train.num_negative_samples
    batch_size = CONFIG.train.batch_size_word2vec
    max_memory_gb = CONFIG.train.max_memory_gb
    
    samples_per_seq = seq_len * window_size * 0.5
    bytes_per_sample = (1 + num_ns + 1) * 4 + (num_ns + 1) * 4
    memory_per_seq = samples_per_seq * bytes_per_sample
    
    seqs_per_chunk = int((max_memory_gb * 1024**3) / memory_per_seq)
    seqs_per_chunk = max(1000, min(seqs_per_chunk, num_sequences))
    
    num_chunks = (num_sequences + seqs_per_chunk - 1) // seqs_per_chunk
    
    logger.info("========== æµå¼è®­ç»ƒé…ç½® ==========")
    logger.info("ç›®æ ‡å†…å­˜é™åˆ¶: %.1f GB", max_memory_gb)
    logger.info("æ¯å—åºåˆ—æ•°: %d", seqs_per_chunk)
    logger.info("æ•°æ®å—æ€»æ•°: %d", num_chunks)
    logger.info("è´Ÿæ ·æœ¬æ•°: %d", num_ns)
    
    # ========== 4. åˆå§‹åŒ–æ¨¡å‹ ==========
    strategy = _get_strategy()
    logger.info("åˆ†å¸ƒå¼ç­–ç•¥: %s", strategy.__class__.__name__)
    
    with strategy.scope():
        model = Word2Vec(vocab_size=vocab_size, embedding_dim=CONFIG.train.embedding_dim)
        loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)
        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
        
        if mixed_precision.global_policy().compute_dtype == "float16":
            optimizer = mixed_precision.LossScaleOptimizer(optimizer)
        
        model.compile(
            optimizer=optimizer,
            loss=loss_fn,
            metrics=["accuracy", positive_recall],
            jit_compile=CONFIG.train.enable_jit_compile
        )
    
    # æ‰“å°æ¨¡å‹å‚æ•°ç»Ÿè®¡
    _log_model_stats(model)
    
    # ========== 5. åˆ†å—æµå¼è®­ç»ƒ ==========
    global_epochs = CONFIG.train.global_epochs
    epochs_per_chunk = CONFIG.train.epochs_per_chunk
    
    logger.info("å…¨å±€è½®æ•°: %d, æ¯å—è®­ç»ƒè½®æ•°: %d", global_epochs, epochs_per_chunk)
    
    lr_callback = ReduceLROnPlateau(
        monitor="loss",
        factor=0.7,
        patience=2,
        min_lr=1e-6,
        verbose=1,
    )
    
    global_bar = tqdm(total=global_epochs * num_chunks, desc="æµå¼è®­ç»ƒæ€»è¿›åº¦")
    
    for global_epoch in range(global_epochs):
        logger.info("---------- å…¨å±€è½®æ¬¡ %d/%d ----------", global_epoch + 1, global_epochs)
        
        chunk_indices = np.random.permutation(num_chunks)
        
        for chunk_idx in chunk_indices:
            start_idx = chunk_idx * seqs_per_chunk
            end_idx = min(start_idx + seqs_per_chunk, num_sequences)
            chunk_sequences = sequences[start_idx:end_idx]
            
            # ç”Ÿæˆæ ·æœ¬
            targets, contexts, labels = _generate_samples_for_chunk(
                chunk_sequences,
                window_size,
                num_ns,
                neg_prob,
                subsample_probs,
                rng_seed=CONFIG.random_seed + global_epoch * 1000 + chunk_idx,
            )
            
            if len(targets) == 0:
                global_bar.update(1)
                continue
            
            targets_arr = np.array(targets, dtype=np.int32)
            contexts_arr = np.stack(contexts).astype(np.int32)
            labels_arr = np.stack(labels).astype(np.float32)
            
            del targets, contexts, labels
            gc.collect()
            
            chunk_samples = len(targets_arr)
            steps_per_epoch = max(1, chunk_samples // batch_size)
            shuffle_buffer = min(CONFIG.train.shuffle_buffer_size, chunk_samples)
            
            dataset = tf.data.Dataset.from_tensor_slices(
                ((targets_arr, contexts_arr), labels_arr)
            )
            dataset = dataset.shuffle(shuffle_buffer)
            dataset = dataset.batch(batch_size, drop_remainder=True)
            dataset = dataset.repeat()
            dataset = dataset.prefetch(tf.data.AUTOTUNE)
            
            model.fit(
                dataset,
                epochs=epochs_per_chunk,
                steps_per_epoch=steps_per_epoch,
                verbose=0,
                callbacks=[lr_callback],
            )
            
            del targets_arr, contexts_arr, labels_arr, dataset
            gc.collect()
            
            current_loss = model.history.history.get("loss", [0])[-1] if model.history else 0
            current_acc = model.history.history.get("accuracy", [0])[-1] if model.history else 0
            global_bar.update(1)
            global_bar.set_postfix({
                "epoch": f"{global_epoch+1}/{global_epochs}",
                "loss": f"{current_loss:.4f}",
                "acc": f"{current_acc:.4f}",
            })
    
    global_bar.close()
    
    # ========== 6. å¯¼å‡ºæ‰€æœ‰æ ¼å¼ ==========
    logger.info("========== å¯¼å‡ºæ¨¡å‹æ–‡ä»¶ ==========")
    tokens = _load_vocab_tokens(vocab_path, vocab_size)
    _export_all_formats(model, tokens, CONFIG.paths.artifacts_dir)
    
    logger.info("========== Word2Vec æµå¼è®­ç»ƒå®Œæˆ ==========")
    return CONFIG.paths.vectors_path, CONFIG.paths.metadata_path
