"""
æ¨¡å‹å¯¼å‡ºæ¨¡å—

æä¾›å¤šç§æ ¼å¼çš„å¯¼å‡ºåŠŸèƒ½ï¼š
1. TSV æ ¼å¼ - TensorFlow Embedding Projector å…¼å®¹
2. ONNX æ ¼å¼ - åœ¨çº¿æ¨ç†éƒ¨ç½²
3. JSON æ ¼å¼ - ç½‘é¡µå¯è§†åŒ–
4. èšç±» JSON - äº¤äº’å¼èšç±»å¯è§†åŒ–
5. æ¨èç³»ç»Ÿé…ç½® - ç½‘é¡µéƒ¨ç½²é…ç½®

ä½¿ç”¨æ–¹æ³•:
    python -m imdb_word2vec.cli export
"""
from __future__ import annotations

import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from tqdm import tqdm

from .config import CONFIG
from .logging_utils import setup_logging


logger = setup_logging(CONFIG.paths.logs_dir)


# ========== åŸºç¡€å¯¼å‡ºå‡½æ•° ==========

def export_tsv(
    weights: np.ndarray,
    tokens: List[str],
    vectors_path: Path,
    metadata_path: Path,
) -> None:
    """å¯¼å‡º TSV æ ¼å¼ï¼ˆTensorFlow Embedding Projector å…¼å®¹ï¼‰ã€‚"""
    vectors_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(vectors_path, "w") as out_v, open(metadata_path, "w") as out_m:
        for idx, token in enumerate(tokens):
            if idx == 0:  # è·³è¿‡ PAD
                continue
            vec = weights[idx]
            out_v.write("\t".join([str(x) for x in vec]) + "\n")
            out_m.write(token + "\n")
    
    logger.info("TSV å¯¼å‡º: %s, %s", vectors_path, metadata_path)


def export_onnx(
    vocab_size: int,
    embedding_dim: int,
    weights: np.ndarray,
    output_path: Path,
) -> None:
    """
    å¯¼å‡º ONNX æ ¼å¼ï¼ˆåœ¨çº¿æ¨ç†ï¼‰ã€‚
    
    ONNX æ¨¡å‹è¾“å…¥: token_ids (int64, shape: [batch_size])
    ONNX æ¨¡å‹è¾“å‡º: embeddings (float32, shape: [batch_size, embedding_dim])
    """
    try:
        import onnx
        from onnx import numpy_helper, TensorProto
        from onnx.helper import make_model, make_node, make_graph, make_tensor_value_info
    except ImportError:
        logger.warning("ONNX æœªå®‰è£…ï¼Œè·³è¿‡ ONNX å¯¼å‡ºã€‚å®‰è£…: pip install onnx")
        return
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»º Embedding æŸ¥æ‰¾è¡¨ï¼ˆä½¿ç”¨ Gather æ“ä½œï¼‰
    # è¾“å…¥: token_ids
    # è¾“å‡º: embeddings
    
    # å®šä¹‰è¾“å…¥
    input_ids = make_tensor_value_info("token_ids", TensorProto.INT64, [None])
    
    # å®šä¹‰è¾“å‡º
    output_embeddings = make_tensor_value_info(
        "embeddings", TensorProto.FLOAT, [None, embedding_dim]
    )
    
    # åˆ›å»ºæƒé‡å¸¸é‡
    embedding_weights = numpy_helper.from_array(
        weights.astype(np.float32), name="embedding_weights"
    )
    
    # åˆ›å»º Gather èŠ‚ç‚¹ï¼ˆå®ç° embedding lookupï¼‰
    gather_node = make_node(
        "Gather",
        inputs=["embedding_weights", "token_ids"],
        outputs=["embeddings"],
        axis=0,
    )
    
    # åˆ›å»ºå›¾
    graph = make_graph(
        nodes=[gather_node],
        name="Word2VecEmbedding",
        inputs=[input_ids],
        outputs=[output_embeddings],
        initializer=[embedding_weights],
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = make_model(graph, opset_imports=[onnx.helper.make_opsetid("", 13)])
    model.ir_version = 8
    
    # éªŒè¯å¹¶ä¿å­˜
    onnx.checker.check_model(model)
    onnx.save(model, str(output_path))
    
    logger.info("ONNX å¯¼å‡º: %s (%.2f MB)", output_path, output_path.stat().st_size / (1024**2))


def export_json_embeddings(
    weights: np.ndarray,
    tokens: List[str],
    output_path: Path,
    max_tokens: int = 50000,
) -> None:
    """
    å¯¼å‡º JSON æ ¼å¼åµŒå…¥ï¼ˆç½‘é¡µå¯è§†åŒ–ï¼‰ã€‚
    
    æ ¼å¼: {
        "tokens": ["MOV_tt001", "ACT_nm002", ...],
        "embeddings": [[0.1, 0.2, ...], [0.3, 0.4, ...], ...],
        "metadata": {
            "vocab_size": 50000,
            "embedding_dim": 128,
            "entity_types": {"MOV": 10000, "ACT": 5000, ...}
        }
    }
    """
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # ç»Ÿè®¡å®ä½“ç±»å‹
    entity_types: Dict[str, int] = {}
    valid_indices = []
    
    for idx, token in enumerate(tokens):
        if idx == 0:  # è·³è¿‡ PAD
            continue
        if idx >= max_tokens:
            break
        
        # æå–å‰ç¼€
        prefix = token.split("_")[0] if "_" in token else "OTHER"
        entity_types[prefix] = entity_types.get(prefix, 0) + 1
        valid_indices.append(idx)
    
    # æ„å»ºè¾“å‡º
    output_tokens = [tokens[i] for i in valid_indices]
    output_embeddings = weights[valid_indices].tolist()
    
    data = {
        "tokens": output_tokens,
        "embeddings": output_embeddings,
        "metadata": {
            "vocab_size": len(output_tokens),
            "embedding_dim": weights.shape[1],
            "entity_types": entity_types,
        }
    }
    
    with open(output_path, "w") as f:
        json.dump(data, f)
    
    logger.info("JSON åµŒå…¥å¯¼å‡º: %s (%.2f MB)", output_path, output_path.stat().st_size / (1024**2))


def export_clustering_visualization(
    weights: np.ndarray,
    tokens: List[str],
    output_path: Path,
    n_samples: int = 5000,
    n_clusters: int = 20,
) -> None:
    """
    å¯¼å‡ºäº¤äº’å¼èšç±»å¯è§†åŒ–æ•°æ®ï¼ˆt-SNE é™ç»´ + K-Means èšç±»ï¼‰ã€‚
    
    è¾“å‡º JSON æ ¼å¼ï¼Œå¯ç›´æ¥ç”¨äº D3.js / Plotly ç­‰ç½‘é¡µå¯è§†åŒ–åº“ã€‚
    """
    try:
        from sklearn.manifold import TSNE
        from sklearn.cluster import KMeans
    except ImportError:
        logger.warning("scikit-learn æœªå®‰è£…ï¼Œè·³è¿‡èšç±»å¯è§†åŒ–å¯¼å‡º")
        return
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # é‡‡æ ·
    valid_indices = [i for i in range(1, len(tokens)) if i < len(weights)]
    if len(valid_indices) > n_samples:
        sample_indices = np.random.choice(valid_indices, n_samples, replace=False)
    else:
        sample_indices = np.array(valid_indices)
    
    sample_tokens = [tokens[i] for i in sample_indices]
    sample_embeddings = weights[sample_indices]
    
    logger.info("t-SNE é™ç»´ä¸­... (%d æ ·æœ¬)", len(sample_indices))
    
    # t-SNE é™ç»´åˆ° 2D
    tsne = TSNE(n_components=2, perplexity=30, random_state=42, n_iter=1000)
    coords_2d = tsne.fit_transform(sample_embeddings)
    
    # K-Means èšç±»
    logger.info("K-Means èšç±»ä¸­... (%d ç°‡)", n_clusters)
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(sample_embeddings)
    
    # æå–å®ä½“ç±»å‹
    entity_types = []
    for token in sample_tokens:
        if "_" in token:
            prefix = token.split("_")[0]
        else:
            prefix = "OTHER"
        entity_types.append(prefix)
    
    # æ„å»ºè¾“å‡º
    points = []
    for i in range(len(sample_tokens)):
        points.append({
            "token": sample_tokens[i],
            "x": float(coords_2d[i, 0]),
            "y": float(coords_2d[i, 1]),
            "cluster": int(cluster_labels[i]),
            "type": entity_types[i],
        })
    
    # èšç±»ä¸­å¿ƒ
    cluster_centers = []
    for c in range(n_clusters):
        cluster_points = [p for p in points if p["cluster"] == c]
        if cluster_points:
            center_x = np.mean([p["x"] for p in cluster_points])
            center_y = np.mean([p["y"] for p in cluster_points])
            
            # æ‰¾å‡ºè¯¥ç°‡ä¸­æœ€å¸¸è§çš„å®ä½“ç±»å‹
            types_in_cluster = [p["type"] for p in cluster_points]
            most_common_type = max(set(types_in_cluster), key=types_in_cluster.count)
            
            cluster_centers.append({
                "cluster_id": c,
                "center_x": float(center_x),
                "center_y": float(center_y),
                "size": len(cluster_points),
                "dominant_type": most_common_type,
            })
    
    data = {
        "points": points,
        "clusters": cluster_centers,
        "metadata": {
            "n_samples": len(points),
            "n_clusters": n_clusters,
            "embedding_dim": weights.shape[1],
        }
    }
    
    with open(output_path, "w") as f:
        json.dump(data, f)
    
    logger.info("èšç±»å¯è§†åŒ–å¯¼å‡º: %s (%.2f MB)", output_path, output_path.stat().st_size / (1024**2))


def export_recommendation_config(
    weights: np.ndarray,
    tokens: List[str],
    output_dir: Path,
) -> None:
    """
    å¯¼å‡ºæ¨èç³»ç»Ÿé…ç½®æ–‡ä»¶ã€‚
    
    åŒ…å«ï¼š
    1. token_to_id.json - Token åˆ° ID çš„æ˜ å°„
    2. id_to_token.json - ID åˆ° Token çš„æ˜ å°„
    3. entity_index.json - æŒ‰å®ä½“ç±»å‹åˆ†ç±»çš„ç´¢å¼•
    4. config.json - æ¨èç³»ç»Ÿé…ç½®
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Token <-> ID æ˜ å°„
    token_to_id = {token: idx for idx, token in enumerate(tokens) if idx > 0}
    id_to_token = {idx: token for idx, token in enumerate(tokens) if idx > 0}
    
    with open(output_dir / "token_to_id.json", "w") as f:
        json.dump(token_to_id, f)
    
    with open(output_dir / "id_to_token.json", "w") as f:
        json.dump(id_to_token, f)
    
    # æŒ‰å®ä½“ç±»å‹åˆ†ç±»
    entity_index: Dict[str, List[str]] = {}
    for token in tokens:
        if "_" in token:
            prefix = token.split("_")[0]
        else:
            prefix = "OTHER"
        
        if prefix not in entity_index:
            entity_index[prefix] = []
        entity_index[prefix].append(token)
    
    with open(output_dir / "entity_index.json", "w") as f:
        json.dump(entity_index, f)
    
    # æ¨èç³»ç»Ÿé…ç½®
    config = {
        "vocab_size": len(tokens),
        "embedding_dim": weights.shape[1],
        "entity_types": {k: len(v) for k, v in entity_index.items()},
        "similarity_metric": "cosine",
        "top_k_default": 10,
        "files": {
            "onnx_model": "word2vec.onnx",
            "embeddings_json": "embeddings.json",
            "clustering_json": "clustering.json",
            "token_to_id": "token_to_id.json",
            "id_to_token": "id_to_token.json",
            "entity_index": "entity_index.json",
        }
    }
    
    with open(output_dir / "config.json", "w") as f:
        json.dump(config, f, indent=2)
    
    logger.info("æ¨èç³»ç»Ÿé…ç½®å¯¼å‡º: %s", output_dir)


def export_html_visualization(
    output_path: Path,
    clustering_json_path: str = "clustering.json",
) -> None:
    """
    å¯¼å‡ºäº¤äº’å¼ HTML å¯è§†åŒ–é¡µé¢ã€‚
    
    ä½¿ç”¨ Plotly.js åˆ›å»ºå¯äº¤äº’çš„èšç±»æ•£ç‚¹å›¾ã€‚
    """
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    html_content = '''<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Word2Vec åµŒå…¥å¯è§†åŒ–</title>
    <script src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
            min-height: 100vh;
            color: #e0e0e0;
        }
        .container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 20px;
        }
        h1 {
            text-align: center;
            padding: 20px;
            background: linear-gradient(90deg, #00d4ff, #7b2cbf);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            font-size: 2.5em;
            margin-bottom: 20px;
        }
        .stats {
            display: flex;
            justify-content: center;
            gap: 30px;
            margin-bottom: 20px;
            flex-wrap: wrap;
        }
        .stat-card {
            background: rgba(255,255,255,0.1);
            border-radius: 12px;
            padding: 15px 25px;
            text-align: center;
            backdrop-filter: blur(10px);
            border: 1px solid rgba(255,255,255,0.1);
        }
        .stat-value {
            font-size: 1.8em;
            font-weight: bold;
            color: #00d4ff;
        }
        .stat-label {
            font-size: 0.9em;
            color: #888;
            margin-top: 5px;
        }
        #chart {
            background: rgba(255,255,255,0.05);
            border-radius: 12px;
            padding: 10px;
            border: 1px solid rgba(255,255,255,0.1);
        }
        .legend {
            display: flex;
            justify-content: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 20px;
        }
        .legend-item {
            display: flex;
            align-items: center;
            gap: 8px;
            background: rgba(255,255,255,0.1);
            padding: 8px 15px;
            border-radius: 20px;
            font-size: 0.9em;
        }
        .legend-color {
            width: 12px;
            height: 12px;
            border-radius: 50%;
        }
        .search-box {
            max-width: 400px;
            margin: 20px auto;
        }
        .search-box input {
            width: 100%;
            padding: 12px 20px;
            border: none;
            border-radius: 25px;
            background: rgba(255,255,255,0.1);
            color: #fff;
            font-size: 1em;
            outline: none;
        }
        .search-box input::placeholder {
            color: #888;
        }
        .info {
            text-align: center;
            margin-top: 20px;
            color: #888;
            font-size: 0.9em;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ¬ Word2Vec åµŒå…¥å¯è§†åŒ–</h1>
        
        <div class="stats" id="stats"></div>
        
        <div class="search-box">
            <input type="text" id="search" placeholder="æœç´¢ Token (å¦‚: MOV_tt0111161)">
        </div>
        
        <div id="chart"></div>
        
        <div class="legend" id="legend"></div>
        
        <p class="info">
            ä½¿ç”¨ t-SNE é™ç»´ + K-Means èšç±» | 
            ç‚¹å‡»æ•°æ®ç‚¹æŸ¥çœ‹è¯¦æƒ… | 
            æ»šè½®ç¼©æ”¾ï¼Œæ‹–æ‹½å¹³ç§»
        </p>
    </div>
    
    <script>
        const COLORS = {
            'MOV': '#ff6b6b',
            'ACT': '#4ecdc4',
            'DIR': '#45b7d1',
            'GEN': '#96ceb4',
            'PER': '#ffeaa7',
            'RAT': '#dfe6e9',
            'ERA': '#a29bfe',
            'TYP': '#fd79a8',
            'OTHER': '#b2bec3'
        };
        
        const TYPE_NAMES = {
            'MOV': 'ç”µå½±',
            'ACT': 'æ¼”å‘˜',
            'DIR': 'å¯¼æ¼”',
            'GEN': 'ç±»å‹',
            'PER': 'äººå‘˜',
            'RAT': 'è¯„åˆ†',
            'ERA': 'å¹´ä»£',
            'TYP': 'ä½œå“ç±»å‹',
            'OTHER': 'å…¶ä»–'
        };
        
        fetch('CLUSTERING_JSON_PATH')
            .then(res => res.json())
            .then(data => {
                // ç»Ÿè®¡
                const stats = document.getElementById('stats');
                const typeCounts = {};
                data.points.forEach(p => {
                    typeCounts[p.type] = (typeCounts[p.type] || 0) + 1;
                });
                
                stats.innerHTML = `
                    <div class="stat-card">
                        <div class="stat-value">${data.points.length.toLocaleString()}</div>
                        <div class="stat-label">æ ·æœ¬æ•°</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-value">${data.clusters.length}</div>
                        <div class="stat-label">èšç±»æ•°</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-value">${Object.keys(typeCounts).length}</div>
                        <div class="stat-label">å®ä½“ç±»å‹</div>
                    </div>
                `;
                
                // å›¾ä¾‹
                const legend = document.getElementById('legend');
                legend.innerHTML = Object.entries(typeCounts)
                    .sort((a, b) => b[1] - a[1])
                    .map(([type, count]) => `
                        <div class="legend-item">
                            <div class="legend-color" style="background: ${COLORS[type] || COLORS.OTHER}"></div>
                            <span>${TYPE_NAMES[type] || type} (${count})</span>
                        </div>
                    `).join('');
                
                // æŒ‰ç±»å‹åˆ†ç»„
                const traces = [];
                const groupedByType = {};
                data.points.forEach(p => {
                    if (!groupedByType[p.type]) {
                        groupedByType[p.type] = { x: [], y: [], text: [], cluster: [] };
                    }
                    groupedByType[p.type].x.push(p.x);
                    groupedByType[p.type].y.push(p.y);
                    groupedByType[p.type].text.push(p.token);
                    groupedByType[p.type].cluster.push(p.cluster);
                });
                
                Object.entries(groupedByType).forEach(([type, points]) => {
                    traces.push({
                        x: points.x,
                        y: points.y,
                        text: points.text,
                        customdata: points.cluster,
                        mode: 'markers',
                        type: 'scatter',
                        name: TYPE_NAMES[type] || type,
                        marker: {
                            color: COLORS[type] || COLORS.OTHER,
                            size: 6,
                            opacity: 0.7,
                        },
                        hovertemplate: '<b>%{text}</b><br>èšç±»: %{customdata}<extra></extra>'
                    });
                });
                
                const layout = {
                    paper_bgcolor: 'rgba(0,0,0,0)',
                    plot_bgcolor: 'rgba(0,0,0,0)',
                    font: { color: '#e0e0e0' },
                    xaxis: {
                        showgrid: true,
                        gridcolor: 'rgba(255,255,255,0.1)',
                        zeroline: false,
                        title: 't-SNE ç»´åº¦ 1'
                    },
                    yaxis: {
                        showgrid: true,
                        gridcolor: 'rgba(255,255,255,0.1)',
                        zeroline: false,
                        title: 't-SNE ç»´åº¦ 2'
                    },
                    legend: {
                        x: 1,
                        y: 1,
                        bgcolor: 'rgba(0,0,0,0.5)',
                    },
                    hovermode: 'closest',
                    margin: { l: 50, r: 50, t: 20, b: 50 }
                };
                
                Plotly.newPlot('chart', traces, layout, { responsive: true });
                
                // æœç´¢åŠŸèƒ½
                const searchInput = document.getElementById('search');
                searchInput.addEventListener('input', (e) => {
                    const query = e.target.value.toLowerCase();
                    if (!query) {
                        Plotly.restyle('chart', { 'marker.opacity': 0.7 });
                        return;
                    }
                    
                    traces.forEach((trace, i) => {
                        const opacities = trace.text.map(t => 
                            t.toLowerCase().includes(query) ? 1 : 0.1
                        );
                        Plotly.restyle('chart', { 'marker.opacity': [opacities] }, [i]);
                    });
                });
            })
            .catch(err => {
                document.getElementById('chart').innerHTML = 
                    '<p style="text-align:center;padding:50px;color:#ff6b6b;">åŠ è½½æ•°æ®å¤±è´¥: ' + err + '</p>';
            });
    </script>
</body>
</html>'''.replace('CLUSTERING_JSON_PATH', clustering_json_path)
    
    with open(output_path, "w") as f:
        f.write(html_content)
    
    logger.info("HTML å¯è§†åŒ–å¯¼å‡º: %s", output_path)


# ========== ä¸»å¯¼å‡ºå‡½æ•° ==========

def export_all(
    weights: np.ndarray,
    tokens: List[str],
    output_dir: Optional[Path] = None,
) -> Dict[str, Path]:
    """
    å¯¼å‡ºæ‰€æœ‰æ ¼å¼çš„æ–‡ä»¶ã€‚
    
    Returns:
        å¯¼å‡ºæ–‡ä»¶è·¯å¾„å­—å…¸
    """
    if output_dir is None:
        output_dir = CONFIG.paths.artifacts_dir
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info("========== å¼€å§‹å¯¼å‡ºæ‰€æœ‰æ ¼å¼ ==========")
    
    exported_files = {}
    
    # 1. TSV æ ¼å¼ï¼ˆåŸæœ‰ï¼‰
    vectors_path = output_dir / "vectors.tsv"
    metadata_path = output_dir / "metadata.tsv"
    export_tsv(weights, tokens, vectors_path, metadata_path)
    exported_files["vectors_tsv"] = vectors_path
    exported_files["metadata_tsv"] = metadata_path
    
    # 2. ONNX æ ¼å¼ï¼ˆåœ¨çº¿æ¨ç†ï¼‰
    onnx_path = output_dir / "word2vec.onnx"
    export_onnx(len(tokens), weights.shape[1], weights, onnx_path)
    if onnx_path.exists():
        exported_files["onnx"] = onnx_path
    
    # 3. JSON åµŒå…¥ï¼ˆç½‘é¡µå¯è§†åŒ–ï¼‰
    embeddings_json_path = output_dir / "embeddings.json"
    export_json_embeddings(weights, tokens, embeddings_json_path)
    exported_files["embeddings_json"] = embeddings_json_path
    
    # 4. èšç±»å¯è§†åŒ– JSON
    clustering_json_path = output_dir / "clustering.json"
    export_clustering_visualization(weights, tokens, clustering_json_path)
    exported_files["clustering_json"] = clustering_json_path
    
    # 5. æ¨èç³»ç»Ÿé…ç½®
    recsys_dir = output_dir / "recsys"
    export_recommendation_config(weights, tokens, recsys_dir)
    exported_files["recsys_config"] = recsys_dir / "config.json"
    
    # 6. HTML å¯è§†åŒ–é¡µé¢
    html_path = output_dir / "visualization.html"
    export_html_visualization(html_path, "clustering.json")
    exported_files["visualization_html"] = html_path
    
    # 7. ä¿å­˜åŸå§‹æƒé‡ï¼ˆNumPy æ ¼å¼ï¼‰
    weights_path = output_dir / "embeddings.npy"
    np.save(weights_path, weights)
    exported_files["embeddings_npy"] = weights_path
    
    logger.info("========== å¯¼å‡ºå®Œæˆ ==========")
    logger.info("å¯¼å‡ºç›®å½•: %s", output_dir)
    
    # æ‰“å°æ–‡ä»¶æ¸…å•
    total_size = 0
    for name, path in exported_files.items():
        if path.exists():
            size = path.stat().st_size / (1024**2)
            total_size += size
            logger.info("  - %s: %.2f MB", path.name, size)
    
    logger.info("æ€»å¤§å°: %.2f MB", total_size)
    
    return exported_files

