"""
å¯¼å‡ºå·¥å…·é¡µé¢ (Export Tools)
==========================

æä¾›æ•°æ®æ–‡ä»¶ä¸‹è½½å’Œ TensorFlow Projector å¯¼å…¥æŒ‡å—ã€‚

åŠŸèƒ½:
- ä¸‹è½½å„ç§æ ¼å¼çš„æ•°æ®æ–‡ä»¶
- TensorFlow Embedding Projector å¯¼å…¥æ•™ç¨‹
- æ–‡ä»¶æ ¼å¼è¯´æ˜

ä½¿ç”¨çš„æ•°æ®æ–‡ä»¶:
- vectors.tsv: TF Projector å…¼å®¹çš„å‘é‡æ–‡ä»¶
- metadata.tsv: Token å…ƒæ•°æ®
- å…¶ä»–æ‰€æœ‰å¯¼å‡ºæ–‡ä»¶
"""
import streamlit as st
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from config import AppConfig, DataFiles
from utils.data_loader import get_data_files_info
from components.sidebar import render_page_header


# =============================================================================
# é¡µé¢é…ç½®
# =============================================================================

st.set_page_config(
    page_title="Export - " + AppConfig.APP_TITLE,
    page_icon="ğŸ’¾",
    layout=AppConfig.LAYOUT,
)


# =============================================================================
# é¡µé¢æ ‡é¢˜
# =============================================================================

render_page_header(
    title="å¯¼å‡ºå·¥å…·",
    description="ä¸‹è½½æ•°æ®æ–‡ä»¶ï¼Œå¹¶äº†è§£å¦‚ä½•åœ¨å…¶ä»–å·¥å…·ä¸­ä½¿ç”¨è¿™äº›æ•°æ®ã€‚",
    icon="ğŸ’¾",
)


# =============================================================================
# ä¾§è¾¹æ 
# =============================================================================

with st.sidebar:
    st.markdown("### ğŸ’¾ å¯¼å‡ºå·¥å…·")
    st.markdown("---")
    
    st.markdown("#### ğŸ“ æ–‡ä»¶æ¦‚è§ˆ")
    
    files_info = get_data_files_info()
    total_size = sum(f["size_mb"] for f in files_info if f["exists"])
    
    st.metric("æ€»æ–‡ä»¶æ•°", len(files_info))
    st.metric("æ€»å¤§å°", f"{total_size:.1f} MB")
    
    st.markdown("---")
    
    st.markdown("#### ğŸ”— ç›¸å…³é“¾æ¥")
    st.markdown("[TensorFlow Projector](https://projector.tensorflow.org/)")
    st.markdown("[ONNX Runtime](https://onnxruntime.ai/)")


# =============================================================================
# æ–‡ä»¶åˆ—è¡¨
# =============================================================================

st.markdown("## ğŸ“ å¯ä¸‹è½½æ–‡ä»¶")

files_info = get_data_files_info()

# æŒ‰ç±»å‹åˆ†ç»„
file_categories = {
    "åµŒå…¥æ•°æ®": ["embeddings.npy", "embeddings.json"],
    "å¯è§†åŒ–æ•°æ®": ["clustering.json", "embedding_tsne.png"],
    "TensorFlow Projector": ["vectors.tsv", "metadata.tsv"],
    "ONNX æ¨¡å‹": ["word2vec.onnx"],
    "æ¨èç³»ç»Ÿé…ç½®": ["config.json", "token_to_id.json", "id_to_token.json", "entity_index.json"],
    "å…¶ä»–": ["visualization.html"],
}

for category, file_names in file_categories.items():
    st.markdown(f"### {category}")
    
    cols = st.columns(len(file_names))
    
    for col, file_name in zip(cols, file_names):
        with col:
            # æŸ¥æ‰¾æ–‡ä»¶ä¿¡æ¯
            file_info = next((f for f in files_info if f["name"] == file_name), None)
            
            if file_info and file_info["exists"]:
                st.markdown(f"**{file_name}**")
                st.caption(f"å¤§å°: {file_info['size_mb']:.2f} MB")
                
                # è¯»å–æ–‡ä»¶å†…å®¹ç”¨äºä¸‹è½½
                file_path = Path(file_info["path"])
                
                # æ ¹æ®æ–‡ä»¶ç±»å‹å†³å®šè¯»å–æ–¹å¼
                if file_name.endswith((".json", ".tsv", ".html")):
                    try:
                        with open(file_path, "r", encoding="utf-8") as f:
                            file_content = f.read()
                        
                        st.download_button(
                            label=f"ğŸ“¥ ä¸‹è½½",
                            data=file_content,
                            file_name=file_name,
                            mime="application/octet-stream",
                            key=f"download_{file_name}",
                        )
                    except Exception as e:
                        st.warning(f"è¯»å–å¤±è´¥: {e}")
                
                elif file_name.endswith((".npy", ".onnx", ".png")):
                    try:
                        with open(file_path, "rb") as f:
                            file_content = f.read()
                        
                        st.download_button(
                            label=f"ğŸ“¥ ä¸‹è½½",
                            data=file_content,
                            file_name=file_name,
                            mime="application/octet-stream",
                            key=f"download_{file_name}",
                        )
                    except Exception as e:
                        st.warning(f"è¯»å–å¤±è´¥: {e}")
            else:
                st.markdown(f"**{file_name}**")
                st.caption("æ–‡ä»¶ä¸å­˜åœ¨")
                st.button("ğŸ“¥ ä¸‹è½½", disabled=True, key=f"download_{file_name}")
    
    st.markdown("---")


# =============================================================================
# TensorFlow Projector æŒ‡å—
# =============================================================================

st.markdown("## ğŸ“– TensorFlow Embedding Projector ä½¿ç”¨æŒ‡å—")

st.markdown("""
TensorFlow Embedding Projector æ˜¯ä¸€ä¸ªå¼ºå¤§çš„åœ¨çº¿å¯è§†åŒ–å·¥å…·ï¼Œå¯ä»¥äº¤äº’å¼åœ°æ¢ç´¢é«˜ç»´åµŒå…¥å‘é‡ã€‚

### ä½¿ç”¨æ­¥éª¤

1. **ä¸‹è½½æ–‡ä»¶**
   - ä¸‹è½½ `vectors.tsv` (å‘é‡æ–‡ä»¶)
   - ä¸‹è½½ `metadata.tsv` (å…ƒæ•°æ®æ–‡ä»¶)

2. **è®¿é—® Projector**
   - æ‰“å¼€ [https://projector.tensorflow.org/](https://projector.tensorflow.org/)

3. **åŠ è½½æ•°æ®**
   - ç‚¹å‡»å·¦ä¾§çš„ **"Load"** æŒ‰é’®
   - åœ¨ **"Step 1: Load a TSV file of vectors"** ä¸­é€‰æ‹© `vectors.tsv`
   - åœ¨ **"Step 2: Load a TSV file of metadata"** ä¸­é€‰æ‹© `metadata.tsv`
   - ç‚¹å‡» **"Publish"** å¤–çš„æŒ‰é’®åŠ è½½æ•°æ®

4. **æ¢ç´¢æ•°æ®**
   - ä½¿ç”¨ **PCA**ã€**t-SNE**ã€**UMAP** åˆ‡æ¢é™ç»´æ–¹æ³•
   - åœ¨æœç´¢æ¡†ä¸­æœç´¢ç‰¹å®š Token
   - ç‚¹å‡»æ•°æ®ç‚¹æŸ¥çœ‹æœ€è¿‘é‚»
""")

# å›¾ç¤º
st.markdown("### ç•Œé¢é¢„è§ˆ")

col1, col2 = st.columns(2)

with col1:
    st.markdown("""
    #### åŠ è½½ç•Œé¢
    ```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Load data                  â”‚
    â”‚                             â”‚
    â”‚  Step 1: Load vectors.tsv   â”‚
    â”‚  [Choose file...]           â”‚
    â”‚                             â”‚
    â”‚  Step 2: Load metadata.tsv  â”‚
    â”‚  [Choose file...]           â”‚
    â”‚                             â”‚
    â”‚  [Load] [Publish]           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    ```
    """)

with col2:
    st.markdown("""
    #### å¯è§†åŒ–ç•Œé¢
    ```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  [PCA] [t-SNE] [UMAP]       â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚                       â”‚  â”‚
    â”‚  â”‚   â—‹ â—‹                 â”‚  â”‚
    â”‚  â”‚     â—‹ â—‹ â—‹             â”‚  â”‚
    â”‚  â”‚   â—‹     â—‹ â—‹           â”‚  â”‚
    â”‚  â”‚                       â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â”‚  Search: [____________]     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    ```
    """)

st.markdown("---")


# =============================================================================
# ONNX æ¨¡å‹ä½¿ç”¨æŒ‡å—
# =============================================================================

st.markdown("## ğŸ“– ONNX æ¨¡å‹ä½¿ç”¨æŒ‡å—")

st.markdown("""
`word2vec.onnx` æ˜¯å¯¼å‡ºçš„ ONNX æ ¼å¼æ¨¡å‹ï¼Œå¯ä»¥åœ¨å¤šç§å¹³å°ä¸Šè¿›è¡Œæ¨ç†ã€‚

### Python ä½¿ç”¨ç¤ºä¾‹
""")

st.code("""
import numpy as np
import onnxruntime as ort

# åŠ è½½æ¨¡å‹
session = ort.InferenceSession("word2vec.onnx")

# å‡†å¤‡è¾“å…¥ (token_id)
token_ids = np.array([1, 2, 3], dtype=np.int64)

# æ‰§è¡Œæ¨ç†
outputs = session.run(None, {"token_ids": token_ids})
embeddings = outputs[0]  # shape: (3, 128)

print(f"åµŒå…¥å‘é‡å½¢çŠ¶: {embeddings.shape}")
""", language="python")

st.markdown("""
### JavaScript ä½¿ç”¨ç¤ºä¾‹ (ONNX Runtime Web)
""")

st.code("""
import * as ort from 'onnxruntime-web';

async function getEmbeddings(tokenIds) {
    // åŠ è½½æ¨¡å‹
    const session = await ort.InferenceSession.create('word2vec.onnx');
    
    // å‡†å¤‡è¾“å…¥
    const inputTensor = new ort.Tensor('int64', 
        BigInt64Array.from(tokenIds.map(BigInt)), 
        [tokenIds.length]
    );
    
    // æ‰§è¡Œæ¨ç†
    const results = await session.run({ token_ids: inputTensor });
    const embeddings = results.embeddings.data;
    
    return embeddings;
}
""", language="javascript")

st.markdown("---")


# =============================================================================
# æ–‡ä»¶æ ¼å¼è¯´æ˜
# =============================================================================

st.markdown("## ğŸ“‹ æ–‡ä»¶æ ¼å¼è¯´æ˜")

file_formats = {
    "embeddings.npy": {
        "æ ¼å¼": "NumPy äºŒè¿›åˆ¶",
        "å½¢çŠ¶": "(vocab_size, 128)",
        "ç”¨é€”": "Python ä¸­ç›´æ¥åŠ è½½ä½¿ç”¨",
        "ç¤ºä¾‹": "np.load('embeddings.npy')",
    },
    "embeddings.json": {
        "æ ¼å¼": "JSON",
        "ç»“æ„": '{"tokens": [...], "embeddings": [...], "metadata": {...}}',
        "ç”¨é€”": "ç½‘é¡µå¯è§†åŒ–ã€è·¨å¹³å°ä½¿ç”¨",
        "ç¤ºä¾‹": "json.load(open('embeddings.json'))",
    },
    "clustering.json": {
        "æ ¼å¼": "JSON",
        "ç»“æ„": '{"points": [...], "clusters": [...], "metadata": {...}}',
        "ç”¨é€”": "é¢„è®¡ç®—çš„ t-SNE åæ ‡å’Œèšç±»æ ‡ç­¾",
        "ç¤ºä¾‹": "åŒ…å« x, y åæ ‡å’Œ cluster æ ‡ç­¾",
    },
    "vectors.tsv": {
        "æ ¼å¼": "TSV (åˆ¶è¡¨ç¬¦åˆ†éš”)",
        "ç»“æ„": "æ¯è¡Œä¸€ä¸ªå‘é‡ï¼Œ128 ä¸ªæµ®ç‚¹æ•°ç”¨åˆ¶è¡¨ç¬¦åˆ†éš”",
        "ç”¨é€”": "TensorFlow Embedding Projector",
        "ç¤ºä¾‹": "0.123\\t0.456\\t0.789\\t...",
    },
    "metadata.tsv": {
        "æ ¼å¼": "TSV",
        "ç»“æ„": "æ¯è¡Œä¸€ä¸ª Token åç§°",
        "ç”¨é€”": "ä¸ vectors.tsv é…åˆä½¿ç”¨",
        "ç¤ºä¾‹": "MOV_tt0111161\\nACT_nm0000001\\n...",
    },
    "word2vec.onnx": {
        "æ ¼å¼": "ONNX",
        "è¾“å…¥": "token_ids (int64, [batch_size])",
        "è¾“å‡º": "embeddings (float32, [batch_size, 128])",
        "ç”¨é€”": "è·¨å¹³å°åœ¨çº¿æ¨ç†",
    },
}

for file_name, info in file_formats.items():
    with st.expander(f"ğŸ“„ {file_name}"):
        for key, value in info.items():
            st.markdown(f"**{key}:** {value}")

