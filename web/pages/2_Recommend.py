"""
æ¨èå…³ç³»é¡µé¢ (Recommendation)
============================

ä½¿ç”¨ ONNX æ¨¡å‹è¿›è¡Œåœ¨çº¿æ¨ç†ï¼Œè®¡ç®—å®ä½“é—´çš„ç›¸ä¼¼åº¦å¹¶å±•ç¤ºæ¨èå…³ç³»ã€‚

åŠŸèƒ½:
- æœç´¢ä»»æ„å®ä½“
- ONNX æ¨ç†è·å–åµŒå…¥å‘é‡
- è®¡ç®— Top-K ç›¸ä¼¼åº¦
- å±•ç¤ºå…³ç³»ç½‘ç»œå›¾
- ç›¸ä¼¼åº¦æ’åè¡¨æ ¼

ä½¿ç”¨çš„æ•°æ®æ–‡ä»¶:
- word2vec.onnx: ONNX æ¨ç†æ¨¡å‹
- recsys/token_to_id.json: Token æ˜ å°„
- embeddings.npy: åµŒå…¥å‘é‡ï¼ˆç”¨äºç›¸ä¼¼åº¦è®¡ç®—ï¼‰
"""
import streamlit as st
from pathlib import Path
import sys
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from config import AppConfig, ENTITY_TYPE_NAMES, ENTITY_TYPE_COLORS
from utils.data_loader import (
    load_embeddings_npy,
    load_token_to_id,
    load_id_to_token,
    search_tokens,
    get_entity_type,
)
from utils.onnx_inference import get_model, get_embedding
from utils.similarity import find_top_k_similar
from utils.visualization import create_radial_network
from components.sidebar import render_page_header
from components.filters import render_type_filter, render_top_k_selector
from components.similarity_list import render_similarity_list, render_similarity_table
from components.entity_card import render_entity_card


# =============================================================================
# é¡µé¢é…ç½®
# =============================================================================

st.set_page_config(
    page_title="Recommendation - " + AppConfig.APP_TITLE,
    page_icon="ğŸ”—",
    layout=AppConfig.LAYOUT,
)


# =============================================================================
# é¡µé¢æ ‡é¢˜
# =============================================================================

render_page_header(
    title="æ¨èå…³ç³»",
    description="ä½¿ç”¨ ONNX æ¨¡å‹è¿›è¡Œåœ¨çº¿æ¨ç†ï¼Œæœç´¢å®ä½“å¹¶è·å–ç›¸ä¼¼æ¨èã€‚",
    icon="ğŸ”—",
)


# =============================================================================
# åŠ è½½æ•°æ®
# =============================================================================

@st.cache_data
def load_all_data():
    """åŠ è½½æ‰€æœ‰éœ€è¦çš„æ•°æ®"""
    embeddings = load_embeddings_npy()
    token_to_id = load_token_to_id()
    id_to_token = load_id_to_token()
    return embeddings, token_to_id, id_to_token


embeddings, token_to_id, id_to_token = load_all_data()

# æ„å»º token åˆ—è¡¨
tokens_list = [""] * len(embeddings)
for token, idx in token_to_id.items():
    if idx < len(tokens_list):
        tokens_list[idx] = token


# =============================================================================
# ä¾§è¾¹æ 
# =============================================================================

with st.sidebar:
    st.markdown("### ğŸ”— æ¨èè®¾ç½®")
    st.markdown("---")
    
    # Top-K è®¾ç½®
    top_k = render_top_k_selector(key="recommend_top_k", default=10, max_value=30)
    
    st.markdown("---")
    
    # ç±»å‹è¿‡æ»¤
    st.markdown("#### ğŸ” ç»“æœç±»å‹è¿‡æ»¤")
    filter_type = st.selectbox(
        "åªæ˜¾ç¤ºç‰¹å®šç±»å‹",
        options=["å…¨éƒ¨"] + list(ENTITY_TYPE_NAMES.keys()),
        format_func=lambda x: f"{ENTITY_TYPE_NAMES.get(x, x)}" if x != "å…¨éƒ¨" else "å…¨éƒ¨ç±»å‹",
        key="recommend_type_filter",
    )
    
    st.markdown("---")
    
    # ONNX æ¨¡å‹ä¿¡æ¯
    st.markdown("#### âš™ï¸ ONNX æ¨¡å‹")
    
    # æ£€æŸ¥ onnxruntime æ˜¯å¦å¯ç”¨
    try:
        import onnxruntime
        st.caption(f"onnxruntime: v{onnxruntime.__version__}")
    except ImportError:
        st.error("onnxruntime æœªå®‰è£…")
    
    model = get_model()
    if model.session:
        model_info = model.get_model_info()
        st.success("æ¨¡å‹å·²åŠ è½½")
        st.caption(f"å¤§å°: {model_info.get('model_size_mb', 0)} MB")
    else:
        st.error("æ¨¡å‹åŠ è½½å¤±è´¥")
        st.caption(f"æ¨¡å‹è·¯å¾„: {model.model_path}")


# =============================================================================
# æœç´¢æ¡†
# =============================================================================

st.markdown("## ğŸ” æœç´¢å®ä½“")

col1, col2 = st.columns([3, 1])

with col1:
    search_query = st.text_input(
        "è¾“å…¥ Token åç§°",
        placeholder="ä¾‹å¦‚: MOV_tt0111161, ACT_nm0000001, DIR_nm0000229",
        key="search_input",
    )

with col2:
    search_button = st.button("ğŸ” æœç´¢", use_container_width=True)

# æœç´¢å»ºè®®
if search_query and len(search_query) >= 2:
    suggestions = search_tokens(search_query, limit=5)
    
    if suggestions:
        st.markdown("**æœç´¢å»ºè®®:**")
        cols = st.columns(min(len(suggestions), 5))
        
        for i, suggestion in enumerate(suggestions):
            with cols[i]:
                entity_type = get_entity_type(suggestion)
                color = ENTITY_TYPE_COLORS.get(entity_type, "#888")
                
                if st.button(
                    suggestion,
                    key=f"suggest_{i}",
                    help=ENTITY_TYPE_NAMES.get(entity_type, entity_type),
                ):
                    st.session_state["selected_token"] = suggestion
                    st.rerun()


# =============================================================================
# è·å–é€‰ä¸­çš„ Token
# =============================================================================

selected_token = st.session_state.get("selected_token", None)

if search_button and search_query:
    # ç²¾ç¡®åŒ¹é…
    if search_query in token_to_id:
        selected_token = search_query
        st.session_state["selected_token"] = selected_token
    else:
        # å°è¯•æ¨¡ç³ŠåŒ¹é…
        matches = search_tokens(search_query, limit=1)
        if matches:
            selected_token = matches[0]
            st.session_state["selected_token"] = selected_token
        else:
            st.warning(f"æœªæ‰¾åˆ°åŒ¹é…çš„ Token: {search_query}")


# =============================================================================
# æ¨èç»“æœ
# =============================================================================

if selected_token:
    st.markdown("---")
    st.markdown(f"## ğŸ“Œ æŸ¥è¯¢å®ä½“: `{selected_token}`")
    
    # è·å–åµŒå…¥å‘é‡
    model = get_model()
    
    # æ£€æŸ¥å¤±è´¥åŸå› 
    if model.session is None:
        st.error("âš ï¸ ONNX æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•è¿›è¡Œæ¨ç†")
        st.info("è¯·æ£€æŸ¥ ONNX æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œæˆ–æŸ¥çœ‹æ§åˆ¶å°é”™è¯¯ä¿¡æ¯")
        query_vec = None
    elif selected_token not in model.token_to_id:
        st.error(f"âš ï¸ Token `{selected_token}` ä¸åœ¨è¯æ±‡è¡¨ä¸­")
        query_vec = None
    else:
        query_vec = get_embedding(selected_token)
        if query_vec is None:
            st.error("âš ï¸ ONNX æ¨ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ§åˆ¶å°é”™è¯¯ä¿¡æ¯")
    
    if query_vec is not None:
        # æ˜¾ç¤ºå®ä½“ä¿¡æ¯
        col1, col2 = st.columns([1, 2])
        
        with col1:
            render_entity_card(
                token=selected_token,
                embedding=query_vec,
                show_vector=True,
            )
        
        with col2:
            # è®¡ç®—ç›¸ä¼¼åº¦
            type_filter = filter_type if filter_type != "å…¨éƒ¨" else None
            
            similar_results = find_top_k_similar(
                query_vec=query_vec,
                embeddings=embeddings,
                tokens=tokens_list,
                k=top_k,
                exclude_self=True,
                query_token=selected_token,
                entity_type_filter=type_filter,
            )
            
            if similar_results:
                st.markdown("### ğŸ“Š ç›¸ä¼¼åº¦æ’å")
                render_similarity_table(similar_results, title="")
        
        # =============================================================================
        # å…³ç³»ç½‘ç»œå›¾
        # =============================================================================
        
        st.markdown("---")
        st.markdown("## ğŸ•¸ï¸ å…³ç³»ç½‘ç»œå›¾")
        
        if similar_results:
            # æ„å»ºç½‘ç»œå›¾æ•°æ®
            center_node = {
                "id": selected_token,
                "label": selected_token.split("_")[-1] if "_" in selected_token else selected_token,
                "type": get_entity_type(selected_token),
            }
            
            related_nodes = []
            for item in similar_results[:10]:  # åªæ˜¾ç¤ºå‰ 10 ä¸ª
                token = item["token"]
                related_nodes.append({
                    "id": token,
                    "label": token.split("_")[-1] if "_" in token else token,
                    "type": get_entity_type(token),
                    "similarity": item["similarity"],
                })
            
            # åˆ›å»ºç½‘ç»œå›¾
            fig = create_radial_network(
                center_node=center_node,
                related_nodes=related_nodes,
                title="",
                height=500,
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            st.caption("èŠ‚ç‚¹å¤§å°è¡¨ç¤ºä¸æŸ¥è¯¢å®ä½“çš„å…³ç³»å¼ºåº¦ï¼Œè·ç¦»ä¸­å¿ƒè¶Šè¿‘ç›¸ä¼¼åº¦è¶Šé«˜")
        else:
            st.info("æ²¡æœ‰æ‰¾åˆ°ç›¸ä¼¼å®ä½“")

else:
    # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
    st.markdown("---")
    st.markdown("""
    ### ğŸ’¡ ä½¿ç”¨è¯´æ˜
    
    1. åœ¨æœç´¢æ¡†ä¸­è¾“å…¥å®ä½“çš„ Token åç§°
    2. Token æ ¼å¼ä¸º: `ç±»å‹_ID`ï¼Œä¾‹å¦‚:
       - `MOV_tt0111161` - ç”µå½±
       - `ACT_nm0000001` - æ¼”å‘˜
       - `DIR_nm0000229` - å¯¼æ¼”
    3. ç‚¹å‡»æœç´¢æˆ–é€‰æ‹©å»ºè®®çš„ Token
    4. æŸ¥çœ‹ç›¸ä¼¼åº¦æ’åå’Œå…³ç³»ç½‘ç»œå›¾
    
    ### ğŸ“ Token ç±»å‹è¯´æ˜
    """)
    
    cols = st.columns(4)
    for i, (entity_type, name) in enumerate(ENTITY_TYPE_NAMES.items()):
        with cols[i % 4]:
            color = ENTITY_TYPE_COLORS.get(entity_type, "#888")
            st.markdown(
                f'<span style="color:{color}">â—</span> **{entity_type}**: {name}',
                unsafe_allow_html=True,
            )

