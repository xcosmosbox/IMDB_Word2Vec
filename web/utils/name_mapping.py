"""
åç§°æ˜ å°„æ¨¡å— (é«˜æ€§èƒ½ç‰ˆ)
========================

å°†å†…éƒ¨ Token (å¦‚ MOV_tt0111161, ACT_nm0000001) æ˜ å°„ä¸ºçœŸå®çš„ç”µå½±å/æ¼”å‘˜å/å¯¼æ¼”åã€‚

ä¼˜åŒ–ç­–ç•¥:
- ä½¿ç”¨ Pickle æ ¼å¼ç¼“å­˜ï¼ˆæ¯” JSON å¿« 10-50 å€ï¼‰
- å‘é‡åŒ–æ“ä½œæ›¿ä»£ iterrowsï¼ˆå¿« 100 å€ï¼‰
- åˆ†ç‰‡ç¼“å­˜ï¼ˆç”µå½±/äººå‘˜åˆ†å¼€å­˜å‚¨ï¼‰
- Streamlit å†…å­˜ç¼“å­˜ + ç£ç›˜ç¼“å­˜åŒé‡åŠ é€Ÿ

ä½¿ç”¨æ–¹æ³•:
    from utils.name_mapping import get_display_name, fuzzy_search, search_entities
    
    # å•ä¸ª token è½¬æ¢
    name = get_display_name("MOV_tt0111161")  # -> "The Shawshank Redemption"
    
    # æ¨¡ç³Šæœç´¢
    results = fuzzy_search("shawshnk", limit=5)  # å®¹å¿æ‹¼å†™é”™è¯¯
"""
import pickle
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any

import pandas as pd
import numpy as np
import streamlit as st

# å¯¼å…¥é…ç½®
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))
from config import PROJECT_ROOT, ENTITY_TYPE_NAMES

# å¯¼å…¥ç¼“å­˜ç®¡ç†å™¨
from .cache_manager import CACHE_DIR, compute_file_hash


# =============================================================================
# æ•°æ®æ–‡ä»¶è·¯å¾„
# =============================================================================

# ç¼“å­˜ç›®å½• (é¢„å¤„ç†ç”Ÿæˆçš„ CSV æ–‡ä»¶)
SOURCE_CACHE_DIR = PROJECT_ROOT / "imdb_word2vec" / "cache"

# ç”µå½±ä¿¡æ¯æ–‡ä»¶
MOVIES_INFO_CSV = SOURCE_CACHE_DIR / "movies_info_df.csv"

# äººå‘˜ä¿¡æ¯æ–‡ä»¶
STAFF_DF_CSV = SOURCE_CACHE_DIR / "staff_df.csv"

# åç§°æ˜ å°„ç¼“å­˜ç›®å½•
NAME_MAPPING_CACHE_DIR = CACHE_DIR / "name_mapping"
NAME_MAPPING_CACHE_DIR.mkdir(exist_ok=True)


# =============================================================================
# é¢„å®šä¹‰æ˜ å°„ (ç±»å‹ã€å¹´ä»£ã€è¯„åˆ†ç­‰)
# =============================================================================

# ç±»å‹æ˜ å°„ (è‹±æ–‡ -> ä¸­æ–‡)
GENRE_MAPPING = {
    "GEN_Action": "åŠ¨ä½œ",
    "GEN_Adventure": "å†’é™©",
    "GEN_Animation": "åŠ¨ç”»",
    "GEN_Biography": "ä¼ è®°",
    "GEN_Comedy": "å–œå‰§",
    "GEN_Crime": "çŠ¯ç½ª",
    "GEN_Documentary": "çºªå½•ç‰‡",
    "GEN_Drama": "å‰§æƒ…",
    "GEN_Family": "å®¶åº­",
    "GEN_Fantasy": "å¥‡å¹»",
    "GEN_Film-Noir": "é»‘è‰²ç”µå½±",
    "GEN_History": "å†å²",
    "GEN_Horror": "ææ€–",
    "GEN_Music": "éŸ³ä¹",
    "GEN_Musical": "æ­Œèˆ",
    "GEN_Mystery": "æ‚¬ç–‘",
    "GEN_News": "æ–°é—»",
    "GEN_Reality-TV": "çœŸäººç§€",
    "GEN_Romance": "çˆ±æƒ…",
    "GEN_Sci-Fi": "ç§‘å¹»",
    "GEN_Short": "çŸ­ç‰‡",
    "GEN_Sport": "è¿åŠ¨",
    "GEN_Talk-Show": "è„±å£ç§€",
    "GEN_Thriller": "æƒŠæ‚š",
    "GEN_War": "æˆ˜äº‰",
    "GEN_Western": "è¥¿éƒ¨",
    "GEN_Adult": "æˆäºº",
    "GEN_Game-Show": "æ¸¸æˆèŠ‚ç›®",
}

# å¹´ä»£æ˜ å°„
ERA_MAPPING = {
    "ERA_SILENT": "é»˜ç‰‡æ—¶ä»£",
    "ERA_1920s": "1920å¹´ä»£",
    "ERA_1930s": "1930å¹´ä»£",
    "ERA_1940s": "1940å¹´ä»£",
    "ERA_1950s": "1950å¹´ä»£",
    "ERA_1960s": "1960å¹´ä»£",
    "ERA_1970s": "1970å¹´ä»£",
    "ERA_1980s": "1980å¹´ä»£",
    "ERA_1990s": "1990å¹´ä»£",
    "ERA_2000s": "2000å¹´ä»£",
    "ERA_2010s": "2010å¹´ä»£",
    "ERA_2020s": "2020å¹´ä»£",
    "ERA_UNKNOWN": "æœªçŸ¥å¹´ä»£",
}

# ä½œå“ç±»å‹æ˜ å°„
TYPE_MAPPING = {
    "TYP_movie": "ç”µå½±",
    "TYP_short": "çŸ­ç‰‡",
    "TYP_tvSeries": "ç”µè§†å‰§",
    "TYP_tvMiniSeries": "è¿·ä½ å‰§",
    "TYP_tvMovie": "ç”µè§†ç”µå½±",
    "TYP_tvSpecial": "ç”µè§†ç‰¹è¾‘",
    "TYP_video": "è§†é¢‘",
    "TYP_videoGame": "è§†é¢‘æ¸¸æˆ",
    "TYP_tvEpisode": "å‰§é›†",
}


# =============================================================================
# é«˜æ€§èƒ½æ•°æ®æ„å»º
# =============================================================================

def _get_cache_version() -> str:
    """
    è®¡ç®—æºæ–‡ä»¶çš„ç‰ˆæœ¬å“ˆå¸Œï¼Œç”¨äºç¼“å­˜å¤±æ•ˆåˆ¤æ–­
    """
    version_parts = []
    if MOVIES_INFO_CSV.exists():
        version_parts.append(compute_file_hash(MOVIES_INFO_CSV))
    if STAFF_DF_CSV.exists():
        version_parts.append(compute_file_hash(STAFF_DF_CSV))
    
    if not version_parts:
        return "no_source"
    
    import hashlib
    combined = hashlib.md5("_".join(version_parts).encode()).hexdigest()[:8]
    return combined


def _build_movie_mapping_fast() -> Dict[str, str]:
    """
    ä½¿ç”¨å‘é‡åŒ–æ“ä½œå¿«é€Ÿæ„å»ºç”µå½±æ˜ å°„
    
    Returns:
        {MOV_ttXXXXXX: title} å­—å…¸
    """
    if not MOVIES_INFO_CSV.exists():
        return {}
    
    try:
        # åªè¯»å–éœ€è¦çš„åˆ—ï¼Œä½¿ç”¨ PyArrow å¼•æ“åŠ é€Ÿ
        df = pd.read_csv(
            MOVIES_INFO_CSV,
            usecols=["tconst", "title"],
            dtype=str,
            engine="pyarrow" if "pyarrow" in pd.io.parsers.readers.__dict__.get("_c_parser_defaults", {}) else "c",
            na_filter=False,  # è·³è¿‡ NA æ£€æµ‹ï¼Œæ›´å¿«
        )
        
        # å‘é‡åŒ–æ“ä½œï¼šæ¯” iterrows å¿« 100 å€
        # è¿‡æ»¤ç©ºå€¼
        mask = (df["tconst"] != "") & (df["title"] != "")
        df = df[mask]
        
        # æ„å»º token
        tokens = "MOV_" + df["tconst"]
        
        # ç›´æ¥è½¬ä¸ºå­—å…¸
        return dict(zip(tokens, df["title"]))
        
    except Exception as e:
        st.warning(f"åŠ è½½ç”µå½±åç§°å¤±è´¥: {e}")
        return {}


def _build_staff_mapping_fast() -> Dict[str, str]:
    """
    ä½¿ç”¨å‘é‡åŒ–æ“ä½œå¿«é€Ÿæ„å»ºäººå‘˜æ˜ å°„
    
    Returns:
        {ACT_nmXXXX: name, DIR_nmXXXX: name, PER_nmXXXX: name} å­—å…¸
    """
    if not STAFF_DF_CSV.exists():
        return {}
    
    try:
        df = pd.read_csv(
            STAFF_DF_CSV,
            usecols=["nconst", "primaryName"],
            dtype=str,
            na_filter=False,
        )
        
        # è¿‡æ»¤ç©ºå€¼
        mask = (df["nconst"] != "") & (df["primaryName"] != "")
        df = df[mask]
        
        # ä½¿ç”¨ pandas å­—ç¬¦ä¸²æ“ä½œï¼ˆæ¯” numpy æ›´å¥å£®ï¼‰
        nconsts = df["nconst"].values
        names = df["primaryName"].values
        
        # ä¸ºæ¯ä¸ªäººå‘˜åˆ›å»º ACT_, DIR_, PER_ ä¸‰ä¸ªæ˜ å°„
        mapping = {}
        
        for prefix in ["ACT_", "DIR_", "PER_"]:
            # ç›´æ¥ä½¿ç”¨åˆ—è¡¨æ¨å¯¼ï¼Œç®€å•å¯é 
            tokens = [f"{prefix}{nc}" for nc in nconsts]
            mapping.update(dict(zip(tokens, names)))
        
        return mapping
        
    except Exception as e:
        print(f"åŠ è½½äººå‘˜åç§°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {}


def _build_static_mapping() -> Dict[str, str]:
    """
    æ„å»ºé™æ€æ˜ å°„ï¼ˆç±»å‹ã€å¹´ä»£ã€è¯„åˆ†ï¼‰
    """
    mapping = {}
    mapping.update(GENRE_MAPPING)
    mapping.update(ERA_MAPPING)
    mapping.update(TYPE_MAPPING)
    
    # è¯„åˆ†æ˜ å°„
    for i in range(0, 101):
        rating = i / 10
        mapping[f"RAT_{rating}"] = f"â­ {rating}åˆ†"
        mapping[f"RAT_{rating:.1f}"] = f"â­ {rating:.1f}åˆ†"
    
    return mapping


def _load_or_build_mapping(cache_name: str, build_fn, version: str) -> Dict[str, str]:
    """
    åŠ è½½ç¼“å­˜æˆ–æ„å»ºæ˜ å°„ï¼ˆä½¿ç”¨ Pickleï¼‰
    """
    cache_file = NAME_MAPPING_CACHE_DIR / f"{cache_name}_{version}.pkl"
    
    # å°è¯•åŠ è½½ç¼“å­˜
    if cache_file.exists():
        try:
            with open(cache_file, "rb") as f:
                return pickle.load(f)
        except Exception:
            pass  # ç¼“å­˜æŸåï¼Œé‡æ–°æ„å»º
    
    # æ„å»ºæ˜ å°„
    mapping = build_fn()
    
    # ä¿å­˜ç¼“å­˜
    try:
        with open(cache_file, "wb") as f:
            pickle.dump(mapping, f, protocol=pickle.HIGHEST_PROTOCOL)
    except Exception:
        pass
    
    return mapping


# =============================================================================
# åˆ†ç‰‡åŠ è½½å‡½æ•°
# =============================================================================

@st.cache_data(ttl=None, show_spinner=False)
def _load_movie_mapping() -> Dict[str, str]:
    """åŠ è½½ç”µå½±æ˜ å°„ï¼ˆStreamlit å†…å­˜ç¼“å­˜ï¼‰"""
    version = _get_cache_version()
    return _load_or_build_mapping("movies", _build_movie_mapping_fast, version)


@st.cache_data(ttl=None, show_spinner=False)
def _load_staff_mapping() -> Dict[str, str]:
    """åŠ è½½äººå‘˜æ˜ å°„ï¼ˆStreamlit å†…å­˜ç¼“å­˜ï¼‰"""
    version = _get_cache_version()
    return _load_or_build_mapping("staff", _build_staff_mapping_fast, version)


@st.cache_data(ttl=None, show_spinner=False)
def _load_static_mapping() -> Dict[str, str]:
    """åŠ è½½é™æ€æ˜ å°„"""
    return _build_static_mapping()


# =============================================================================
# å¹¶è¡ŒåŠ è½½æ”¯æŒ
# =============================================================================

def _load_all_mappings_parallel() -> Dict[str, str]:
    """
    å¹¶è¡ŒåŠ è½½æ‰€æœ‰æ˜ å°„åˆ†ç‰‡
    
    ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡ŒåŠ è½½ç”µå½±å’Œäººå‘˜æ˜ å°„ï¼Œæå‡åŠ è½½é€Ÿåº¦
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed
    
    mapping = {}
    
    # é™æ€æ˜ å°„ï¼ˆåŒæ­¥ï¼Œå¾ˆå¿«ï¼‰
    mapping.update(_build_static_mapping())
    
    version = _get_cache_version()
    
    # å®šä¹‰åŠ è½½ä»»åŠ¡
    tasks = {
        "movies": lambda: _load_or_build_mapping("movies", _build_movie_mapping_fast, version),
        "staff": lambda: _load_or_build_mapping("staff", _build_staff_mapping_fast, version),
    }
    
    # å¹¶è¡Œæ‰§è¡Œ
    with ThreadPoolExecutor(max_workers=2) as executor:
        futures = {executor.submit(fn): name for name, fn in tasks.items()}
        
        for future in as_completed(futures):
            name = futures[future]
            try:
                result = future.result()
                mapping.update(result)
            except Exception as e:
                print(f"åŠ è½½ {name} æ˜ å°„å¤±è´¥: {e}")
    
    return mapping


# =============================================================================
# ä¸»åŠ è½½å‡½æ•°
# =============================================================================

@st.cache_data(ttl=None, show_spinner="åŠ è½½åç§°æ˜ å°„...")
def load_name_mapping() -> Dict[str, str]:
    """
    åŠ è½½å®Œæ•´çš„ Token â†’ åç§° æ˜ å°„
    
    ä½¿ç”¨åˆ†ç‰‡+å¹¶è¡ŒåŠ è½½ç­–ç•¥ï¼š
    - ç”µå½±å’Œäººå‘˜æ˜ å°„å¹¶è¡ŒåŠ è½½
    - æ¯ä¸ªåˆ†ç‰‡ç‹¬ç«‹ç¼“å­˜ï¼ˆPickle æ ¼å¼ï¼‰
    - Streamlit å†…å­˜ç¼“å­˜ç¡®ä¿åªåŠ è½½ä¸€æ¬¡
    
    Returns:
        {token: display_name} å­—å…¸
    """
    return _load_all_mappings_parallel()


@st.cache_data(ttl=None, show_spinner=False)
def load_reverse_mapping() -> Dict[str, str]:
    """
    åŠ è½½ åç§° â†’ Token çš„åå‘æ˜ å°„
    
    Returns:
        {lowercase_name: token} å­—å…¸
    """
    forward = load_name_mapping()
    
    reverse = {}
    for token, name in forward.items():
        key = name.lower()
        # ä¼˜å…ˆä¿ç•™ç”µå½±/æ¼”å‘˜ï¼ˆè€Œéç±»å‹ç­‰ï¼‰
        if key not in reverse or token.startswith(("MOV_", "ACT_", "DIR_")):
            reverse[key] = token
    
    return reverse


@st.cache_data(ttl=None, show_spinner=False)
def load_search_list() -> List[Tuple[str, str, str]]:
    """
    åŠ è½½æœç´¢åˆ—è¡¨
    
    Returns:
        [(display_name, token, entity_type), ...] åˆ—è¡¨
    """
    forward = load_name_mapping()
    
    search_list = []
    for token, name in forward.items():
        entity_type = token.split("_")[0] if "_" in token else "OTHER"
        search_list.append((name, token, entity_type))
    
    return search_list


# =============================================================================
# æŸ¥è¯¢å‡½æ•°
# =============================================================================

def get_display_name(token: str) -> str:
    """
    è·å– Token çš„æ˜¾ç¤ºåç§°
    
    Args:
        token: å†…éƒ¨ Tokenï¼Œå¦‚ "MOV_tt0111161"
        
    Returns:
        æ˜¾ç¤ºåç§°ï¼Œå¦‚ "The Shawshank Redemption"
    """
    # å¿«é€Ÿè·¯å¾„ï¼šæ£€æŸ¥é™æ€æ˜ å°„
    if token in GENRE_MAPPING:
        return GENRE_MAPPING[token]
    if token in ERA_MAPPING:
        return ERA_MAPPING[token]
    if token in TYPE_MAPPING:
        return TYPE_MAPPING[token]
    if token.startswith("RAT_"):
        try:
            rating = float(token[4:])
            return f"â­ {rating}åˆ†"
        except ValueError:
            pass
    
    # åŠ è½½å®Œæ•´æ˜ å°„
    mapping = load_name_mapping()
    return mapping.get(token, token)


def token_to_display(token: str, show_token: bool = False) -> str:
    """
    å°† Token è½¬æ¢ä¸ºæ˜¾ç¤ºæ–‡æœ¬
    
    Args:
        token: Token å­—ç¬¦ä¸²
        show_token: æ˜¯å¦åœ¨åç§°åæ˜¾ç¤º token
        
    Returns:
        æ ¼å¼åŒ–çš„æ˜¾ç¤ºæ–‡æœ¬
    """
    name = get_display_name(token)
    if show_token and name != token:
        return f"{name} ({token})"
    return name


def get_entity_type_name(token: str) -> str:
    """
    è·å–å®ä½“ç±»å‹çš„ä¸­æ–‡åç§°
    
    Args:
        token: Token å­—ç¬¦ä¸²
        
    Returns:
        ç±»å‹åç§°ï¼Œå¦‚ "ç”µå½±", "æ¼”å‘˜"
    """
    prefix = token.split("_")[0] if "_" in token else "OTHER"
    return ENTITY_TYPE_NAMES.get(prefix, "æœªçŸ¥")


# =============================================================================
# æ¨¡ç³Šæœç´¢
# =============================================================================

@st.cache_data(ttl=3600, show_spinner=False)
def fuzzy_search(
    query: str,
    limit: int = 10,
    threshold: int = 60,
    entity_types: Optional[List[str]] = None,
) -> List[Tuple[str, str, str, float]]:
    """
    æ¨¡ç³Šæœç´¢å®ä½“
    
    Args:
        query: æœç´¢è¯
        limit: è¿”å›æ•°é‡é™åˆ¶
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ (0-100)
        entity_types: é™åˆ¶å®ä½“ç±»å‹ï¼Œå¦‚ ["MOV", "ACT"]
        
    Returns:
        [(display_name, token, entity_type, score), ...] åˆ—è¡¨
    """
    if not query or len(query) < 1:
        return []
    
    try:
        from rapidfuzz import fuzz, process
    except ImportError:
        # é™çº§ä¸ºç²¾ç¡®åŒ¹é…
        return exact_search(query, limit, entity_types)
    
    search_list = load_search_list()
    
    # è¿‡æ»¤å®ä½“ç±»å‹
    if entity_types:
        search_list = [
            (name, token, etype)
            for name, token, etype in search_list
            if etype in entity_types
        ]
    
    if not search_list:
        return []
    
    # æ„å»ºæœç´¢å­—å…¸
    name_to_info = {name: (token, etype) for name, token, etype in search_list}
    names = list(name_to_info.keys())
    
    # ä½¿ç”¨ rapidfuzz è¿›è¡Œæ¨¡ç³ŠåŒ¹é…
    results = process.extract(
        query,
        names,
        scorer=fuzz.WRatio,
        limit=limit,
        score_cutoff=threshold,
    )
    
    return [
        (name, name_to_info[name][0], name_to_info[name][1], score)
        for name, score, _ in results
    ]


def exact_search(
    query: str,
    limit: int = 10,
    entity_types: Optional[List[str]] = None,
) -> List[Tuple[str, str, str, float]]:
    """
    ç²¾ç¡®å­ä¸²æœç´¢ï¼ˆé™çº§æ–¹æ¡ˆï¼‰
    """
    query_lower = query.lower()
    search_list = load_search_list()
    
    results = []
    for name, token, etype in search_list:
        if entity_types and etype not in entity_types:
            continue
        if query_lower in name.lower():
            # ä½¿ç”¨ç®€å•çš„åŒ¹é…åº¦è®¡ç®—
            score = len(query) / len(name) * 100
            results.append((name, token, etype, score))
    
    # æŒ‰åŒ¹é…åº¦æ’åº
    results.sort(key=lambda x: x[3], reverse=True)
    return results[:limit]


def search_entities(
    query: str,
    limit: int = 10,
    entity_types: Optional[List[str]] = None,
) -> List[str]:
    """
    æœç´¢å®ä½“ï¼Œè¿”å› token åˆ—è¡¨
    
    Args:
        query: æœç´¢è¯
        limit: è¿”å›æ•°é‡
        entity_types: é™åˆ¶ç±»å‹
        
    Returns:
        [token1, token2, ...] åˆ—è¡¨
    """
    results = fuzzy_search(query, limit=limit, entity_types=entity_types)
    return [token for _, token, _, _ in results]


def get_popular_entities(
    limit: int = 10,
    entity_types: Optional[List[str]] = None,
) -> List[Tuple[str, str, str]]:
    """
    è·å–çƒ­é—¨å®ä½“ï¼ˆç”¨äºç©ºæœç´¢æ—¶æ˜¾ç¤ºï¼‰
    
    è¿”å›ç”µå½±å’Œäººå‘˜å„ä¸€åŠ
    """
    search_list = load_search_list()
    
    if entity_types:
        search_list = [
            (name, token, etype)
            for name, token, etype in search_list
            if etype in entity_types
        ]
    
    # ç®€å•ç­–ç•¥ï¼šè¿”å›å‰ N ä¸ª
    # å®é™…å¯ä»¥åŸºäºè¯„åˆ†æˆ–å…¶ä»–æŒ‡æ ‡æ’åº
    return search_list[:limit]


# =============================================================================
# åå‘æŸ¥è¯¢
# =============================================================================

def name_to_token(name: str) -> Optional[str]:
    """
    é€šè¿‡åç§°æŸ¥æ‰¾ token
    
    Args:
        name: æ˜¾ç¤ºåç§°
        
    Returns:
        å¯¹åº”çš„ tokenï¼Œæ‰¾ä¸åˆ°è¿”å› None
    """
    reverse = load_reverse_mapping()
    return reverse.get(name.lower())


def batch_get_display_names(tokens: List[str]) -> Dict[str, str]:
    """
    æ‰¹é‡è·å–æ˜¾ç¤ºåç§°ï¼ˆæ›´é«˜æ•ˆï¼‰
    
    Args:
        tokens: token åˆ—è¡¨
        
    Returns:
        {token: display_name} å­—å…¸
    """
    mapping = load_name_mapping()
    return {token: mapping.get(token, token) for token in tokens}


# =============================================================================
# å·¥å…·å‡½æ•°
# =============================================================================

def format_entity_display(
    token: str,
    include_type: bool = True,
    include_token: bool = False,
) -> str:
    """
    æ ¼å¼åŒ–å®ä½“æ˜¾ç¤º
    
    Args:
        token: Token
        include_type: æ˜¯å¦åŒ…å«ç±»å‹æ ‡ç­¾
        include_token: æ˜¯å¦åŒ…å«åŸå§‹ token
        
    Returns:
        æ ¼å¼åŒ–å­—ç¬¦ä¸²
    """
    name = get_display_name(token)
    parts = [name]
    
    if include_type:
        type_name = get_entity_type_name(token)
        parts.append(f"[{type_name}]")
    
    if include_token and name != token:
        parts.append(f"({token})")
    
    return " ".join(parts)


def get_entity_emoji(token: str) -> str:
    """
    è·å–å®ä½“ç±»å‹çš„ emoji
    """
    prefix = token.split("_")[0] if "_" in token else "OTHER"
    emoji_map = {
        "MOV": "ğŸ¬",
        "ACT": "ğŸ­",
        "DIR": "ğŸ¬",
        "PER": "ğŸ‘¤",
        "GEN": "ğŸ·ï¸",
        "ERA": "ğŸ“…",
        "TYP": "ğŸ“",
        "RAT": "â­",
    }
    return emoji_map.get(prefix, "ğŸ“Œ")


def get_entity_type(token: str) -> str:
    """
    ä» Token ä¸­æå–å®ä½“ç±»å‹å‰ç¼€
    
    Args:
        token: å¦‚ "MOV_tt0111161", "ACT_nm0000001"
        
    Returns:
        å®ä½“ç±»å‹å‰ç¼€ï¼Œå¦‚ "MOV", "ACT"
    """
    if "_" in token:
        return token.split("_")[0]
    return "OTHER"


def search_by_name(
    query: str,
    limit: int = 10,
    entity_types: Optional[List[str]] = None,
) -> List[str]:
    """
    é€šè¿‡åç§°æœç´¢å®ä½“ï¼ˆè¿”å› token åˆ—è¡¨ï¼‰
    
    è¿™æ˜¯ fuzzy_search çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œåªè¿”å› token
    
    Args:
        query: æœç´¢è¯
        limit: è¿”å›æ•°é‡
        entity_types: é™åˆ¶ç±»å‹
        
    Returns:
        [token1, token2, ...] åˆ—è¡¨
    """
    results = fuzzy_search(query, limit=limit, entity_types=entity_types)
    return [token for _, token, _, _ in results]
